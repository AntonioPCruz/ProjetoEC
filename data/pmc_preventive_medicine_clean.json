[
  {
    "pmc_id": "12915807",
    "title": "Influenza vaccine Hesitancy in older adults in China: A latent profile analysis",
    "abstract": "ABSTRACT Influenza vaccination coverage among older adults in China is low. We sought to identify latent vaccine-hesitancy profiles and their correlates. This community-based cross-sectional survey from May to July 2025 involved 1773 older adults from various areas in Jiangsu province. Data were collected via Wenjuanxing and included demographics, the Influenza Vaccine Hesitancy Scale, and the vaccine literacy scale. Group differences were examined using chi-square tests and one-way ANOVA; latent profile analysis (LPA) identified vaccine hesitancy subgroups, and multinomial logistic regression estimated correlates of profile membership. Three profiles emerged: Low Hesitancy (23.0%), Moderate Hesitancy (35.0%), and High Hesitancy (42.0%). Rural residence predicted Moderate (OR = 2.030) and High (OR = 2.993) hesitancy. Lower household income and chronic disease were associated with the Moderate Hesitancy profile, whereas male sex was associated with the High Hesitancy profile. Higher interactive (OR = 0.686) and critical (OR = 0.599) vaccine literacy were inversely associated with High hesitancy.Concerns about vaccine quality predicted both Moderate (OR = 1.433) and High (OR = 1.376) groups; knowledge gaps and fear of adverse reactions concentrated in the High group. Older adults show heterogeneous vaccine hesitancy phenotypes. Uptake efforts should move beyond one-size-fits-all messaging toward segmented strategies. These strategies should integrate cost-related measures with literacy-sensitive, trust-oriented communication, prioritizing rural residents, older men, and those with chronic conditions. The reported proportions of hesitancy profiles reflect our sample only and should not be viewed as nationally representative.",
    "text": "Seasonal influenza represents a formidable global health challenge, responsible annually for an estimated one billion infections, three to five million cases of severe illness, and between 290,000 and 650,000 respiratory deaths worldwide. 1  The burden of this disease falls disproportionately on older adults. Due to immunosenescence and a higher prevalence of comorbidities, they face an elevated risk of severe complications, hospitalization, and mortality. 2  In China, an estimated 71,000 individuals aged 60 and over die from influenza-related respiratory diseases each year, accounting for 80% of the nation’s excess mortality from influenza. 3  Beyond its respiratory toll, influenza exacerbates chronic conditions, particularly cardiovascular diseases, further contributing to the excess mortality observed in this vulnerable population. 4\n\nAnnual vaccination is the most effective method for influenza prevention and remains a major public health priority globally. 5  The World Health Organization (WHO) has set a target vaccination coverage rate of 75% for the elderly to mitigate the severe outcomes of the disease. 6  Many high-income countries have made significant strides toward this goal. For example, during the 2022/23 influenza season, vaccination rates for people aged 65 and older reached 78.0% in Denmark, 75.8% in Portugal, and 75.4% in Ireland. Some countries, like Portugal and the UK, have consistently achieved or exceeded the 75% target in recent seasons. 7  This stands in stark contrast to the situation in China, which is home to the world’s largest and most rapidly aging population, with over 264 million people aged 60 or over. 8  Despite the significant threat, influenza vaccination coverage among Chinese seniors is critically low. For instance, from 2020 to 2022, the average influenza vaccination coverage among registered elderly individuals in Shanghai was a mere 4.1%. 9  A separate cross-sectional survey in Shanghai during the 2021–2022 flu season found a similarly low coverage of 11.70% for elderly individuals. 10  In Ningbo, a study found that targeted interventions such as free vaccination policies could significantly boost this figure, with coverage among older people rising from 1.14% in 2017–18 to 33.41% in 2022–23. 11  These figures highlight a profound public health vulnerability.\n\nA primary barrier to adequate vaccine uptake is vaccine hesitancy (VH). It is defined by the WHO as a delay in acceptance or refusal of vaccination despite the availability of services, 12  recognized as a top ten global health threat. VH in China is a complex phenomenon rooted in concerns about vaccine safety and efficacy, a low perceived risk of influenza, and issues of access and convenience. 13  Previous research on this topic in China has often identified these determinants through traditional regression analyses, which tend to treat the elderly as a monolithic group. This approach, however, may not fully capture the heterogeneity of beliefs, attitudes, and barriers that exist within this large and diverse population. Therefore, public health interventions based on these findings might not be targeted enough to be fully effective.\n\nTo advance understanding, this study employs Latent Profile Analysis (LPA), a person-centered method that moves beyond single predictors to identify distinct subgroups of the elderly. These latent profiles are defined by shared patterns in beliefs, attitudes, and behaviors regarding influenza vaccination. By characterizing these “phenotypes” of hesitancy and exploring their link to factors like vaccine literacy, this research will help provide a deeper and more structured understanding of the issue. The objective is to provide policymakers with evidence to develop more targeted strategies. Such strategies can more effectively address the specific drivers of hesitancy within different subgroups. The ultimate goal is to boost vaccination rates and protect China’s vulnerable older population.\n\nThe study conducted a community-based cross-sectional survey in Jiangsu Province, China, from May to July 2025. Participants were recruited using convenience sampling from three prefecture-level cities selected to cover different geographic areas of Jiangsu (Nanjing, Wuxi, and Yangzhou). Within each city, community health centers were identified in collaboration with local health authorities to ensure coverage of both urban and rural communities; older adults were then approached and invited to participate through these centers and their affiliated community outreach sites. To improve comparability across regions, a quota of approximately ~600 participants per city was prespecified (with minor deviations driven by local availability and response rates).\n\nEligibility criteria were: (1) age ≥ 60 y; (2) residence in the study city for ≥6 months; and (3) adequate expressive and comprehension abilities to complete the survey. Exclusion criteria were: (1) diagnosed psychiatric disorders; and (2) cognitive impairment or communication disorders that would preclude valid participation. After providing written informed consent, eligible individuals completed an anonymous questionnaire; confidentiality and privacy were strictly protected. Ethics approval was granted by the medical ethics committee of Nanjing Women and Children’s Healthcare Hospital (Approval No. PJ-2025KY076-002).\n\nA standardized influenza-vaccination questionnaire was administered via Wenjuanxing ( www.wjx.cn ) in Nanjing, Wuxi, and Yangzhou. Survey implementation was conducted by trained personnel from community health centers following a brief, standardized protocol. Eligible older adults were approached in community settings and, after electronic informed consent, completed the survey anonymously by scanning a site-specific QR code. An interviewer-administered paper version was available for participants with visual or literacy limitations or those without a digital device. These completed forms were subsequently double entered into the Wenjuanxing database. Across the three cities, 2007 questionnaires were distributed. After applying prespecified quality-control criteria, 1773 questionnaires were deemed valid and included in the analysis (Nanjing: 602; Wuxi: 588; Yangzhou: 583). Recruitment within each city was conducted through participating sites and aimed to include both urban and rural residents. To minimize duplication and enhance data integrity, the platform restricted submissions to one per device/IP and embedded basic logic checks. Quality-control procedures prespecified exclusion of records with implausibly short completion times, uniform response patterns (straight-lining), or substantial missingness. The final dataset was fully de-identified prior to export for statistical analysis.\n\nThis study gathered a set of sociodemographic and health-related variables from older adults that include age, sex, urban/rural, education, marital status, number of children, primary caregiver, average monthly household income, primary source of income, and type of health insurance. Health-related variables included self-rated general health, physician-diagnosed chronic disease status (yes/no), and history of influenza vaccination. We also assessed knowledge and attitudes toward influenza vaccination. Knowledge indicators were measured using brief self-report items (“Yes/No/Not sure”) and dichotomized, with No/Not sure indicating a lack of perceived relevant knowledge.\n\nThe Influenza Vaccine Hesitancy Scale was a 14-item scale developed for older adults, organized into three domains: Confidence (7 items), Risk (4 items), and Support (3 items). 14  Each item is answered on a 5-point Likert scale. The instrument was designed for seasonal influenza and older populations in China and aligns conceptually with the WHO Strategic Advisory Group of Experts on Immunization (SAGE) Vaccine Hesitancy framework and subsequent adult adaptations. A total score is calculated by summing all 14 items (range 14–70), with higher scores indicating greater hesitancy. In this study, Cronbach’s alpha for the scale was 0.793.\n\nThe Vaccine Literacy Scale was used with the revised Chinese Health Literacy about Vaccination in adults’ questionnaire (China-HLVa-IT). 15  which contains 14 items across three domains: functional (items 1–5), interactive (items 6–10), and critical literacy (items 11–14). Items were rated on a 5-point Likert scale. Functional items reflect difficulties in reading/understanding vaccination information and were reverse-scored (5 = never to 1 = often), whereas interactive and critical items were scored in the forward direction (1 = never to 5 = often). Thus, all items were oriented so that higher scores consistently indicate higher vaccine literacy. Domain scores (and the total score, if reported) were calculated as the mean of the relevant items. In this study, the scale showed good internal consistency (Cronbach’s α = 0.876).\n\nAll statistical analyses were conducted using Mplus version 8.3 for the latent profile analysis (LPA) and SPSS version 26.0 for all subsequent procedures. A two-tailed  p -value < .05 was considered statistically significant for all tests.\n\nFirst, descriptive statistics were calculated to summarize the sample’s demographic and health-related characteristics. Categorical variables were presented as frequencies (n) and percentages (%), while continuous variables were reported as means and standard deviations (SD). To assess for common method bias, Harman’s single-factor test was performed.\n\nLatent Profile Analysis (LPA) was employed to identify distinct, underlying subgroups of older adults based on their responses across the dimensions of vaccine hesitancy. A series of models specifying one to five latent profiles were estimated. The optimal number of profiles was determined through a comprehensive evaluation of multiple statistical fit indices and substantive theoretical considerations. These indices included the Akaike Information Criterion (AIC), the Bayesian Information Criterion (BIC), and the sample-size Adjusted BIC (aBIC), where lower values indicate a better relative model fit. The Lo-Mendell-Rubin Likelihood Ratio Test (LMR-LRT) and the Bootstrap Likelihood Ratio Test (BLRT) were used to compare nested models; a significant p-value ( p  < .05) indicates that the k-profile model provides a superior fit to the (k-1)-profile model. Classification accuracy was assessed using the Entropy statistic, with values exceeding 0.80 reflecting high precision. The final model selection was also guided by the principles of parsimony, the substantive interpretability of the resulting profiles, and the requirement that each profile contains a meaningful proportion (>5%) of the sample.\n\nOnce the optimal latent profile solution was identified, predictors of profile membership were examined. Univariate analyses were first conducted using one-way analysis of variance (ANOVA) for continuous variables and Pearson’s chi-square (χ 2 ) tests for categorical variables to assess differences across the identified profiles. Subsequently, a multinomial logistic regression was performed to identify the independent predictors of profile membership. The latent profile variable was set as the dependent outcome, with the “Low Hesitancy Group” serving as the reference category. Variables that were statistically significant in the univariate analyses were entered as independent variables into the regression model. A forward stepwise procedure was employed for variable selection, with entry and removal criteria set at α ≤ 0.05 and α ≥ 0.10, respectively. Results from the regression are reported as Odds Ratios (ORs) with their corresponding 95% Confidence Intervals (CI).\n\nThe study included a total of 1773 participants. A preliminary analysis using Harman’s single-factor test was conducted to assess common method bias. The first unrotated factor accounted for 30.78% of the total variance, below the established 40% threshold. This result suggests that common method bias was not a significant concern in this dataset.\n\nLatent Profile Analysis (LPA) was performed to identify distinct subgroups of older adults based on their vaccine hesitancy characteristics. A series of models, specifying from one to five latent profiles, were estimated and compared ( Table 1 ). The AIC, BIC, and aBIC values consistently decreased as the number of profiles increased, indicating improved relative model fit with more profiles. Furthermore, the LMR-LRT and BLRT results were significant ( p  < .001) for the two-, three-, four-, and five-profile models, suggesting that each of these was a significant improvement over a model with one fewer profile. Table 1. Fit indices for latent profile models. Model AIC BIC aBIC Entropy LMR BLRT Profile Proportions (%) 1 15681.818 15714.700 15695.639 – – – – 2 12063.201 12118.005 12086.236 0.976 <0.001 <0.001 24.0, 76.0 3 11277.845 11354.571 11310.094 0.852 <0.001 <0.001 42.0, 23.0, 35.0 4 10426.426 10525.074 10467.889 0.907 <0.001 <0.001 12.9, 35.5, 13.2, 38.5 5 9210.270 9330.839 9260.947 0.946 <0.001 <0.001 13.2, 12.6, 30.1, 4.0, 40.0\n\nHowever, the model selection was also guided by parsimony and interpretability. The five-profile solution yielded a profile comprising only 4.0% of the sample, which is below the recommended 5% threshold for stability and meaningful interpretation. While the four-profile model fit the data well, its profiles were not substantively more distinct or interpretable than the more parsimonious three-profile solution. In contrast, the three-profile model demonstrated excellent fit indices, high classification accuracy (Entropy = 0.852), and yielded three theoretically clear and interpretable profiles. Therefore, the three-profile solution was selected as the best fit for the data.\n\nThe three identified latent profiles were labeled based on their distinct patterns of hesitancy scores ( Figure 1 ). As shown in  Figure 1 , the three profiles demonstrated clearly separated domain-level patterns across Confidence, Risk, and Support. The Low Hesitancy profile showed consistently low scores across all three domains, whereas the High Hesitancy profile showed consistently elevated scores across domains. The Moderate Hesitancy profile lay between these two extremes, indicating an intermediate level of hesitancy across domains. Profile 1: Low Hesitancy (n = 408, 23.0%) showed consistently low scores across all three domains, indicating minimal hesitancy overall; within this profile, scores were slightly higher on the Risk domain than on Confidence and Support, suggesting relatively greater (yet still low) risk-related concerns. Profile 2: Moderate Hesitancy (n = 621, 35.0%) exhibited intermediate scores across domains, with the highest mean score on Risk and a comparatively lower score on Support, consistent with conditional acceptance shaped by perceived risks and limited supportive facilitators. Profile 3: High Hesitancy (n = 744, 42.0%) showed the highest scores overall, with particularly elevated Support and Confidence domain scores relative to Risk, reflecting more entrenched barriers consistent with high hesitancy ( Figure 1 ).\n Figure 1. Three influenza vaccine hesitancy profiles.\n\nThe high reliability of this classification was confirmed by the average posterior probabilities for profile membership, which were 0.912, 0.987, and 0.892 for the Low, Moderate, and High Hesitancy profiles, respectively ( Table 2 ). Table 2. Average posterior probabilities for each latent profile (row) by assigned profile (column). Identified Profile Low Hesitancy Moderate Hesitancy High Hesitancy Low Hesitancy 0.912 <0.001 0.063 Moderate Hesitancy <0.001 0.987 0.013 High Hesitancy 0.074 0.009 0.892\n\nAverage posterior probabilities for each latent profile (row) by assigned profile (column).\n\nUnivariate analyses (one-way ANOVA and Pearson’s chi-square tests) were conducted to compare characteristics across the three vaccine hesitancy profiles, as shown in supplementary Table 1. Significant differences ( p  < .05) were found for the majority of variables, including key sociodemographic factors (age, residence, gender, education, income), health literacy scores, previous vaccination history, and specific knowledge and concerns regarding influenza vaccination. In contrast, no significant differences ( p  > .05) were observed for primary caregiver, chronic disease status, or marital status.\n\nTo further investigate the factors influencing membership in the vaccine hesitancy profiles, a multinomial logistic regression analysis was conducted. The latent profile membership was set as the dependent variable, with variables that were significant in the univariate analyses entered as independent variables. A forward stepwise procedure was used for model building, with entry and removal criteria set at α ≤ 0.05 and α ≥ 0.10, respectively. The Low Hesitancy profile was used as the reference category. The analysis identified several independent predictors for the Moderate and High Hesitancy profiles, as detailed in  Table 3 . Notably, some variables that were significant in univariate analyses were not retained after adjustment; for example, previous influenza vaccination history differed across profiles (χ 2  = 11.068,  p  = .004;) but did not meet the entry criterion in the forward stepwise model after adjustment, suggesting that its association with profile membership may be attenuated when accounting for more proximal determinants. Table 3. Multinomial logistic regression analysis of predictors for latent profile membership (reference: low Hesitancy profile). Model Variable B SE Wald P OR 95%CI Moderate Hesitancy (Intercept) 1.838 0.412 19.869 <.001     Residence (Rural vs. Urban) 0.708 0.217 10.674 .001 2.030 1.328–3.105 Gender (Male vs. Female) 0.097 0.152 0.403 .526 1.101 0.817–1.484 Chronic Disease (Yes vs. No) 0.375 0.136 7.593 .006 1.455 1.114–1.901 Knows Transmission (No vs. Yes) −0.028 0.157 0.033 .856 0.972 0.715–1.322 Concern: Quality (Yes vs. No) 0.360 0.136 6.963 .008 1.433 1.097–1.872 Concern: Adverse Reactions (Yes vs. No) −0.082 0.142 0.336 .562 0.921 0.698–1.216 Believes Unnecessary (Yes vs. No) 0.398 0.164 5.875 .015 1.489 1.079–2.056 Age Group −0.149 0.058 6.544 .011 0.861 0.768–0.966 Household Income −0.213 0.059 13.206 <.001 0.808 0.721–0.907 Interactive Literacy −0.092 0.122 0.572 .449 0.912 0.718–1.158 Critical Literacy −0.160 0.116 1.909 .167 0.852 0.680–1.069 High Hesitancy (Intercept) 2.371 0.422 31.601 <.001     Residence (Rural vs. Urban) 1.096 0.216 25.800 <.001 2.993 1.961–4.568 Gender (Male vs. Female) 0.353 0.151 5.461 .019 1.424 1.059–1.915 Chronic Disease (Yes vs. No) 0.161 0.137 1.379 .240 1.175 0.898–1.538 Knows Transmission (No vs. Yes) 0.560 0.169 10.999 .001 1.751 1.257–2.438 Concern: Quality (Yes vs. No) 0.319 0.138 5.367 .021 1.376 1.050–1.803 Concern: Adverse Reactions (Yes vs. No) 0.368 0.141 6.835 .009 1.445 1.097–1.904 Believes Unnecessary (Yes vs. No) −0.029 0.175 0.028 .868 0.971 0.689–1.370 Age Group 0.052 0.058 0.798 .372 1.053 0.940–1.181 Household Income −0.056 0.061 0.840 .359 0.946 0.839–1.066 Interactive Literacy −0.377 0.120 9.845 .002 0.686 0.542–0.868 Critical Literacy −0.513 0.115 19.825 <.001 0.599 0.478–0.751\n\nMultinomial logistic regression analysis of predictors for latent profile membership (reference: low Hesitancy profile).\n\nSeveral factors significantly predicted membership in the Moderate Hesitancy profile. Compared to the Low Hesitancy reference, rural residence was associated with 103.0% higher odds of being in the Moderate Hesitancy profile (OR = 2.030), chronic disease with 45.5% higher odds (OR = 1.455), concern about vaccine quality with 43.3% higher odds (OR = 1.433), and believing vaccination was unnecessary with 48.9% higher odds (OR = 1.489). In contrast, for each one-category increase in age group, the odds of Moderate Hesitancy decreased by 13.9% (OR = 0.861), and for each one-category increase in household income, the odds decreased by 19.2% (OR = 0.808).\n\nDistinct predictors emerged for the High Hesitancy profile. Rural residence was associated with 199.3% higher odds of High Hesitancy (OR = 2.993), male sex with 42.4% higher odds (OR = 1.424), lack of knowledge of influenza transmission routes with 75.1% higher odds (OR = 1.751), concern about vaccine quality with 37.6% higher odds (OR = 1.376), and concern about adverse reactions with 44.5% higher odds (OR = 1.445). Critically, for each one-unit increase in interactive and critical literacy, the odds of being in the High Hesitancy profile (vs. Low Hesitancy) decreased by 31.4% (OR = 0.686) and 40.1% (OR = 0.599), respectively.\n\nOur study utilized Latent Profile Analysis (LPA) to investigate the heterogeneity of influenza vaccine hesitancy among older adults in China. The analysis identified three distinct subgroups: a Low Hesitancy (23.0%), a Moderate Hesitancy (35.0%), and a High Hesitancy (42.0%) group. This finding confirms that vaccination attitudes among older adults are not uniform, supporting the need for targeted rather than one-size-fits-all public health approaches. The existence of these distinct profiles aligns with conceptual frameworks like the WHO “3Cs” model, which posits that hesitancy arises from varying combinations of factors related to confidence, complacency, and convenience. 12  In line with the WHO “3Cs” model, our profiles appear to differ in the relative salience of confidence, complacency, and convenience-related barriers. The Low Hesitancy profile shows broadly favorable attitudes with limited apparent barriers. The Moderate Hesitancy profile may be shaped by lower perceived necessity (complacency) together with affordability-related constraints (convenience). By comparison, the High Hesitancy profile may reflect a stronger confidence-related barrier, as concerns about vaccine safety with lower interactive health literacy and knowledge gaps, potentially increasing uncertainty and perceived risk. This mapping is summarized in  Figure 2 . In addition, the results demonstrate that the Moderate and High Hesitancy profiles are shaped by different determinants. Moderate hesitancy was primarily associated with pragmatic and health condition related considerations (e.g., chronic disease status and household income), consistent with a risk benefit calculation influenced by perceived vulnerability and affordability. In contrast, high hesitancy was characterized by markers of informational disadvantage and distrust (e.g., male gender, poor knowledge of influenza transmission, and lower interactive health literacy), pointing to more fundamental barriers in accessing, evaluating, and trusting vaccine-related information.\n Figure 2. Mapping the three vaccine hesitancy profiles to the WHO 3Cs framework.\n\nConsistent with existing literature on health disparities, 16 , 17  our study confirmed that socio-structural factors are significant determinants of vaccine hesitancy. Our analysis, however, provides a more detailed understanding of how these factors predict membership in different hesitancy profiles. Rural residence was a predictor for both moderate (OR = 2.030) and high hesitancy, but its effect was most pronounced for the High Hesitancy group (OR = 2.993). This suggests that while rurality is a general barrier, it might be associated with deficits in knowledge and high levels of mistrust that characterize the most hesitant individuals. Rural areas often contend with relative deficiencies in health resources and limited channels for accessing reliable vaccine information, which can lead to knowledge gaps and misconceptions, such as perceiving vaccines as ineffective or unnecessary. Moreover, previous research indicates that beyond mere physical distance, the quality of healthcare interactions and trust in local providers is often lower in rural settings, contributing to greater skepticism. 18  This finding suggests that limited access to well-resourced healthcare facilities and trained professionals in rural areas may contribute to knowledge gaps and erode trust in the health system. 19\n\nInterestingly, lower household income and the presence of a chronic disease were significant predictors for the Moderate Hesitancy group but not the High Hesitancy group. The association with lower income highlights the critical barrier of out-of-pocket costs, as the influenza vaccine is not covered by China’s National Immunization Program. 20  This financial barrier has been cited as a primary reason for forgone influenza vaccination, 21  acting as a pragmatic filter that forces a direct cost-benefit decision.\n\nThe relationship between chronic disease and vaccine hesitancy is notably complex. While some research suggests that individuals with chronic conditions may have higher vaccine acceptance rates due to a greater perceived risk of disease, 22  our finding that this group is more likely to belong to the Moderate Hesitancy profile reveals a critical tension. This suggests that many individuals with chronic diseases are caught between two competing considerations: a heightened awareness of their vulnerability to severe influenza, which motivates acceptance, 23  and a simultaneous, pronounced fear that the vaccine could negatively interact with their specific health condition. 24  This internal conflict aligns perfectly with the characteristics of the Moderate Hesitancy profile, which is defined not by a rejection of vaccination in principle, but by a highly focused and anxious risk-benefit calculation. Therefore, their hesitancy is not a result of simple complacency, but rather a manifestation of this specific, unresolved safety concern. Our finding suggests that for a significant portion of this population, the default position is not acceptance, but cautious hesitation. Pragmatically, this subgroup may benefit from brief, structured counseling embedded in routine chronic disease follow-up visits. A one-on-one risk assessment co-delivered by a physician and a nurse can review comorbidities and current medications, clarify true contraindications versus misconceptions, and provide condition-specific risk–benefit communication.\n\nBased on these findings, multi-faceted interventions are recommended. For rural older adults, personalized health education campaigns clarifying vaccine safety and importance are crucial. 25  This should be coupled with structural improvements, such as increasing the number of vaccination sites, offering mobile or home-based vaccination services, and simplifying appointment procedures. To address economic barriers, policymakers should explore including the influenza vaccine in the national immunization program, especially for high-risk groups in key regions or during peak seasons, through free or subsidized schemes. Furthermore, targeted science communication for patients with chronic diseases is needed to explain the protective benefits of vaccination against secondary complications, thereby enhancing their willingness to be vaccinated. For patients with chronic diseases, brief clinician–nurse co-delivered one-on-one counseling that addresses vaccine–disease/medication concerns and clarifies contraindications may be particularly effective in reducing uncertainty and improving acceptance.\n\nBeyond structural factors, our analysis identified distinct individual-level predictors. Within the 60+ cohort, younger age was a significant predictor for membership in the Moderate Hesitancy profile. This aligns with research on optimism bias, 26  where “younger-old” adults may underestimate their vulnerability to severe influenza complications due to a perception of better health compared to their older peers. Indeed, a more optimistic outlook on future health can inadvertently lower the perceived urgency of preventative measures like annual vaccinations. 27  Male gender was a significant predictor for the High Hesitancy group, an association that appears linked to this profile’s characteristics of low health literacy and high distrust. The vaccination decision-making process for men may be influenced by masculine norms prescribing independence and self-reliance. For instance, prior research indicates that older men may engage less frequently with preventative health services and exhibit lower health information-seeking behaviors. 28  This behavioral tendency is compounded by the critical factor of gender differences in risk perception, as men and women often perceive the same risks differently or prioritize different risks altogether.\n\nWhile age and gender are significant, our analysis reveals that the most critical individual-level predictor distinguishing the highly hesitant is health literacy. A key finding of this study is the significant protective role of health literacy against high levels of vaccine hesitancy. While functional literacy did not differ significantly, lower levels of both interactive (OR = 0.686) and critical (OR = 0.599) health literacy were strong and independent predictors of membership in the High Hesitancy group. This provides a crucial insight that high hesitancy is strongly linked to an individual’s capacity to access, understand, and critically apply health information, extending beyond individual attitudes or beliefs alone. This finding is consistent with a broad body of research that identifies low health literacy as a formidable barrier to the uptake of preventive health services and a key determinant of health outcomes. 29\n\nOur results suggest that these two advanced dimensions of health literacy may protect against the drivers of high hesitancy. Interactive literacy, the ability to actively seek out and communicate about health information, 30  is essential for overcoming knowledge gaps about topics like influenza transmission that were prevalent in the High Hesitancy group. Individuals with low interactive literacy are less likely to ask clarifying questions of healthcare providers or effectively navigate complex health systems, rendering them passive recipients of information and more susceptible to anecdotal evidence from their social circles. In contrast, critical literacy, the ability to critically appraise the source and credibility of information, 31  directly counters the fears about vaccine quality and adverse reactions that define the High Hesitancy profile. This skill enables individuals to differentiate between credible scientific evidence and pervasive misinformation and understand relative risk. Without this capacity, individuals are more susceptible to cognitive biases, 32  leading to an exaggerated perception of vaccine risk. Therefore, low health literacy can be seen as a foundational vulnerability that exposes individuals to the specific fears and knowledge gaps, which are the characteristics of the most hesitant group.\n\nFrom a public health perspective, these findings suggest that communication should be tailored and sensitive to people’s health literacy levels. The central role of health literacy suggests that public health efforts should focus on building skills for “how to think,” not just delivering information on “what to think.” This can be achieved by training healthcare providers in patient-focused communication techniques like “teach-back.” 33  It can also be done by developing campaigns that teach people how to identify misinformation, such as proactive strategies to debunk common misrepresentations before they spread. Within this framework, messages can be more specifically targeted. For example, when communicating with younger-old adults at risk of moderate hesitancy, the focus should be on helping them reevaluate their personal risk with clear, relatable data. For older men, it may be more effective to use messages that frame vaccination as a proactive way to maintain independence and protect their family.\n\nTaken together, these findings indicate two distinct pathways to hesitancy. Moderate hesitancy appears more “decision-focused,” where individuals are not uniformly opposed to vaccination but remain uncertain due to affordability and concerns about vaccine suitability in the context of chronic illness. High hesitancy appears more “information- and trust-focused,” where limited health literacy and gaps in basic influenza knowledge may increase susceptibility to misinformation and amplify perceived vaccine risk. Positioning the Discussion around this contrast helps explain why a single strategy is unlikely to be effective across profiles.\n\nThe reasons for vaccine hesitancy are tied to psychological factors. Our findings show that different ways of thinking drive the moderate and High Hesitancy groups, and it’s not just about a simple lack of information. People in the Moderate Hesitancy group were most likely to believe that the vaccine is unnecessary. This points to complacency, a mind-set where people underestimate their personal risk of getting sick. 34  This is especially common among healthier or “younger-old” adults who think the flu is not a serious illness. For them, the effort of getting a vaccine seems to outweigh the benefits. In contrast, the High Hesitancy group had different issues. They were not concerned about whether the vaccine was necessary. Instead, they lacked basic knowledge about how the flu spreads and had strong fears about vaccine safety. This suggests a different set of psychological drivers. Their lack of knowledge can lead to wrong ideas, like believing that staying indoors is enough to prevent infection, which creates a false sense of security. 35  At the same time, their strong safety concerns show a common bias where people fear the potential harm from an action, like vaccination, more than the harm from inaction, like the risk of getting sick from the flu. This bias is often made worse by low health literacy because people who cannot critically evaluate information are more likely to believe personal stories or misinformation about side effects.\n\nThese thinking patterns are closely linked to another key factor, which is trust. Our study highlights that a lack of trust is a central part of vaccine hesitancy in older adults. Concern about vaccine quality was a significant predictor for both the moderate (OR = 1.433) and high hesitancy (OR = 1.376) groups, suggesting a pervasive lack of confidence that extends beyond the vaccine product itself to the broader health system. This form of institutional distrust may be contextualized by public memory of past domestic vaccine safety incidents in China, which can create a lasting erosion of confidence in regulatory oversight. 36  A strong and specific fear of side effects was a unique driver for the High Hesitancy group, suggesting that while moderate hesitancy may stem from general doubts, high hesitancy is driven by a deep, personal fear of being harmed. This erosion of trust can be understood through two interconnected pathways. First, it directly impacts risk perception, often amplified by cognitive biases, 37  especially when vivid stories of side effects stick in their minds. Second, when people don’t trust the healthcare system, they look for information elsewhere and often find misinformation, which is a particular problem for those with low health literacy. 38\n\nTo address these psychological drivers, interventions should focus on helping people reevaluate their personal risk while rebuilding trust. For the moderately hesitant, whose issue is primarily complacency, interventions should use clear statistics and relatable case studies to illustrate the severe potential outcomes of influenza, thereby challenging their optimistic bias. For the highly hesitant, an interpersonal approach is essential. This involves empowering trusted primary healthcare providers to engage in empathetic, one-on-one conversations that validate concerns while patiently explaining the minimal risks of vaccination versus the substantial risks of the disease. 39  This communication can be powerfully supported by other methods. Public health messages should be transparent about vaccine quality control. Testimonials from vaccinated peers also serve as a relatable source of reassurance. This can help reframe the decision as a way to protect the community.\n\nFor the High Hesitancy profile (predominantly rural older men with low critical literacy and high distrust), “literacy-sensitive, trust-oriented communication” should be delivered by locally trusted primary care providers (e.g., township clinicians/village doctors) through brief face-to-face counseling in community clinics or outreach settings. Messages should use plain language and simple visuals, address a few core concerns (influenza transmission, vaccine quality control, and expected side effects), and use teach-back to confirm understanding. Peer testimonials from vaccinated community members and simplified access (same-day vaccination, streamlined registration, or mobile outreach) may further support uptake. For the Moderate Hesitancy profile, brief individualized risk–benefit consultations – especially for people with chronic conditions – can provide condition-specific reassurance and reduce worries about adverse interactions. Accordingly, interventions should combine provider-led, literacy-sensitive trust-building for the High Hesitancy group with brief clinician counseling for the Moderate Hesitancy group, alongside measures that reduce cost and access barriers. In practical terms, this can be delivered as 3–5-minute micro-counseling by township clinicians/village doctors using a pictorial one-page leaflet (transmission routes, vaccine quality assurance, expected mild reactions, and warning signs), followed by a brief teach-back question to confirm understanding. Implementation can be paired with same-day vaccination/mobile outreach and male peer testimonials in familiar community settings to reduce both cognitive and access barriers.\n\nWhile this study provides valuable insights into the heterogeneity of influenza vaccine hesitancy among older adults in China, several limitations should be noted. First, the cross-sectional design precludes causal inference; longitudinal studies are needed to track changes over time and clarify pathways underlying shifts between hesitancy profiles. Second, all measures were self-reported and thus subject to recall and social desirability bias, and the knowledge indicators reflected perceived rather than objectively tested knowledge. Future work could adopt mixed-methods approaches, such as combining surveys with qualitative interviews, to better capture the beliefs and experiences shaping hesitancy. Third, convenience sampling from three cities in Jiangsu (a coastal, economically developed province) limits generalizability and may introduce selection bias, as individuals engaged with community health services could be overrepresented. Accordingly, although the identification of three profiles is informative, the estimated prevalence (23.0% low, 35.0% moderate, 42.0% high) should be interpreted cautiously. Probability-based sampling across more diverse regions, including less developed inland areas, is needed to validate these findings and provide more robust population estimates. In addition, although prior influenza vaccination history differed across profiles in univariate analysis, it was not retained in the adjusted multinomial model, and vaccination-related experiences were not assessed, limiting the ability to elucidate how past behavior shapes hesitancy profiles.\n\nFinally, despite including a broad set of variables, some factors were not measured, including family and social network influences, healthcare experiences, and media exposure (particularly to misinformation). In addition, chronic disease was assessed only as a binary (yes/no) indicator, without information on disease type, severity, or control status; therefore, condition-specific differences in hesitancy among older adults with chronic illnesses could not be examined. In older Chinese adults, vaccination decisions may be shaped by perceived social support and family involvement – especially adult children’s participation in decision-making – and by interpersonal trust in frontline healthcare providers, which differs from general institutional trust. These contextual factors were not included primarily due to pragmatic constraints on questionnaire length and respondent burden in a community-based survey. The absence of these measures may have limited our ability to fully explain profile membership, and future studies should incorporate multilevel determinants (family, provider, and community) and distinguish provider trust from institutional trust.\n\nDespite these limitations, applying latent profile analysis moves beyond a one-size-fits-all view of older adults. The identified profiles and their distinct predictors can inform more targeted, effective, and empathetic interventions to improve influenza vaccination coverage in China.\n\nThis study revealed considerable heterogeneity in influenza vaccine hesitancy among older adults in Jiangsu, China, identifying three distinct profiles (low, moderate, and high hesitancy). Misconceptions persist regarding influenza transmission, the need for vaccination, and adverse reactions to vaccination, but specific concerns about vaccine quality and vaccine adverse effects were found in the highly hesitant profile. Some of the determinants of hesitancy found were rural residence, male sex, lower household income status, chronic disease status, and lower degrees of interactive and critical vaccine literacy.\n\nTailored educational interventions are needed to address the specific informational needs of different hesitancy profiles, using literacy-sensitive communication strategies to reduce knowledge gaps and build trust. Health systems should also implement measures to alleviate practical barriers to vaccination, such as expanding vaccination sites, offering mobile or door-to-door services, simplifying appointment procedures, and providing financial support for low-income, high-risk older adults. National vaccination campaigns should prioritize rural areas, older men, and individuals with chronic diseases while fostering trust through transparent communication. Because the profiles identified were derived from a non-probability sample in Jiangsu Province, the findings should be interpreted with caution, and future research using nationally representative data is warranted.",
    "mesh_query": "preventive medicine[MeSH Terms]",
    "source_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12915807/",
    "data_crawling": "2026-02-19T18:21:48.395398"
  },
  {
    "pmc_id": "12913767",
    "title": "Cardiovascular disease prevention by personalized health promotion considering educational attainment",
    "abstract": "To examine whether educational attainment modifies the effectiveness of personalized lifestyle recommendations for improving cardiovascular risk factors. This post-hoc analysis used data from a population-based randomized controlled trial in Girona, Spain, including adults aged 35–74 years without cardiovascular disease at baseline. Participants (n = 759; 48.7% men) were randomized to an intervention group (n = 380), which received personalized recommendations on diet and physical activity, or a control group (n = 379), which received a standard report of baseline results. Changes in systolic and diastolic blood pressure, LDL cholesterol, and physical activity energy expenditure from baseline to 12 months were analyzed. Multivariable linear regression models adjusted for age included an interaction term defined as group × educational attainment × time (1 year) to assess effect modification. Analyses were stratified by sex. Among women in the intervention group, significant interactions by educational attainment were observed. Compared with women with lower educational attainment, those with higher attainment showed more favorable changes in diastolic blood pressure [beta-coefficient (95% confidence interval): − 1.98 (− 4.23; 0.27) vs. 1.63 (− 0.21; 3.48)], LDL cholesterol [− 4.61 (− 11.40; 2.18) vs. 5.71 (0.25; 11.17)], and physical activity energy expenditure [0.52 (0.04; 1.23) vs. − 0.01 (− 0.36; 0.26)]. No significant interactions were found among men in the intervention group or among participants in the control group. Overall, within-group changes in these outcomes did not reach statistical significance. Personalized lifestyle interventions may provide greater benefits for women with higher educational attainment, although overall improvements were modest. Supplementary Information The online version contains supplementary material available at 10.1038/s41598-026-36654-4.",
    "text": "Cardiovascular diseases (CVDs) remain the leading cause of death worldwide, accounting for an estimated 17.9 million deaths annually 1 . Evidence shows that adopting a healthy lifestyle can substantially reduce premature mortality from these conditions and extend life expectancy by up to 14 years for women and 12 years for men 2 . Educational attainment plays a critical role in promoting health by enhancing self-awareness and improving access to healthcare services 3 . Cardiovascular risk factors—such as hypertension, elevated body mass index, and diabetes—partially mediate the relationship between education and CVD incidence 4 , 5 . Despite widespread public health messages encouraging healthy behaviors, persistent barriers prevent certain population groups from fully benefiting from advances in information and communication technologies 6 . Limited educational attainment, in particular, acts as an invisible barrier to prevention and healthcare delivery, generating significant costs at both individual and population levels 7 , 9 .\n\nSeveral strategies have been proposed to implement personalized prevention effectively, including targeted educational programs, community awareness initiatives, population empowerment, and robust information systems 10 . An innovative self-screening method for cardiovascular risk—validated in a randomized crossover clinical trial—addresses these strategies by providing personalized recommendations on diet and physical activity to manage risk factors, showing promising results 11 , 13 . Although scientific evidence consistently links higher educational attainment to better health outcomes 14 , its influence on the effectiveness of personalized health recommendations remains unclear and might vary by gender 15 , 17 . We hypothesize that educational attainment modifies the impact of personalized lifestyle and cardiovascular risk management advice delivered through a validated self-screening system. Specifically, individuals with higher education levels are expected to achieve greater improvements in the control of cardiovascular risk factors compared to those with lower education. Furthermore, gender may influence how educational background affects individuals’ ability to benefit from such interventions, potentially leading to different outcomes in men and women. The objective of this study was to assess whether educational attainment influences the effectiveness of personalized recommendations for improving the management of cardiovascular risk factors, using sex-stratified analyses to explore potential gender differences.\n\nThis study was a randomized, parallel-arm controlled trial with a 12-month follow-up. The detailed methodology has been described elsewhere 11 , 12 . Participants were eligible if they were aged 35 to 74 years, resided in Girona or surrounding areas in northeastern Spain, and had no history of CVD at baseline. Eligible individuals were randomly selected from the population and randomly assigned to either the intervention or control group. Invitations were sent by postal mail, followed by a phone call from a field worker to confirm participation and schedule an appointment.\n\nParticipants were allocated to the intervention or control group using a computer-generated random sequence integrated into custom-designed data collection software. This sequence was concealed from the field workers responsible for enrollment. The intervention group received personalized preventive recommendations tailored to each individual’s cardiovascular risk profile, based on the latest scientific evidence. In contrast, the control group received only a standard letter reporting baseline examinations results, mailed within one month of data collection.\n\nThe primary analysis of the randomized controlled trial on which this study is based demonstrated intervention efficacy among men, leading to reductions in systolic and diastolic blood pressure and improved control of total and LDL cholesterol levels. In contrast, women did not experience significant changes in any of the assessed cardiovascular risk factors 12 . A non-stratified post-hoc analysis indicated promising effects on lifestyle behaviors—particularly improved adherence to the Mediterranean diet and smoking cessation—highlighting the potential of this personalized, evidence-based intervention as an innovative tool for cardiovascular prevention 13 .\n\nAll participants were fully informed about the study and provided written informed consent prior to enrollment. All methods were performed in accordance with the relevant guidelines and regulations. The clinical trial protocol was registered at ClinicalTrials.gov (# NCT02373319 ) and approved by the Institutional Research Board [Clinical Research Ethics Committee of Parc de Salut Mar (CEIC-PSMAR, #2014/5815/I)].\n\nExaminations were conducted by a team of trained nurses using standardized questionnaires and measurement methods at both baseline and 12-month follow-up. Data on sex, age, and socio-demographic information were reported via self-administered standardized questionnaires. Participants were asked to indicate their educational attainment, categorized as follows: (1) lower educational attainment (no formal education, primary, or secondary education) and (2) higher educational attainment (university education).\n\nA precision scale with easy calibration was used to measure weight, with participants wearing only their underwear. Height was measured with a standard measuring rod, with participants standing barefoot. Body mass index was calculated as weight divided by height squared (kg/m 2 ). Blood pressure was measured using an automatic monitor with a cuff appropriately sized (young, adult, obese) for each participant’s upper arm circumference). After a 5-min rest, two measurements were taken at least 2 min apart, and the lower value was recorded for the study.\n\nAdherence to the Mediterranean diet was assessed using the 14-item Mediterranean Diet Adherence Score (MEDAS) questionnaire, validated for the Spanish population 18 . The questionnaire comprises 12 questions on the frequency of food consumption and 2 questions on food intake habits characteristic of the Spanish Mediterranean diet. Each question was scored 0 or 1, with the final score ranging from 0 to 14. To measure energy expenditure from leisure-time physical activity (EEPA), the REGICOR questionnaire includes six two-part questions that gather information on the four dimensions of physical activity: type, frequency, duration, and intensity. EEPA was estimated by multiplying the intensity assigned to each activity by the number of days it was performed in a month and the average number of minutes per day. The metabolic equivalent of task values for the six activities were: walking (4), brisk walking (5), gardening (5), walking trails (6), climbing stairs (8), and any sport activity (10) 19 .\n\nTo determine lipid profile and glycaemia, blood samples were drawn within 60 s after a 10–14 h fast. Serum aliquots were stored at −80 °C. Total cholesterol and high-density lipoprotein (HDL) cholesterol concentrations were measured using enzymatic and direct methods, respectively (ABXHoriba, Montpellier, France). When triglycerides were < 300 mg/dL, low-density lipoprotein (LDL) cholesterol was calculated using the Friedewald formula. All analyses were conducted in a central laboratory.\n\nThe personalization process began by creating an individual profile for each participant based on collected data, including: (a) prevalence of cardiovascular risk factors, (b) tobacco use, and (c) physical activity level. Recommendations for the intervention group were based on the most up-to-date scientific evidence 20 , 22 . Participants received a packet containing the results of baseline examinations (blood pressure, lipid profile, diabetes status, and smoking) along with their estimated cardiovascular risk, calculated using the Framingham–REGICOR risk function validated for the Spanish population 23 . A trained nurse provided a detailed explanation of personalized recommendations during a 30-min consultation, focusing on strategies to improve cardiovascular health through weight control, smoking cessation, adherence to the Mediterranean diet, and increased physical activity tailored to individual performance levels (sedentary, moderate, or vigorous). Supplementary Materials include the personalized recommendations based on a Mediterranean dietary pattern and physical activity provided to each participant in the intervention group according to their individual profile.\n\nAt the end of the 12-month follow-up, a re-examination was conducted to measure various biomarkers. Outcomes were defined as the change in values between the baseline examination and the 12-month assessment. These outcomes consisted of continuous changes in the levels of various cardiovascular risk factors: (1) systolic blood pressure, (2) diastolic blood pressure, (3) LDL cholesterol, (4) HDL cholesterol, (5) glycaemia, (6) body mass index, (7) adherence to the Mediterranean diet, and (8) EEPA.\n\nData were analyzed using a per-protocol approach including participants who completed the 12-month follow-up (80% of the total sample). Normality of continuous variables was visually assessed using Q–Q plots. All results were stratified by sex and educational attainment. Baseline and 12-month values for systolic and diastolic blood pressure, LDL and HDL cholesterol, body mass index, glycemia and adherence to the Mediterranean diet were summarized using means, while EEPA was summarized using medians. These values were plotted by educational attainment (low vs. high) and intervention/control groups, separately for women and men. The percentage change at 12 months was calculated by subtracting the baseline biomarker value from the follow-up value, dividing the result by the baseline value, and multiplying by 100.\n\nTo assess effect modification, we tested whether educational attainment influenced the intervention effect on changes in cardiovascular risk factors from baseline to 12 months. Multivariable linear regression models adjusted for age included an interaction term defined as group × educational attainment × time (1 year), which was the main analysis. Analyses were stratified by sex. Interaction significance was evaluated using the − 2 log-likelihood test comparing nested models with and without the interaction term. For outcomes showing significant interactions, adjusted beta coefficients and 95% confidence intervals were estimated to illustrate the magnitude of change within the intervention group by educational attainment. EEPA values were log-transformed prior to inclusion in the models due to non-normal distribution. Model assumptions—linearity, homoscedasticity, normality of residuals, and absence of multicollinearity—were verified using standard diagnostic tools (residuals vs. fitted plots, Q–Q plots, and variance inflation factors).\n\nAll statistical analyses were conducted using the R Statistical Package (R Foundation for Statistical Computing, Vienna, Austria; V.4.2.2) 24 .\n\nThe study included 759 individuals (48.7% men), having high educational attainment 140 (36.0%) women and 134 (36.2%) men.\n\nTables  1  and  2  present the baseline characteristics of women and men, respectively, stratified by educational attainment. Participants with lower educational attainment exhibited a worse cardiovascular profile. Supplementary Table 1 shows the distribution of cardiovascular risk factors across the full sample. Table 1 Characteristics at baseline and 12-month change in women by educational attainment. Low educational attainment Control (N = 125) 12- month percent change Intervention (N = 124) 12- month percent change Age (years)* 53 (10) – 55 (11) – Systolic blood pressure (mmHg)* 107 (16) 1.0 [− 6.9; 8.3] 110 (19) 0.9 [− 6.8; 8.4] Diastolic blood pressure (mmHg)* 72 (10) − 1.4 [− 8.7; 5.5] 74 (12) 0.0 [− 6.1; 5.7] LDL cholesterol (mg/dL)* 127 (31) 0.0 [− 8.4; 15.4] 142 (33) 2.4 [− 4.0; 11.7] HDL cholesterol (mg/dL)* 59 (13) − 1.7 [− 8.2; 7.4] 60 (13) − 0.9 [− 8.9; 7.5] Glycaemia (mg/dL)* 92 (16) 1.2 [− 3.3; 6.3] 93 (13) 2.2 [− 3.8; 7.1] Body mass index (kg/m 2 )* 26.1 (5.0) 0.0 [− 0.5; 0.5] 27.0 (4.7) 0.1 [− 0.6; 0.8] Adherence to Mediterranean diet* 7.1 (2.0) 0.0 [− 12.5; 22.2] 7.1 (2.0) 0.0 [− 11.1; 20.0] EEPA (kcal/day) † 1790 [958; 2769] − 12.1 [− 47.9; 79.9] 1856 [1096; 2947] − 16.6 [− 52.5; 32.8] High educational attainment Control (N = 71) 12−  month percent change Intervention (N = 69) 12- month percent change Age (years)* 46 (8) – 47 (9) – Systolic blood pressure (mmHg)* 102 (13) 0.8 [− 6.0; 7.7] 101 (11.8) − 0.9 [− 6.4; 6.9] Diastolic blood pressure (mmHg)* 69 (10) 1.4 [− 5.4; 7.8] 69 (9) − 1.8 [− 8.4; 5.0] LDL cholesterol (mg/dL)* 127 (27) 6.2 [− 7.8; 15.3] 125 (30) 3.2 [− 2.7; 10.1] HDL cholesterol (mg/dL)* 60 (13) 4.0 [− 6.4; 8.9] 61 (13) − 4.0 [− 8.2; 6.9] Glycaemia (mg/dL)* 88 (9) − 1.1 [− 3.8; 5.8] 88 (9) 1.3 [− 5.1; 5.8] Body mass index (kg/m 2 )* 25.0 (4.4) 0.0 [− 0.5; 0.5] 24.4 (4.0) 0.0 [− 0.6; 0.5] Adherence to Mediterranean diet* 7.2 (1.9) 14.3 [− 4.5; 28.6] 7.5 (1.7) 0.0 [− 12.5; 16.7] EEPA (kcal/day) † 1497 [946; 2352] 1.3 [− 34.0; 66.1] 1911 [751; 2532] 24.9 [− 13.1; 104.6] EEPA, Energy expenditure in physical activity. HDL, High-density lipoprotein. IQR, interquartile range. LDL, Low-density lipoprotein. SD, standard deviation. *Mean (standard deviation).  † Median [Interquartile range]. Table 2 Characteristics at baseline and 12-month change in men by educational attainment. Low educational attainment Control (N = 123) 12- month percent change Intervention (N = 113) 12- month percent change Age (years)* 50 (10) – 51 (10) – Systolic blood pressure (mmHg)* 118 (14) − 1.7 [− 6.0; 8.2] 120 (15) − 0.9 [− 6.7; 5.2] Diastolic blood pressure (mmHg)* 76 (9) 1.2 [− 6.6; 7.1] 79 (9) − 1.9 [− 8.8; 6.9] LDL cholesterol (mg/dL)* 140 (34) 1.3 [− 5.6; 11.0] 142 (38) − 0.9 [− 9.5; 13.3] HDL cholesterol (mg/dL)* 50 (10) − 0.2 [− 6.7; 6.2] 50 (11) − 1.1 [− 6.8; 7.4] Glycaemia (mg/dL)* 95 (12) 0.5 [− 4.5; 6.8] 100 (25) 2.1 [− 3.5; 7.0] Body mass index (kg/m 2 )* 26.7 (3.8) 0.1 [− 0.4; 0.7] 27.5 (3.8) 0.1 [− 0.5; 0.8] Adherence to Mediterranean diet* 6.7 (1.9) 0.0 [− 12.5; 25.0] 6.8 (2.2) 0.0 [− 14.3; 20.0] EEPA (kcal/day) † 2308 [1087; 4192] − 15.8 [− 46.3; 49.8] 2543 [979; 3916] − 10.3 [− 50.0; 43.3] High educational attainment Control (N = 63) 12- month percent change Intervention (N = 71) 12- month percent change Age (years)* 50 (10) – 49 (11) – Systolic blood pressure (mmHg)* 117 (15) 0.0 [− 5.1; 5.2] 117 (17) − 1.5 [− 8.8; 4.1] Diastolic blood pressure (mmHg)* 77 (10) 1.3 [− 5.1; 8.3] 77 (10) − 1.4 [− 9.4; 6.1] LDL cholesterol (mg/dL)* 134 (35) 3.5 [− 8.5; 14.4] 128 (27) 0.0 [− 6.6; 13.0] HDL cholesterol (mg/dL)* 50 (12) 1.3 [− 7.7; 11.1] 50 (9) 0.2 [− 6.6; 6.6] Glycaemia (mg/dL)* 98 (26) 1.1 [− 4.5; 5.7] 98 (34) 1.5 [− 4.5; 4.9] Body mass index (kg/m 2 )* 26.8 (3.4) 0.1 [− 0.4; 0.7] 26.8 (4.7) − 0.1 [− 0.4; 0.5] Adherence to Mediterranean diet* 7.1 (2.0) 10.0 [− 5.0; 22.5] 7.5 (2.2) 0.0 [− 10.6; 26.8] EEPA (kcal/day) † 2014 [1070; 3360] − 15.1 [− 42.3; 27.0] 2023 [1136; 3399] 10.7 [− 35.2; 79.3] EEPA, Energy expenditure in physical activity. HDL, High-density lipoprotein. IQR, interquartile range. LDL, Low-density lipoprotein. SD, standard deviation. *Mean (standard deviation).  † Median [Interquartile range].\n\nCharacteristics at baseline and 12-month change in women by educational attainment.\n\nEEPA, Energy expenditure in physical activity. HDL, High-density lipoprotein. IQR, interquartile range. LDL, Low-density lipoprotein. SD, standard deviation.\n\nCharacteristics at baseline and 12-month change in men by educational attainment.\n\nEEPA, Energy expenditure in physical activity. HDL, High-density lipoprotein. IQR, interquartile range. LDL, Low-density lipoprotein. SD, standard deviation.\n\nEducational attainment did not significantly interact with the 12-month modification of any cardiovascular risk factors in men. However, in women, it interacted with the association between the intervention and two biomarkers: diastolic blood pressure ( p -value for interaction = 0.019) and LDL cholesterol ( p -value for interaction = 0.029). Additionally, EEPA showed a marginally significant interaction ( p -value for interaction = 0.057) in women (Figs.  1  and  2 ). Supplementary Table 2 presents the p-values for all interaction terms tested. Fig. 1 Baseline and 12-month values for systolic and diastolic blood pressure, LDL and HDL cholesterol, body mass index, glycaemia and adherence to the Mediterranean diet (summarized as means), as well as energy expenditure in physical activity (summarized as medians), stratified by educational attainment and intervention/control groups among women. Fig. 2 Baseline and 12-month values for systolic and diastolic blood pressure, LDL and HDL cholesterol, body mass index, glycaemia and adherence to the Mediterranean diet (summarized as means), as well as energy expenditure in physical activity (summarized as medians), stratified by educational attainment and intervention/control groups among men.\n\nBaseline and 12-month values for systolic and diastolic blood pressure, LDL and HDL cholesterol, body mass index, glycaemia and adherence to the Mediterranean diet (summarized as means), as well as energy expenditure in physical activity (summarized as medians), stratified by educational attainment and intervention/control groups among women.\n\nBaseline and 12-month values for systolic and diastolic blood pressure, LDL and HDL cholesterol, body mass index, glycaemia and adherence to the Mediterranean diet (summarized as means), as well as energy expenditure in physical activity (summarized as medians), stratified by educational attainment and intervention/control groups among men.\n\nA stratified analysis by educational attainment was conducted to explore these significant interactions. The beta coefficients (95% confidence intervals [CI]) for diastolic blood pressure were − 1.98 (− 4.23; 0.27) for women with higher educational attainment and 1.63 (− 0.21; 3.48) for those with lower attainment. For LDL cholesterol, the beta coefficients were − 4.61 (− 11.40; 2.18) for women with higher educational attainment and 5.71 (0.25; 11.17) for those with lower attainment. In addition, log-linear regression models stratified by educational attainment were fitted to account for the non-normal distribution of EEPA in women. The beta coefficients (95% CI) were 0.52 (0.04; 1.23) for women with higher educational attainment and − 0.01 (− 0.36; 0.26) for those with lower attainment (Fig.  3 ). Fig. 3 Adjusted beta coefficients for absolute changes in diastolic blood pressure and LDL cholesterol, and log-linear beta coefficients for relative changes in energy expenditure in physical activity, among women by educational attainment, comparing intervention and control groups.\n\nAdjusted beta coefficients for absolute changes in diastolic blood pressure and LDL cholesterol, and log-linear beta coefficients for relative changes in energy expenditure in physical activity, among women by educational attainment, comparing intervention and control groups.\n\nHigher educational attainment was associated with better control of specific cardiovascular risk factors only in women. No statistically significant differences were observed in men. This population-based randomized controlled trial evaluated the efficacy of a personalized health promotion intervention and revealed significant interactions with educational attainment. After 12 months of follow-up, the influence of educational level was particularly evident in the control of diastolic blood pressure, LDL cholesterol, and increased EEPA among women. In contrast, the remaining risk factors showed changes of similar magnitude across educational levels, consistent with findings from previous studies 12 , 13 . These results highlight the importance of considering socio-demographic variables, such as educational attainment, alongside cardiovascular risk profiles when designing personalized health promotion strategies 25 , 27 .\n\nThis population-based randomized controlled trial demonstrated that individuals with lower educational attainment exhibited a worse cardiovascular profile at baseline, consistent with findings from previous studies 28 , 29 . In our sample, cardiovascular risk profiles significantly worsened with age. Additionally, a higher proportion of women had lower educational attainment compared to men. Consequently, all statistical analyses were stratified by sex, and multivariable models were adjusted for age—both variables known to contribute substantially to health inequalities 30 , 31 . Clouston et al. have highlighted additional determinants such as adolescent cognition, non-cognitive skills, and the rate of cognitive decline as predictors of health literacy in later life, which are closely associated with sex and age 32 . These interrelated factors reinforce the notion that educational attainment is a modifiable determinant of health inequalities 33 , 34 .\n\nFrom a clinical perspective, these findings underscore the need to tailor cardiovascular prevention strategies to socio-educational contexts, particularly among women 7 . The observed associations between higher educational attainment and improved control of diastolic blood pressure, LDL cholesterol, and physical activity suggest that educational level may influence individuals’ ability to engage with and benefit from personalized health interventions. Although the intervention was uniformly delivered, its effectiveness varied by educational level, highlighting potential disparities in health literacy, self-management capacity, and access to supportive resources 35 . Recognizing and addressing these differences is essential to ensure equitable health outcomes and to optimize the impact of preventive strategies in diverse populations 36 .\n\nDahlgren and Whitehead’s model of health determinants illustrates the interplay between individual lifestyles—shaped by social norms and networks—and broader socioeconomic and cultural conditions 37 . Our findings partially reflect this model: after 12 months, disparities in cardiovascular risk profiles by educational attainment widened (e.g., prevalence of optimal blood pressure, systolic blood pressure). This trend may be explained by the sample’s average educational level and the intervention design, which offered personalized recommendations based on cardiovascular risk but did not account for educational differences. Education is closely linked to occupation and income, and multiple pathways connect it to health outcomes—primarily through health literacy, the ability to access, understand, and apply health information 38 . Since health literacy is a modifiable risk factor for CVD, improving educational attainment could support both primary and secondary prevention 7 , 9 , 39 . However, this strategy requires structural and policy-level efforts to reduce socioeconomic inequalities and ensure equitable access to education 3 , 40 . Personalized health promotion programs should, therefore, integrate socioeconomic factors and literacy-sensitive communication strategies, rather than focusing solely on individual risk factors. As Geoffrey Rose compellingly argued, effective prevention of CVD requires not only targeting individuals at high risk but also addressing the broader social, structural, and environmental determinants that shape population-level risk distributions. His concept of the “cause of causes” underscores the importance of shifting the entire risk curve through upstream interventions—such as policies that reduce social inequalities, improve living and working conditions, and promote healthier environments. Although this population-based strategy may produce less visible short-term effects, it has the potential for a much greater overall impact. It also places responsibility on policymakers to create conditions that support cardiovascular health for all 41 .\n\nEvidence-based health promotion, which considers individual lifestyles, social norms, networks and living and working conditions, is essential for maintaining and improving cardiovascular health. This holistic approach underscores the urgent need to implement effective healthcare practices that empower individuals to take greater control over their health and enhance overall well-being 42 . Importantly, this strategy must also address how gender shapes health outcomes, as women and men often encounter different barriers and opportunities influenced by education, social roles, and access to healthcare services 22 , 23 . Womenface unique challenges in health literacy and empowerment, often rooted in gender-specific social and economic dynamics 24 . Therefore, one of the central challenges in cardiovascular prevention is to promote health literacy and empower individuals—especially women—while minimizing the risk of exacerbating existing health inequalities 25 , 26 . Addressing these disparities requires that interventions designed to improve cardiovascular health explicitly incorporate gender-sensitive approaches, ensuring that differences between men and women are carefully considered in both the design and implementation of these programs 29 , 43 , 44 .\n\nThis analysis included individuals who completed the 12-month follow-up of a randomized controlled trial (81%). Although the study sample was randomly selected from the population, participants with higher educational attainment were more likely to complete the follow-up. Consequently, the distribution of educational attainment was slightly altered: the percentage of individuals with university education increased from 35.1% to 36.1%, which may have introduced selection bias. Baseline differences in cardiovascular risk factors by education level were also observed, with individuals with lower educational attainment exhibited a worse cardiovascular risk profile. While changes in biomarkers were considered to mitigate this limitation, residual confounding cannot be ruled out.\n\nIn addition, information on medication use during follow-up was not collected. Although this fact introduces some uncertainty, the limitation likely biases toward the null hypothesis, as women with lower educational attainment had higher baseline LDL cholesterol and diastolic blood pressure values compared to those with higher educational attainment (142 mg/dL vs. 125 mg/dL and 74 mmHg vs. 69 mmHg, respectively), making them more likely to initiate treatment. Finally, the sample size did not allow for stratified analyses by level of higher education (e.g., bachelor’s, master’s, doctoral degrees) or by age. Future studies with larger samples and longer follow-up periods will be necessary to explore cohort and period effects in greater detail.\n\nPersonalized interventions tailored to control cardiovascular risk factors may offer greater benefits for women with higher educational attainment, particularly in improving diastolic blood pressure, LDL cholesterol levels, and EEPA. However, as within-group improvements in these outcomes did not reach statistical significance, the findings should be interpreted with caution. These results underscore the importance of incorporating literacy-sensitive communication strategies and addressing socioeconomic disparities when designing effective health promotion programs.",
    "mesh_query": "preventive medicine[MeSH Terms]",
    "source_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12913767/",
    "data_crawling": "2026-02-19T18:21:49.499783"
  },
  {
    "pmc_id": "12914620",
    "title": "Safe Babies Safe Moms Women's and Infants’ Services Perinatal Mental Health and Wellness Program: A Comprehensive Integrated Model for Prevention and Treatment",
    "abstract": "Abstract The Safe Babies Safe Moms Women's and Infants’ Services Perinatal Mental Health and Wellness Program offers screening, assessment, prevention, and treatment options, as well as referrals, for complex care needs using an integrated collaborative care framework. Group and individual therapy options are available, leveraging both office‐based and telehealth options. The article details the program's framework for services, describes the care team and program workflow, and summarizes program activities. In calendar year 2024, 3383 screenings using the Edinburgh Postnatal Depression Scale were completed for 1914 unique patients. Perinatal social workers conducted 1328 care coordination visits and completed 148 perinatal mental health assessments for elevated screenings. Integrated therapeutic services reached 220 people, and 53 were referred to psychiatry for complex care needs. The strengths of this program stem from early initiation of screening, multiple screenings throughout the perinatal period, care coordination for social services interventions, and a shared‐decision‐making process centered around the patients’ needs and goals for perinatal mental health care. This integrated mental health model establishes a distinct, well‐defined, yet dynamic pathway to provide screening, care coordination, and interventions for both prevention and treatment to optimize perinatal mental health outcomes. Evaluation of program outcomes is ongoing.",
    "text": "Perinatal mental health conditions are the most common complication of the perinatal period. These conditions are a leading cause of disability and preventable pregnancy‐related mortality, accounting for 23% of deaths. \n 1 \n  This is particularly true for perinatal individuals from minoritized communities in the United States, including those from Black and Hispanic backgrounds who experience significant burden due to structural racism. \n 2 \n ,  \n 3 \n ,  \n 4 \n ,  \n 5 \n ,  \n 6 \n  Given the significant burden on individuals with perinatal mental health conditions, the American College of Nurse‐Midwives, \n 7 \n  the American College of Obstetricians and Gynecologists, \n 8 \n  and the Alliance for Innovation on Maternal Health (AIM) Perinatal Mental Health Safety Bundle \n 9 \n  provide guidance for supporting perinatal mental health across the treatment cascade (ie, screening, evaluation, treatment, and follow‐up).\n\nEven with increased screening, fewer than 25% of people report accessing mental health treatment in the perinatal period. \n 10 \n  Implementation of best mental health practices in perinatal care settings has been challenged by multiple barriers, including lack of provider training and awareness, time constraints during perinatal appointments, and stigma. Additional obstacles include workforce shortages; fragmentation of care between perinatal, mental health, and community resources; inadequate screening protocols with limited follow‐up; and cultural and language barriers that hinder effective communication and care coordination. \n 11 \n \n QUICK POINTS \n ✦ Screening to identify perinatal mental health needs and adverse social drivers of health occurs multiple times during the perinatal period to engage patients in communicating their lived experience and centers their social identity(ies), culture, context of their lives, support systems, and goals connected to their care experience. ✦ Led by perinatal social workers, program clinicians develop a plan with the patient based on identified strengths, goals, and needs using a dynamic team approach including nurses, referral specialists, therapists, psychologist, psychiatrist, midwives, and physicians. ✦ The perinatal mental health team offers recommendations and treatment options including prevention services, individual brief, targeted therapy, intensive therapy, group therapy, and medication management. Both in‐person and telehealth options are available.\n\n✦ Screening to identify perinatal mental health needs and adverse social drivers of health occurs multiple times during the perinatal period to engage patients in communicating their lived experience and centers their social identity(ies), culture, context of their lives, support systems, and goals connected to their care experience. ✦ Led by perinatal social workers, program clinicians develop a plan with the patient based on identified strengths, goals, and needs using a dynamic team approach including nurses, referral specialists, therapists, psychologist, psychiatrist, midwives, and physicians. ✦ The perinatal mental health team offers recommendations and treatment options including prevention services, individual brief, targeted therapy, intensive therapy, group therapy, and medication management. Both in‐person and telehealth options are available.\n\nScreening to identify perinatal mental health needs and adverse social drivers of health occurs multiple times during the perinatal period to engage patients in communicating their lived experience and centers their social identity(ies), culture, context of their lives, support systems, and goals connected to their care experience.\n\nLed by perinatal social workers, program clinicians develop a plan with the patient based on identified strengths, goals, and needs using a dynamic team approach including nurses, referral specialists, therapists, psychologist, psychiatrist, midwives, and physicians.\n\nThe perinatal mental health team offers recommendations and treatment options including prevention services, individual brief, targeted therapy, intensive therapy, group therapy, and medication management. Both in‐person and telehealth options are available.\n\nModels such as collaborative care \n 12 \n ,  \n 13 \n  and Program in Support of Moms (PRISM) \n 14 \n  have shown effectiveness in supporting the integration of mental health services in perinatal care. Collaborative care is a strategy that includes screening and identification by the health care provider, a range of supportive interventions by the care manager, and case review and focused consultation from a perinatal psychiatrist. \n 15 \n  A care manager facilitates initial treatment planning, brief intervention, longitudinal symptom monitoring, and implementation of mental health specialist–informed stepped‐care recommendations. PRISM enhances capacity by providing access to training for perinatal care providers, psychiatric consultation, mental health resources, and intensive implementation support. This approach promotes proactive, multifaceted care supported by perinatal care team members who facilitate treatment and follow‐up. Other effective models include the HUGS/Abrazos program, \n 16 \n  which also emphasizes access to brief psychological support and therapy within perinatal care settings, in addition to referral support to access psychiatric and psychological care as well as social support. Many models focus on improving best practices outlined in the AIM Perinatal Mental Health Bundle of recommendations. \n 9 \n  However, these models often focus on integration of screening and follow‐up as needed, rather than delivery of prevention interventions and treatment, and they may offer limited access to direct care by a psychiatrist, even in the setting of complex psychiatric presentations during the perinatal period. Information on how clinicians in the perinatal setting can offer screening plus prevention interventions, treatment, and connections to psychiatry specialty services for complex care is needed.\n\nThe purpose of this article is to describe an integrated perinatal mental health wellness and treatment program that includes social services interventions, perinatal mood disorder prevention interventions, brief and extended therapy options, collaborative psychiatry, and coordinated referral for acute and complex behavioral health needs. The program is one component of the Safe Babies Safe Moms (SBSM) Initiative, a health system initiative launched in 2020 to improve maternal and child outcomes in the District of Columbia with an aspiration to advance maternal health equity. \n 17 \n  There are 4 clinical pillars under SBSM, one of which is Women's and Infants’ Services (WIS), in which this program operates. \n 18 \n  This report may guide other organizations interested in offering similar services.\n\nThe SBSM WIS initiative recognizes the emotional well‐being of expectant and birthing persons as an essential component of healthy pregnancy and successful transition to parenting. It offers an integrated wellness model that is inclusive of social service interventions, promotion of emotional regulation and prevention of perinatal mood disorders, and treatment for psychiatric conditions. The wellness model is led by perinatal social workers (PSWs), and team workflow is provided in Figure  1 . Care begins with screening of individual emotional, social, and environmental factors that impact perinatal health, and it is completed up to 3 times: (1) at the initial prenatal visit, (2) at the early third trimester visit aligned with glucola testing for gestational diabetes (approximately 28 weeks), and (3) at the first postpartum office visit (approximately 6 weeks after giving birth). The goal of these assessments is to engage patients in a process that communicates their lived experience and centers their social identity(ies), support systems, and goals connected to their care experience. Assessment is repeated in recognition of how circumstances may change throughout the course of care and to create multiple opportunities to share information. The triage care team suggested these points because they aligned with the established workflow when a comprehensive history or routine laboratory results were collected. Table  1  provides a description of team members, scope of care, and training.\n\nThe Women's and Infants’ Services Safe Babies Safe Moms Workflow Responds to Prevention, Persistent Conditions, and Acute Needs\n\nAbbreviations: CBE, Certified Breastfeeding Educator; C‐EFM, Certification in Electronic Fetal Monitoring; GED, Graduate Equivalency Degree; IBCLC, International Board Certified Lactation Consultant; Perinatal‐LAW, Perinatal Legal Assistance & Well‐being Project; SBSM, Safe Babies Safe Moms; WIC, Supplemental Nutritional Program for Women, Infants, and Children.\n\nThe assessment incorporates screening for perinatal mood and anxiety disorders using the Edinburgh Postnatal Depression Scale (EPDS) \n 19 \n  as well as questions about housing conditions and food access that are based on the EveryONE project social needs screening tool. \n 20 \n  Questions about intimate partner violence and substance use during the perinatal period are guided by the Abuse Assessment Screen \n 21 \n  and the 5Ps Substance Abuse Screen, \n 22 \n  respectively. Health harming legal needs are identified and referred to a health‐legal partnership (Perinatal Legal Assistance & Well‐being Project [Perinatal‐LAW]), which is described in detail elsewhere. \n 23 \n ,  \n 24 \n  The SBSM WIS team also included questions about the quality of the patient's relationship with coparents and support systems, as well as health education interests.\n\nPatients receive a paper copy of the combined screening tool at arrival for their prenatal appointment associated with the assessment interval in their language of choice (English, Spanish, Amharic, French). An interpreter is available for other languages, and a reader is provided for individuals with literacy barriers. A nurse navigator reviews the information with the patient and enters responses in the electronic health record. Together, the patient and nurse‐navigator develop a care plan that reflects patient goals in identified areas. Care plan objectives related to social service interventions are connected to the SBSM WIS referral specialist. Health education and clinical management support are provided by a nurse navigator. The PSWs meet with patients who identify factors related to substance use, personal safety, emotional well‐being, and mental health, to review and determine a plan with the patient. Next steps may include care team planning, referral for brief therapeutic supports or psychotherapy, referral for more intensive and specialized psychotherapy for chronic and acute needs, and consultation with perinatal psychiatry. PSWs adopt a harm reduction approach to substance use and draw upon motivational interviewing to engage people in care planning for this condition. Treatment is offered in collaboration with specialty settings. The PSW provides support throughout the perinatal period, offering individual assistance to navigate the unique stigmas people may encounter during the perinatal period.\n\nThe perinatal wellness services described below are available to all patients as needed and desired. The perinatal mental health team offers recommendations and develops a management plan with the patient based on identified strengths, goals, and adverse social, environmental, legal, and behavioral drivers of health. Screening is followed by assessment and treatment options, with referral to more intensive therapy and psychiatric care as indicated (Figure  2 ).\n\nThe Women's and Infants’ Services Safe Babies Safe Moms Protocol to Guide Team Recommendations Reflects Screening, Assessment, Treatment, and Referral Pathways and Includes Ongoing Monitoring for Symptom Improvement\n\nAbbreviations: CBT, cognitive behavioral therapy; GAD‐7, General Anxiety Disorder 7‐item scale; EPDS, Edinburgh Postnatal Depression Scale; MH, mental health; PHQ‐9, Patient Health Questionnaire 9‐item scale; PPD, postpartum depression; ROSE, Reach Out, Stay Strong, Essentials; TFCBT, Trauma‐Focused CBT.\n\nScreening for adverse social drivers of health is completed as part of usual triage practice. Nurse navigators connect patients to the SBSM WIS referral specialist to assist patients with accessing community resources. There is ongoing collaboration between perinatal health team members and the referral specialist when the need for social service interventions is identified. Diverse referrals for social service interventions are facilitated by the PSW through outreach and facilitated connections to social service agencies in the community, such as accessible food banks. There are 5 core services with collaborative workflows within the SBSM team. These are summarized in Table  2 . Perinatal health team members facilitate a direct linkage to the referral specialist for assistance accessing the Special Supplemental Nutrition Program for Women, Infants, and Children, our medical legal partnership known as Perinatal‐LAW, and transportation. A defined supply of free diapers is provided to patients through partnership with the Greater DC Diaper Bank (GDCDB). Perinatal health care team members provide diapers directly to patients, and the referral specialist maintains partnership with GDCDB through monthly data tracking and reporting. The referral specialist also places reminder calls for mental health appointments. The nurse navigation team manages the safe sleep education and distribution of cribettes.\n\nAbbreviations: GDCDB, Greater DC Diaper Bank; Perinatal‐LAW, Perinatal Legal Assistance & Well‐being Project; SBSM, Safe Babies Safe Moms; WIS, Women's and Infants’ Services; WIC, Supplemental Nutritional Program for Women, Infants, and Children.\n\nSBSM WIS offers care coordination to facilitate access to these 5 key social service programs.\n\nApproaches to treatment and perinatal mental health services offered are reviewed by the interprofessional team during a weekly meeting and based on clinical need and patient preferences. Determination of clinical need is based on an evaluation of symptom severity, comprehensive mental health history, and the complexity of the behavioral health presentation, including the presence of co‐occurring disorders. Figure  1  presents the care continuum and decision‐making tree that guides individual treatment and care recommendations. Detailed description of the interventions is presented in Figure  1 .\n\nReach Out, Stay Strong, Essentials (ROSE) for mothers of newborns is an evidence‐based intervention demonstrated to prevent the onset of postpartum depression. Recommended by the US Preventive Services Task Force, the program has been tested in a randomized clinical trial and demonstrated a 50% reduction in the risk of postpartum depression among pregnant and birthing people from low‐resource settings. \n 25 \n  These findings remained stable when the program was tested with people of diverse social identities, including race and ethnicity. \n 26 \n  ROSE is offered to all interested patients. The group is held virtually, facilitated weekly by a PSW, and consists of 4 psychoeducation classes on postpartum depression, managing stress, communication skills, and social support.\n\nPSWs meet with patients who screen positive or are identified as having risk factors for perinatal mood and anxiety disorders to further assess their needs and goals related to mental health. PSWs and patients jointly develop an intervention plan to respond to symptoms and concerns identified. Following the assessment, the PSW aligns the patient's preferences and needs with the appropriate level of care for mental health support and treatment, which can include time‐limited, problem‐focused, mental health interventions with integrated therapist to reduce symptoms and improve patient functioning; collaboration with the integrated perinatal psychiatrist to facilitate medication treatment for depression and anxiety; supported referral to specialized outpatient psychiatric program for those with severe, acute, and complex mental health conditions; or referral and linkage to (or in some cases, re‐engagement with) a community mental health program for long‐term needs that extend beyond perinatal period, such as substance use disorder–specific treatment or treatment for complex posttraumatic stress disorder. Services offered in a co‐located and integrated manner are described in additional detail.\n\nThe PSW team provides solution‐focused interventions to support the mental and behavioral health of patients focused on specific, individual goals. Sessions take place in an integrated setting with a goal of destigmatizing psychotherapy and improving patient functioning. Sessions may also include supporting patients to access community resources. Sessions are aligned with monthly prenatal care appointments. Typically, these sessions last about 30 minutes, and patients engage for at least 4 and up to 12 sessions.\n\nThe integrated therapist provides psychotherapeutic treatment for a range of perinatal mental health conditions and emotional challenges using cognitive behavioral therapy (CBT). Sessions are typically 45 minutes, occur via telehealth, and vary in frequency based on clinical need. Individual therapy may commence anytime during the perinatal period and continue for up to one year postpartum. The therapist works with individuals who desire or need longer‐term treatment to identify a community mental health provider and to facilitate a successful transition in care.\n\nAfterglow is an open telehealth psychotherapy group designed to support postpartum patients experiencing mild to moderate perinatal mood and anxiety disorders based on EPDS score and maternal mental health assessment. Led by a PSW using CBT‐based techniques, it explores 6 topics including perinatal mental health, birth experiences, coping skills for parenting transitions, and self‐care. Participants gain a deeper understanding of their challenges, develop practical tools to improve their mood, connect with other birthing parents, and discover strategies for coping and self‐care.\n\nIn alignment with the collaborative care model, the perinatal clinician is designated as the primary prescriber of pharmacotherapy, with the perinatal psychiatrist serving primarily in a consultative and supportive role. Prenatal health care providers prescribe and manage psychiatric medications in direct consultation with the perinatal psychiatrist and the rest of the perinatal mental health care team. The perinatal psychiatrist meets with the PSW and perinatal care providers every other week to provide consultation support on cases including diagnostic and treatment considerations (pharmacologic and nonpharmacologic, level of care); support measurement‐based care by reviewing symptom assessments, tracking patient progress, and adjusting as needed; support team members with crisis intervention; and provide education for team members as needed.\n\nPatients with acute clinical presentations or severe perinatal mood and anxiety disorders symptoms necessitating frequent individual psychotherapy sessions, direct evaluation and treatment by a perinatal psychiatrist, or intensive treatment for severe or life‐threatening psychiatric conditions (eg, intensive outpatient, partial hospitalization, or inpatient admission), or patients requiring longer‐term mental health care beyond the perinatal period, are referred via the perinatal behavioral health care coordinator to the Women's Mental Health Program (WMHP) at MedStar Georgetown Department of Psychiatry. The WMHP is the primary site in the District of Columbia offering comprehensive perinatal mental health care with specialists. The availability of behavioral health treatment via telehealth has increased access to psychiatric treatment for perinatal individuals across the District of Columbia. Patients requiring more intensive programming and disorder specific specialized care (eg, substance use, eating disorder treatment) are referred to appropriate community‐based specialty programs.\n\nScreening and initial follow‐up treatment planning occurs with the SBSM WIS perinatal mental health team, and they provide treatment for most patients in an integrated fashion. In calendar year (CY) 2024, 1914 people were screened using the EPDS, and 3383 screenings were completed. Care coordination for social services interventions was provided to 741 people (38.7% of those served). Consistent with trauma‐informed care practices, additional assessment with PSWs was offered to all individuals who identified symptoms during assessment and decisions to decline services were honored. Prevention services through ROSE reached 118 people. Collectively, the integrated PSW team provided distinct therapeutic follow‐up services to 25.4% (486 of 1914) of patients. Of those, 7.4% received brief, targeted therapy, 6.2% participated in Afterglow postpartum group sessions, and 31.7% engaged in therapy with the integrated perinatal therapist. Facilitated external referrals for more complex care needs occurred for 53 (3.9%) people in CY 2024. A perinatal psychiatrist is an important part of the workflow and provides weekly consultation to the PSW and perinatal care providers and is available for individual provider consultation as needed.\n\nStrengths of this approach have emerged. There are multiple assessments during the perinatal journey, and care is initiated at the time patient goals or adverse circumstances are identified. Patients receive an immediate response to the information they share and decide how they wish to proceed with recommendations and support. Centering the care plan as a set of goals and priorities that stem from completing the perinatal mental health assessment amplifies the patient's voice and grounds the work in a process of shared‐decision‐making. Core services include education, social service interventions and support, and both preventive and therapeutic intervention. Access to evidence‐based prevention services amplifies the whole‐person approach to care.\n\nPatients also can choose how they engage with therapy services, selecting visit format (in‐person or telehealth), duration, and type (individual or group). Of note, telehealth remains the most frequently requested option, representing more than 75% of individual encounters and 100% of groups. The weekly team meeting, including the program coordinator, a midwife who liaises with the provider team, perinatal psychiatrist, PSW, and therapist, allows for collaborative care and adjustments to care plans to ensure therapeutic progress toward patient goals. The team communicates with the broader group of health care providers primarily through the electronic health record and in person as appropriate. Finally, referrals for medical management as well as more intensive therapeutic needs are facilitated.\n\nAvailability of prevention services, therapy treatment, and psychopharmacological management as a component of care appears to be an unusual combination based on our review of the existing literature. Additionally, the program benefits from a diverse team, the majority of whom are racially concordant with patients, which may enhance the therapeutic connection with the goal of fostering a sense of understanding, trust, and cultural reflexivity. Racial concordance between therapists and patients may validate their identities, reduce the need to explain cultural nuances, and support more effective communication and rapport‐building. \n 27\n\nKey recommendations derived from developing and implementing this program are provided in Table  3 . They offer practical guidance for others who may be interested in developing a similar constellation of services. Strategies to advance service delivery in the context of common challenges are included. It is important to note that this program was created in the context of allocated resources that allowed significant comprehensive system changes to address perinatal health. Programs may consider starting with the resources already within their systems of care and connecting with freely available national resources such as Perinatal Psychiatry Access Programs to develop a stepwise approach that fits their context needs and resources.\n\nLaunching an integrated mental health program within a perinatal setting requires interprofessional collaboration, guideline alignment, effective team communication, and creative problem‐solving. Select questions to build the assessment tool with input from all team members, with attention to time to complete the tool. Plan to continuously refine it to ensure relevance, effectiveness, and efficiency.\n\nScreening identified many adverse social drivers of health. Facilitating access to social service interventions overwhelmed PSW capacity and limited their availability to offer therapeutic interventions. The model pivoted to align screening with nurse navigators who then triaged responses. A referral specialist who facilitated access to social service interventions also joined the team. As a result, PSWs could focus their time and expertise on treatment. These services include individual brief, focused therapy as well as perinatal CBT, group psychotherapy for postpartum depression, care coordination to establish psychiatric care for individuals, and a weekly collaborative care meeting with a psychiatrist and psychologist for complex referrals.\n\nPatients often juggled competing priorities, expressing common barriers such as lack of time to devote to therapy, transportation difficulties, prior poor experiences with services, and fear of processing difficult emotions. These hurdles necessitated an adaptive strategy with flexible care models. PSWs adopted drop‐in hours and brief, solution‐focused therapy. Referral specialists also provided proactive communication and logistical support, which improved access and continuity. Finally, adhering to health equity best practices to offer a diverse team may help facilitate engagement.\n\nStrong referral networks for specialized and intensive psychiatric treatment are essential to ensure continuity of care beyond what a perinatal care setting can provide. Since 2008, the WMHP has been a leading regional provider of comprehensive perinatal mental health care and an essential resource within our health system. The program expanded to establish the Mother‐Baby Intensive Outpatient Program (IOP) in April 2024, which offers psychiatric and therapeutic services for people with moderate to severe symptoms. The Mother‐Baby IOP provides intensive services (10‐12 hours of treatment over 3‐4 days per week), which is well beyond what standard outpatient treatment can offer. By filling this gap, the program extends integrated mental health services and bridges specialized outpatient psychiatric care with hospital‐based inpatient psychiatric services.\n\nThe COVID‐19 pandemic initially hampered physical integration of mental health professionals within perinatal settings; however, telehealth's expansion during the pandemic mitigated this issue, ultimately allowing flexible service delivery through a blend of virtual and in‐person interactions. Without pandemic restrictions, the program offered co‐located in‐person psychiatric appointments on a biweekly basis. Surprisingly, this in‐person option was underused and the program pivoted to the telehealth approach described. In‐person appointments remain available, yet patients most often choose telehealth visits.\n\nInitially, the program faced challenges such as staff turnover complicated by delays in recruitment and hiring and persistently high demand. Virtual team meetings and peer supervision facilitated collaboration and eased logistical challenges of coordination. Periodic team meetings facilitated by independent perinatal mental health trainers addressed burnout and compassion fatigue and provided training. These sessions proved essential to ongoing development of team expertise, adoption of new therapeutic modalities, and fostering professional resiliency.\n\nThis perinatal mental and behavioral health program demonstrates a comprehensive, integrated approach that connects screening with education, assessment, treatment, and referral. By ensuring screening is only conducted when treatment pathways exist, nurse navigators, PSWs, and referral specialists collaboratively streamline triage, care coordination, and follow‐up. This article provides a framework for other perinatal care settings aiming to implement a sustainable, patient‐centered perinatal mental health program. Future work will focus on evaluation of program outcomes. To guide broader dissemination, we also recognize the importance of efforts exploring implementation in other settings, assessing scalability, and examining cost‐effectiveness.",
    "mesh_query": "preventive medicine[MeSH Terms]",
    "source_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12914620/",
    "data_crawling": "2026-02-19T18:21:51.253669"
  },
  {
    "pmc_id": "12915538",
    "title": "Successfully Recruiting Black and Hispanic/Latino Adolescents for Sexually Transmitted Infection and HIV Prevention Research",
    "abstract": "Disparities in rates of sexually transmitted infections (STIs) and HIV between Black/African American and Hispanic/Latino adolescents and their white counterparts are well documented. Researchers may encounter notable challenges recruiting Black/African American and Hispanic/Latino adolescents for sexual risk reduction studies. In this article, we present information to assist with planning, implementing, and evaluating recruitment and retention strategies. We also provide practical examples of challenges and solutions from three STI/HIV epidemiologic or prevention intervention studies with different study purposes and populations. Researchers can use this information to aid proposal development, create or refine a recruitment/retention protocol before implementation, and troubleshoot challenges during implementation.",
    "text": "Adolescents and young adults (aged 15–24 years) are at risk for sexually transmitted infections (STIs), including human immunodeficiency virus (HIV). In 2016, persons aged 15–24 years accounted for 63% (1,008,403 of 1,598,354), 46.6% (218,302 of 468,514), and 21.2% (8428 of 39,782) of reported cases of chlamydia, gonorrhea, and HIV diagnoses, respectively [ 1 ,  2 ]. For each of these infections, Blacks/African Americans (referred to hereafter as Black) and Hispanics/Latinos are disproportionately affected. For example, the rate of HIV diagnoses per 100,000 for Black adolescents (26.0) was nearly 19 times the rate for white adolescents (1.4) in 2016; the rate for Hispanic/Latino adolescents (5.1) was 4 times that of their white peers that year [ 3 ]. A combination of cultural, biological, and behavioral factors increases adolescents’ vulnerability for STI or HIV acquisition [ 4 ]. Targeted efforts are warranted to reach adolescents during this period of development when they are navigating changes in their physical and cognitive development, renegotiating relationships with family, friends, and community, and establishing patterns that will likely persist into adulthood [ 5 ].\n\nConducting STI/HIV prevention intervention research with adolescents, as well as the epidemiologic research that serves as its foundation, presents unique challenges. Although some attention has been devoted to developing appropriate measures and data collection methodologies for research conducted with adolescents, less attention has been given to strategies for more effective recruitment of adolescent research participants. This is especially important regarding Black and Hispanic/Latino adolescents because these populations are disproportionately affected by STI/HIV, and, historically, racial/ethnic minorities have been underrepresented in ethical research [ 6 ,  7 ]. Because of this gap in the method-focused evidence base, adolescent-focused researchers could benefit from additional guidance to facilitate recruitment, achieve study aims for adolescent engagement, and ensure more efficient use of human and material resources.\n\nThe purpose of this article is to share effective practices for recruitment and retention of adolescents in STI/HIV prevention research. The information is gleaned from our experiences successfully conducting research with Black and Hispanic/Latino adolescents. We provide information to assist with planning, implementing, and evaluating recruitment and retention strategies, as well as practical examples of challenges and solutions from three STI/HIV epidemiologic or prevention intervention studies with different topics, research designs, and study populations (see  Table 1  for detailed study descriptions). This information can be useful for various purposes, including proposal development, developing or refining a recruitment/retention protocol before implementation, and troubleshooting challenges during implementation.\n\nPlanning plays a vital role in successful recruitment. Careful consideration of the research context, including the study aims, population, resources, geographic location, and social and political climate, is needed to develop the most appropriate recruitment strategy ( Table 2 ). For adolescent populations, it is necessary to consider limitations related to dependence on parents or other adults for financial resources or transportation, or differences in privacy compared with adults. Given potential changes in social norms, technology, or preferred methods for social interaction for example, which can occur rapidly, formative research can be used to develop a strategy that aligns with the adolescent population’s current context. Recruitment-specific queries can be integrated into formative research being conducted for other purposes, such as focus groups or surveys to identify appropriate intervention content.\n\nYouth advisory boards (YAB) or community advisory boards (CAB) comprised of individuals who are representative of the study population also provide valuable insight [ 12 ,  13 ]. Because the lived experiences of racial/ethnic minorities are underrepresented in the existing body of literature, YABs are especially useful for identifying contextual nuance (e.g., cultural norms, racial/ethnic discrimination) that can inform the development of tailored recruitment/retention strategies. For example, YABs can identify a clinic or other venue that is not a safe space for racial/ethnic minorities, or access and utilization issues based on the geographic location of a research site, which could hinder recruitment and/or retention.\n\nLanier et al. identified key community stakeholders through online searches for youth-serving organizations, community walkthroughs, and/or referrals. Based on the study purpose, she identified individuals who were from the population of interest, worked and/or lived in those communities, had experience working directly with the population, and/or had a strong background in youth sexual health research. YAB/CAB-specific recruitment materials, such as flyers and informational sheets about the study, were developed to inform individuals about the project. Financial compensation was offered, and refreshments were served during meetings to compensate YAB/CAB members for their time and insights.\n\nGiven the focus of her study, Brawner et al. strategically recruited YAB members from the schools, mental health programs, and community-based organizations that served as research sites [ 8 ]. Recruitment materials were disseminated at the sites. She and her research team held 30-min YAB information sessions to explain the purpose and inner workings of the board and to recruit members. This approach yielded overwhelming interest from the youth. Staff at the sites also referred youth for whom they believed the YAB would be mutually beneficial. This positively affected recruitment at each site because potential participants heard about the study from their peers who were enthusiastic about the work. Of note, a key benefit of identifying YAB members from the different research sites was that they provided contextualized information in the study planning, implementation, and data analysis phases based on their unique knowledge of their specific contexts (e.g., issues that arise in schools might be different than those in mental health treatment programs). Youth who became aware of the YAB through other means (e.g., word of mouth, social media) and contacted the research team were also given information and invited to join the YAB if interested. Financial compensation and food/refreshments were also offered at the monthly, 1-hour long meetings.\n\nTable 3  provides examples of different recruitment strategies. Lanier et al. and Brawner et al. recruited from public venues (e.g., public transportation stops, local concerts) to reach a wider range of potential participants than recruiting from traditional venues (e.g., clinic, school) alone. However, challenges can arise when conducting eligibility screenings in less-contolled settings (e.g., abrupt end if the individual’s bus or train arrives, difficulty finding a private location for screening, engaging an adolescent who is with others). Yet, with appropriate planning (e.g., identifying an unoccupied seating area in advance, awareness of transit schedules), resources (e.g., pens with a phone number to call for screening later), and staffing (e.g., multiple staff members to effectively engage groups), it can be an effective strategy to consider.\n\nRecruitment via social media has become increasingly popular and has demonstrated utility for engaging populations who might be harder to reach with traditional methods [ 14 ]. Ukuku Miller et al. recruited via Facebook by creating an account for the research study and purchasing ad space. The recruitment flyer was used as the ad. Potential participants who clicked on the ad were taken to the study page that prompted them to complete the eligibility screener. At the time, Facebook utilized targeted advertising, which allowed a user to identify key target areas through zip codes. She used three zip codes within the city to ensure the ad was reaching the population of interest. Brawner et al. recruited via study accounts the team created on Facebook, Twitter, and Instagram. The recruitment flyer used eye-catching imagery with popular slogans (e.g., “Keep Calm”) to encourage youth to call the research office for additional information and to be screened. To increase visibility of the postings, the team also built relationships with social media influencers to facilitate reposting for a broader audience, tagged relevant individuals and organizations, and used trending hashtags.\n\nFor longitudinal studies, it is important to develop an effective retention strategy. Attrition can negatively affect studies, as loss of participants can lead to a smaller sample than needed for sufficient statistical power [ 15 ].  Table 4  provides tips for creating an effective retention strategy. Effort devoted in the early phases of research (e.g., proposal development, protocol development) to formulate effective strategies for recruitment and, if applicable, retention, can help maximize efficient use of time and resources, and alleviate frustration for researchers and study staff.\n\nOnce a plan is developed, selecting and training appropriate recruitment and retention staff members is essential. The best-laid recruitment or retention plan will not be effective if it is not implemented properly. Recruiters are the “face” of the research study. As a potential participant’s first point of contact with the research study, it is imperative that the recruiter makes a great first impression. Although all individuals require training, it is important to ensure that a recruiter possesses personal attributes (e.g., ability to engage, pleasant demeanor) to successfully accomplish the required tasks (e.g., approaching strangers, adhering to study protocol) before starting training. Additionally, to remove potential barriers to engagement, the recruitment/retention staff for each study discussed here was comprised of individuals from or similar to the priority population [ 7 ]. Although individuals who are not from or not demographically or culturally similar to the priority population might be effective, employing individuals familiar with participants’ lived experiences facilitated more natural engagement with participants and yielded key insights about how to address recruitment/retention challenges that arose.\n\nRecruiters play a key role in building trust with adolescents. Because participation in a research study can be inconvenient, time-consuming, or pose some risk, engaging with friendly, trustworthy recruiters is vital. Participants must be confident in accurately reporting sensitive or private information for eligibility screening or retention purposes. Further, because recruiters facilitate the consent process, each must be knowledgeable about the study aims and methods, and adherent to study protocol to ensure ethical treatment of participants. Inadequately trained recruiters can jeopardize the success of a particular study, as well as broader research and programmatic efforts. Community members likely will not distinguish one research study from another, or research from other programs or services that appear similar in nature. Ethical misconduct with adolescents can have long-lasting detrimental consequences such as loss of a community’s trust of researchers or entities offering programs and services, or loss of research funding. Additionally, positive interaction with trusted staff members encourages adolescents’ continued participation, which supports retention efforts. Consequently, it is necessary to build a well-trained recruitment and retention team.  Table 5  highlights important elements to consider when selecting and training recruitment staff.\n\nCrafting an effective recruitment or retention strategy is an iterative process. It is beneficial to begin planning early and to regularly monitor progress. A basic tracking system can capture information used to identify and address challenges before they become insurmountable. For example, when recruiters document important information (e.g., recruitment location; the number of people approached, screened, and eligible or ineligible; reasons for ineligibility; other field notes), researchers are able to identify locations or strategies with the greatest yield and identify recruiters in need of additional training or other action. Presenting a summary of this information to staff during meetings creates opportunities for further discussion to generate solutions and to celebrate successes of individuals and the team (e.g., reaching milestones), which can boost morale. Having a system in place can also ease the process of generating summaries for reports to funders or other stakeholders or preparing manuscripts or presentations.\n\nWhen possible, pilot testing recruitment strategies (e.g., location, time, materials, and methods) can be advantageous. For example, Lanier et al. documented the recruitment yield at several sites and recognized the number of adolescents screened at one site was comparable with another, but did not yield as many eligible participants. Early assessment provided the opportunity to make adjustments. Similarly, piloting strategies can help determine the best day and time for a particular location (e.g., Friday or Saturday evening near a movie theater for recruiting couples, transit stops after school). Additionally, when Lanier et al. consulted her YAB for feedback on couple-focused study materials, members noted that wording such as “romantic relationship” implied a serious and/or sexual relationship. Consequently, they believed this language could deter some youth from participating if they were not sexually active or if they did not define their relationship in that way. The language was revised to more accurately reflect how adolescents classified romantic relationships (i.e., romantic relationship was changed to dating relationship).\n\nMinors, or youth under the age of 18, require special consideration to protect their well-being. A primary consideration is obtaining parental consent. Generally, research involving minors requires the assent of the minor and parental or guardian permission. However, in many states, minors can consent to sexual and reproductive health services without parental permission [ 16 ]. Also, for highly vulnerable minors, such as homeless or runaway youth, requiring parental consent might either not be feasible or might place the youth at substantial risk or harm. Researchers should determine whether parental consent is necessary or if a waiver of parental consent is possible. One recommendation is that researchers consult with CAB members to get a better understanding of the community’s views on parental consent as well as specific challenges and opportunities to obtain parental consent. Engaging members of the community, including adolescents, parents, advocates, and adolescent service providers, in open dialogue can create opportunities to generate solutions that are in the best interest of the adolescent and that the community finds acceptable. In Lanier et al.’s study, YAB members noted that many parents might not be aware that their child/ren are currently involved in a relationship. Therefore, youth who have confidentiality concerns regarding their relationship status might decide not to participate if parental forms divulge their relationship status.\n\nResearchers should also consult with their Institutional Review Board (IRB) office for advisory guidance regarding their study. Lanier et al.’s and Brawner et al.’s institutions offered consultation hours where investigators could meet one-on-one with IRB administrators; others may allow researchers to request a meeting with the review committee prior to review of their protocol. Researchers seeking a waiver of parental consent should present a clear argument about why the study would not be feasible with parental consent [ 9 ,  17 ]. This argument should include recommendations from their YAB or CAB. Investigators should also discuss specific safe-guards in place to protect adolescents. For example, researchers recruiting adolescents at community organizations could consider appointing a child advocate within the organization who could observe and advocate on a child’s behalf.\n\nIf parental permission is needed, parents should be given adequate information about the study to make a fully informed decision about their child’s involvement. Researchers can develop supplemental materials, such as a frequently asked questions (FAQ) document, to provide information in addition to the consent form. To promote transparency and trust, Ukuku Miller et al. permitted parents to review survey items before their child completed the survey, if desired. She encouraged parents to ask questions and addressed any concerns. Additionally, all documents should be easy to understand and at an appropriate reading level. Researchers should conduct a Flesch-Kincaid grade level reading test to ensure the readability of their forms. Researchers working in communities where English is not the primary language might also consider developing parental forms in the primary language spoken within the community. Likewise, it could be helpful to have bilingual research staff members or phone-accessible translation service providers who are able to communicate directly with parents regarding the study and answer any questions.\n\nLastly, some studies exclude populations dealing with mental illnesses due to concerns regarding vulnerability and ability to complete the study procedures. While these concerns are valid, it is important to consider one’s actual ability to consent to research without undermining their decision-making autonomy by assuming that because they have a mental illness they cannot participate in research [ 17 ,  18 ]. Brawner et al. enrolled youth receiving outpatient mental health treatment. At the time of screening, trained research team members assessed potential participants’ ability to consent. The assessment included a determination of whether the individual was able to follow the screening procedures, and whether they verbally demonstrated comprehension of the study purpose and procedures. For studies involving populations with serious mental illnesses (e.g., psychosis, schizophrenia), there are also standardized measures that can be implemented to establish and document capacity to consent [ 19 ].\n\nState laws regarding the age of consent are also important. Each state has laws regarding the age at which one can legally consent to sexual intercourse. Researchers should have a clear understanding of the age of consent and reporting requirements in their respective states. Investigators who are conducting research that could include age discordant couples might consider ensuring that participants are aware that federally funded research is covered by Certificates of Confidentiality to further protect participants’ confidentiality and privacy [ 17 ].\n\nResearch involving dyads requires both individuals to meet the study eligibility criteria and jointly agree to participate. Research staff typically recruit one member of the dyad (index participant) into the study and then invite the index partner to recruit the other dyad member (nominated participant). One main disadvantage of this approach is that the full responsibility for recruiting the nominated dyad member is placed on the index participant. There are a number of concerns with this approach including the ability of the adolescent index participant to effectively communicate the study purpose and protocol to their dyad member. One effective strategy is targeting sites and venues where adolescent couples frequent (e.g., movie theaters, parks). This allows for simultaneous recruitment and screening of both couple members. The approach not only takes the burden of recruitment off of the index participant, but it also allows for immediate scheduling of the data collection appointment, which is another major barrier to dyadic research.\n\nConfidentiality is a key concern when adolescents disclose information about their sexual behaviors and practices. It is important that research staff ensure both dyad members know that their confidentiality will be maintained and any information collected will not be shared with anyone, including the other member of the dyad. In cases where confidentiality cannot be maintained (e.g., reporting incidents of abuse to parents or legal authorities), this should be clearly stated during the consent process and research staff should ensure that adolescents fully understand reporting procedures. Additionally, data should be collected in a private area. For example, Lanier et al. and Ukuku Miller et al. conducted qualitative interviews in nonadjacent rooms to prevent members of the dyad from overhearing the other interview. If possible, conducting interviews simultaneously or creating another task (e.g., filling out a survey with noise-canceling headphones) for one member to complete while the other is being interviewed can aid confidentiality. If STI/HIV testing is conducted, test results should be disclosed to each dyad member separately in a private area. In dyad studies involving romantic partners or parents and children, research staff should provide participants with strategies for sharing results with their dyad member.\n\nResearch conducted with racial/ethnic minority adolescents can be strengthened by carefully considering the historical context of racial/ethnic minority communities’ involvement in research and engagement with the US health care system. Well-documented unethical practices, such as the Tuskegee Syphilis Study [ 20 ], the Guatemala STI Experiments [ 21 ], gynecologic experiments performed on enslaved Black women [ 22 ], and the Birth Control Pill Trials in Puerto Rico [ 23 ], have helped shape individual and collective community experiences with research and medicine for Blacks and Hispanics/Latinos [ 7 ]. Such practices have had direct or indirect effects on some patients’ trust of researchers and health care providers, as well as researchers’ and health care providers’ incorrect or incomplete knowledge about racial/ethnic minority patients or study populations that adversely affects how they engage racial/ethnic minorities.\n\nConsequently, having members from or similar to the population be involved in key research study roles (e.g., investigators, study staff, advisory boards) and utilizing community-based participatory research approaches could help build trust and increase the likelihood of culturally appropriate research [ 12 ,  13 ,  24 – 30 ]. Some aspects of effective engagement strategies include (1) emphasizing transparency by ensuring community members fully understand study purposes and procedures; (2) providing community members an opportunity to voice their concerns (e.g., listening sessions), have their questions answered (e.g., community forums), and see pertinent concerns addressed in study design and implementation; and (3) selecting a study design that ensures all study participants eventually have access to an effective treatment (e.g., wait list control design). Additionally, cultural beliefs or norms regarding familial and relational structure, or decision-making practices, for example, should be well understood. For communities with strong familial ties, research with parent-adolescent dyads can be valuable. Research strategies should be grounded in feedback from the YAB and CAB.\n\nIn this article, we described approaches to recruitment and retention that have been effective in epidemiologic or intervention STI/HIV risk reduction studies conducted with racial/ethnic minority adolescents and used three studies as illustrations. Thoughtful planning for recruitment and retention, building a well-trained staff, and monitoring and improving processes throughout a study play an essential role in the successful recruitment of adolescents. Attention to planning, implementing, and monitoring recruitment and retention strategies can result in a better use of time and resources, and promote good morale and retention of study staff members.\n\nAlthough research studies were discussed, the content may also be useful for programmatic or other efforts with a similar priority population. Additionally, because we provide questions to inform a process rather than specific guidance, much of the content might be useful for other racial/ethnic minority or white adolescent populations, or studies with adults. The strategies discussed are based on experiences of several researchers conducting studies in the southern and northeastern regions of the United States, which limits generalizability. Thus, each research study or program must develop a nuanced approach that best fits the context of their study. However, using principles described here could provide a foundation or starting point.\n\nResearch with Black and Hispanic/Latino adolescents can be a challenging and complicated endeavor, but it can also yield important and valuable insights. Findings from research with Black and Hispanic/Latino adolescents provide knowledge needed to inform culturally and developmentally appropriate efforts to establish healthy practices relatively early in life, especially in the context of heightened STI/HIV risk for adolescents and young adults. Taking the time to conduct thoughtful, considerate research that directly engages adolescents and young adults is a vital part of developing effective interventions to decrease the heavy STI/HIV burden within this age group and, over time, decrease these STI/HIV disparities.",
    "mesh_query": "preventive medicine[MeSH Terms]",
    "source_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12915538/",
    "data_crawling": "2026-02-19T18:21:55.141578"
  },
  {
    "pmc_id": "12915590",
    "title": "Optimal Cut‐Off Values of the Aldosterone‐Renin Ratio in Different Age Groups for Primary Aldosteronism Screening",
    "abstract": "ABSTRACT This study sought to establish age‐specific optimal cut‐off values for the aldosterone‐to‐renin ratio (ARR) in screening for primary aldosteronism (PA). The analysis included 281 patients with confirmed PA and 1242 with essential hypertension (EH), categorized into four age groups: ≤39, 40–49, 50–59, and ≥60 years. A positive correlation was observed between ARR and age in both patient groups. Receiver operating characteristic analysis determined the optimal ARR cut‐offs to be 2.3, 2.8, 3.1, and 3.6 for the respective age groups, with all demonstrating high diagnostic accuracy. Further evaluation revealed that for patients under 50‐years‐old, a unified ARR cut‐off of 2.4 provided high sensitivity and specificity, while for patients aged 50 or older, a cut‐off of 3.7 was more appropriate. In conclusion, the optimal ARR cut‐off for PA screening increases with age, and we recommend using a value of 2.4 for patients under 50 and 3.7 for those aged 50 or older to achieve superior diagnostic performance.",
    "text": "Primary aldosteronism (PA) is characterized by hypertension, hypokalemia, and metabolic alkalosis, affecting 5%–20% of hypertensive patients [ 1 ,  2 ]. Compared to those with essential hypertension (EH), PA patients exhibit a higher prevalence of cardio‐cerebrovascular and renal complications [ 3 ,  4 ]. Early identification and appropriate intervention are therefore crucial.\n\nAccording to the 2020 Italian Practical Guidelines [ 5 ], the aldosterone‐to‐renin ratio (ARR) is widely recognized as the most reliable and effective screening test for PA [ 6 ,  7 ,  8 ]. However, ARR measurements can be influenced by several confounding factors, including age, sex, diet, sampling conditions, medications, serum potassium levels, and renal function [ 4 ,  9 ,  10 ]. While factors such as menstrual cycle, drug withdrawal, potassium supplementation, and dietary modifications can be controlled to improve ARR accuracy, age remains a major non‐modifiable source of variation. Existing evidence indicates that both plasma renin and aldosterone levels decline with age, with a more pronounced reduction in renin leading to an age‐related increase in ARR [ 11 ]. Applying a uniform ARR threshold across all age groups may lead to false‐positive or false‐negative results. Thus, establishing age‐stratified ARR cut‐off values is essential for accurate PA diagnosis. Current guidelines recommend ARR cut‐offs of 2.4, 3.7, and 4.9 (ng/dL)/(mU/L) when aldosterone and renin are measured in ng/dL and mU/L, respectively, with 3.7 being commonly adopted in clinical practice [ 7 ].\n\nThis study aimed to refine and identify optimal age‐specific ARR cut‐off values based on guideline‐recommended thresholds.\n\nThe study included patients who underwent secondary hypertension screening at the Department of Heart Failure and Hypertension, The First Affiliated Hospital of Dalian Medical University, between January 2017 and July 2024.Exclusion criteria: (1) other types of secondary hypertension, such as renal parenchymatous hypertension, severe renal artery stenosis (≥75%), Cushing syndrome, pheochromocytoma, severe OSA, and so forth; White coat hypertension, false hypertension; (2) Severe liver damage [AST or ALT exceeding three times the upper limit of normal] and severe kidney damage [estimated glomerular filtration rate, (eGFR) <60 mL/min/1.73 m 2 ]; (3) Heart failure with left ventricular ejection fraction (LVEF) <50%; (4) Women who have taken contraceptives recently or during pregnancy; (5) A history of atherosclerotic cardiovascular disease (ASCVD) within 3 months; (6) History of major surgery within 1 year; (7) History of acute infection during hospitalization; (8) History of malignant tumor; and (9) Refuse to participate in the study.\n\nBefore admission, they were asked to discontinue drugs that affect ARR, such as angiotensin‐converting enzyme inhibitors (ACEI), angiotensin receptor blockers(ARB), dihydropyridine calcium antagonists, and beta blockers, and switched to diltiazem and/or terazosin for at least 4 weeks. Prior to ARR measurements, the premenopausal women were not taking any oral contraceptives, nor were the postmenopausal women using any hormone replacement therapy.\n\nAmong the 1622 eligible hypertensive subjects, 15 patients had PAC > 20 ng/dL, DRC below measurable level, and were directly diagnosed with PA without confirmatory tests. The ARR of 510 patients ≥2.4 continued to confirm the diagnosis, and 1071 patients with ARR < 2.4 were diagnosed with EH. A total of 125 patients were diagnosed with PA by saline testing with PAC > 10 ng/dL. One hundred seventy‐one patients had PAC < 5 ng/dL and were diagnosed with EH. Among 214 patients with 5–10 ng/dL PAC, 123 patients were confirmed by captopril inhibition test, 33 patients were confirmed by AVS indicating unilateral dominant secretion. Eighty‐four patients were not included in the final analysis. Among them, 58 patients were not diagnosed with PA or EH, and 26 patients were clinically diagnosed with PA but had an ARR value less than 2.4. Finally, a total of 281 patients were included in the PA group and 1242 in the EH group (Figure  1 ).\n\nThe procedure of subject recruitment. ARR, aldosterone/renin ratio; AVS, adrenal vein sampling; DRC, direct renin concentration; EH, essential hypertension; PA, primary aldosteronism; PAC, plasma aldosterone concentration; SIT, saline infusion test.\n\nClinical data collected included age, sex, body mass index (BMI), duration of hypertension, systolic and diastolic blood pressure, and history of smoking and alcohol consumption. After fasting for at least 6 h, morning blood samples were obtained and analyzed on the same day. Measured parameters included alanine aminotransferase (ALT), aspartate aminotransferase (AST), creatinine (Cr), uric acid (UA), serum sodium, potassium, fasting blood glucose (Glu), total cholesterol (TC), triglycerides (TG), high‐density lipoprotein cholesterol (HDL‐C), low‐density lipoprotein cholesterol (LDL‐C), and ejection fraction (EF%). All serum indices, along with 24‐h urinary sodium and potassium, were measured using an automated biochemical analyzer. Direct renin concentration (DRC) and plasma aldosterone concentration (PAC) were measured after the patient had remained seated for 5–15 min, at least 2 h after waking. Measurements were performed via chemiluminescence assay (Diasorin S.P.A., Saluggia, Italy). Daily quality control procedures were implemented in the laboratory.\n\nPA was diagnosed according to established guidelines [ 7 ], based on clinical presentation (e.g., hypertension with spontaneous or diuretic‐induced hypokalemia), screening results, and confirmatory tests.\n Screening test: A positive screening result was defined as ARR ≥ 2.4 (nanograms per deciliter)/(microunits per liter), which would prompt a confirmatory test. Patients with spontaneous hypokalemia, those with undetectable renin levels, and those with PAC > 20 ng per deciliter could be directly diagnosed as PA without further confirmatory tests. Saline infusion test (SIT) [ 12 ]: After 1 h of bed rest, 2 L of 0.9% sodium chloride were infused over 4 h. DRC, PAC, and potassium were measured before and after infusion. A post‐infusion aldosterone level >10 ng/dL confirmed PA, while <5 ng/dL excluded it. Values between 5 and 10 ng/dL warranted further evaluation with either a captopril challenge test or adrenal vein sampling (AVS), particularly in patients with evident adrenal adenoma and hypokalemia. PA was also confirmed via unilateral aldosterone hypersecretion on AVS [ 9 ,  13 ]. Captopril challenge test (CCT) [ 14 ]: After 1 h in seated rest, 50 mg captopril was administered orally. DRC and PAC were measured before, and at 1 and 2 h after administration. Patients remained seated throughout. PA was diagnosed if aldosterone decreased by <30% or its absolute value remained >11 ng/dL.\n\nScreening test: A positive screening result was defined as ARR ≥ 2.4 (nanograms per deciliter)/(microunits per liter), which would prompt a confirmatory test. Patients with spontaneous hypokalemia, those with undetectable renin levels, and those with PAC > 20 ng per deciliter could be directly diagnosed as PA without further confirmatory tests.\n\nSaline infusion test (SIT) [ 12 ]: After 1 h of bed rest, 2 L of 0.9% sodium chloride were infused over 4 h. DRC, PAC, and potassium were measured before and after infusion. A post‐infusion aldosterone level >10 ng/dL confirmed PA, while <5 ng/dL excluded it. Values between 5 and 10 ng/dL warranted further evaluation with either a captopril challenge test or adrenal vein sampling (AVS), particularly in patients with evident adrenal adenoma and hypokalemia. PA was also confirmed via unilateral aldosterone hypersecretion on AVS [ 9 ,  13 ].\n\nCaptopril challenge test (CCT) [ 14 ]: After 1 h in seated rest, 50 mg captopril was administered orally. DRC and PAC were measured before, and at 1 and 2 h after administration. Patients remained seated throughout. PA was diagnosed if aldosterone decreased by <30% or its absolute value remained >11 ng/dL.\n\nThe study was conducted in accordance with the principles of the Declaration of Helsinki and approved by the Ethics Committee of The First Affiliated Hospital of Dalian Medical University.\n\nData are presented as median (interquartile range). Spearman's correlation was used to assess relationships between age and DRC, PAC, and ARR. Categorical variables were compared using the χ 2  test, and continuous variables with the  t ‐test or non‐parametric tests as appropriate. Receiver operating characteristic (ROC) curve analysis was employed to evaluate the predictive accuracy and optimal cut‐off values of ARR across age groups. A two‐sided  p ‐value < 0.05 was considered statistically significant. All analyses were performed using SPSS version 26 (IBM, USA).\n\nThe mean age was significantly higher in the PA group (51 years) compared to the EH group (44 years;  p  < 0.05). Relative to the EH group, patients with PA showed significantly higher levels of PAC, ARR, serum sodium, 24‐h urinary potassium, 24‐h urinary sodium, systolic blood pressure, diastolic blood pressure, and DRC, along with significantly lower serum potassium (all  p  < 0.05). Although TC, TG, LDL‐C, ALT, and AST were significantly lower in the PA group ( p  < 0.05), all values remained within normal ranges. Additionally, the duration of hypertension was significantly longer in the PA group ( p  < 0.05). No significant differences were observed between groups in smoking or drinking history, gender distribution, BMI, serum creatinine, UA, EF%, Glu, or HDL‐C (all  p  > 0.05) (Table  1 ).\n\nIn the univariate analysis, age, duration of hypertension, and ARR were all significantly associated with the diagnosis of PA (all  p  < 0.001). In the multivariate model adjusted for confounding factors and including interaction terms, the “age × ARR” interaction term showed a highly statistically significant result (odds ratio [OR] = 0.98, 95% confidence interval [CI]: 0.98–0.98,  p  < 0.001). Meanwhile, the interaction between “hypertension duration × ARR” did not reach statistical significance (OR = 1.00, 95% CI: 1.00–1.01,  p  = 0.222). In the multivariate model, hypertension duration itself remained an independent predictor of PA (corrected OR = 1.07, 95% CI: 1.03–1.11,  p  < 0.001) (Table  2 ).\n\nIn the EH group, ARR was positively correlated with age ( r  = 0.479,  p  < 0.05), while both DRC and PAC were negatively correlated with age ( r  = –0.546,  p  < 0.05;  r  = –0.136,  p  < 0.05, respectively). In the PA group, DRC was negatively correlated with age ( r  = –0.261,  p  < 0.05), and ARR was positively correlated ( r  = 0.201,  p  < 0.05). No significant correlation was found between PAC and age ( r  = –0.080,  p  = 0.09) (Table  3 ).\n\nPatients were stratified into four age groups: ≤39, 40–49, 50–59, and ≥60 years to evaluate trends in DRC, PAC, and ARR among PA and EH patients. In the EH group, DRC and PAC decreased significantly with advancing age ( p  < 0.05), with notable differences between patients under and over 50 years old ( p  < 0.05). Conversely, ARR increased with age and also differed significantly between these two age subgroups ( p  < 0.05). In the PA group, DRC decreased with age ( p  < 0.05), though no significant difference was observed among patients ≥50 years old. PAC did not differ significantly across age groups ( p  > 0.05) (Table  4 ).\n\nReceiver operating characteristic (ROC) curve analysis was used to determine the optimal ARR cut‐off values, Youden's index (YI = sensitivity + specificity – 1), sensitivity, and specificity for predicting PA across age groups. The optimal ARR cut‐offs were 2.3, 2.8, 3.1, and 3.6 for the ≤39, 40–49, 50–59, and ≥60 years groups, respectively. The sensitivity of these values is all greater than 94%, specificities of 94.6%, 89.4%, 73%, and 82.7%, and YIs of 0.946, 0.868, 0.67, and 0.774.\n\nAt a uniform ARR cut‐off of 2.4, sensitivities across all age groups (≤39 to ≥60 years) were 93.8%, 98.7%, 98.8%, and 100%, with specificities of 94.6%, 86.5%, 63.9%, and 68.6%, and YIs of 0.884, 0.852, 0.627, and 0.686, respectively. Using a cut‐off of 3.7, sensitivities were 71.9%, 80.3%, 84.5%, and 93%; specificities were 97.6%, 92.7%, 78.6%, and 83.2%; and YIs were 0.695, 0.73, 0.631, and 0.762. At a cut‐off of 4.9, sensitivities declined to 62.5%, 68.4%, 75%, and 66.7%, while specificities were 98.7%, 96.7%, 85.3%, and 88.5%, with YIs of 0.612, 0.651, 0.603, and 0.552.\n\nNotably, among patients <50 years old, an ARR cut‐off of 2.4 yielded the highest YI, sensitivity, and specificity, whereas for those ≥50 years, a cut‐off of 3.7 provided better diagnostic performance (Tables  5  and  6 ).\n\nROC curve analysis evaluated the best truncation, Jorden index, sensitivity and specificity of ARR as a screening test for predicting PA at different ages.\n\nROC curve analysis and evaluation of ARR = 2.4, 3.7, 4.9, the screening test of different age groups when PA Jorden index, the sensitivity and specificity.\n\nAge‐related variations in the ARR may influence the accuracy of PA screening, potentially leading to missed diagnoses or unnecessary invasive procedures in different age groups [ 15 ,  16 ]. In our study, we observed that within the EH group, both direct renin concentration (DRC) and plasma aldosterone concentration (PAC) decreased with age, with a more pronounced decline in DRC than PAC, resulting in a progressive increase in ARR with advancing age. A similar trend was observed in the PA group, consistent with previous reports [ 17 ]. Ma et al. demonstrated that using a uniform ARR cut‐off of ≥3.7 was associated with a high false‐negative rate, particularly among patients under 60 years of age. They recommended an ARR of 3.7 for patients ≥60 years, while suggesting that lower thresholds should be adopted for younger individuals [ 18 ]. A low ARR cut‐off may increase false‐positive rates [ 15 ]. Therefore, age‐specific ARR thresholds are essential for accurate PA screening.\n\nIn this study, there were significant differences in the age and duration of hypertension between patients with PA and those with EH. Patients with PA were older and had a longer duration of hypertension. To directly test whether age modulates the association between ARR and PA diagnosis, we constructed a multivariate logistic regression model that included age, duration of hypertension, ARR, as well as the interaction terms “age *ARR” and “duration * ARR.” This cohort confirmed that age is a significant modulator of the diagnostic efficacy of ARR. This finding explains from the pathophysiological mechanism why a uniform ARR cut‐off value is not applicable to all age groups: elderly patients have a baseline increase in ARR due to physiological renin decline, which makes the “PA risk signal” carried by the same ARR absolute value weaker. Therefore, adopting an age‐stratified interpretation strategy for ARR (2.4 for those <50 years old and 3.7 for those ≥50 years old) is not an arbitrary choice but the result of quantifying and calibrating the diagnostic efficacy differences of ARR with age changes, aiming to optimize the balance of sensitivity and specificity in screening across all age groups. Patients were stratified into four age categories: ≤39, 40–49, 50–59, and ≥60 years to determine the optimal ARR cut‐off for each group. ROC analysis identified optimal ARR values of 2.3, 2.8, 3.1, and 3.6, respectively, showing a progressive increase with age. In the 50–59 age group, the optimal ARR cut‐off value (3.1) corresponds to a specificity of 73%, which is much lower than that of other age groups. This may lead to a relatively high false positive rate in clinical applications. However, as shown in this study, even in patients with primary hypertension (EH), the ARR significantly increases with age (the median ARR in the 50–59 EH group has reached 1.46). This means that the “normal background value” in this age group is approaching the diagnostic range of PA, reducing the “signal‐to‐noise ratio” for distinguishing physiological from pathological increases, and thus causing a natural decrease in specificity. Therefore, for older populations, it is recommended to combine the ARR value with indicators such as blood potassium, blood pressure levels, and imaging to interpret the results, in order to improve the screening efficiency. To enhance clinical applicability, we also evaluated guideline‐recommended values. At ARR ≥2.4, sensitivity reached 93.8% in the ≤39‐year group, while specificity was 94.6% and 86.5% in patients under 50 years, and 63.9% and 68.6% in those ≥50 years, indicating an elevated risk of false positives among older individuals. At ARR ≥3.7, sensitivity among patients ≥50 years was 84.5% and 93%, with specificity of 78.6% and 83.2%; meanwhile, in patients under 50 years, sensitivity decreased to 71.9% and 80.3%, with specificity values of 97.6% and 92.7%, suggesting a higher risk of false negatives in younger populations. The corresponding YI for the four age groups at ARR = 2.4 and 3.7 were (0.884 vs. 0.695), (0.852 vs. 0.730), (0.627 vs. 0.631), and (0.686 vs. 0.762). In conjunction with guideline recommendations, our findings support an ARR cut‐off of 2.4 for patients <50 years and 3.7 for those ≥50 years.\n\nOur conclusion that “elderly patients require a higher ARR cutoff value to optimize specificity” is highly consistent with the findings of Luo et al. [ 15 ]. Their study showed that using a universal threshold would lead to a significant decrease in specificity among individuals aged 50 and above, while raising the cutoff value to 25 (ng/dL)/(ng/mL/h) (approximately 3.0 (ng/dL)/(mU/L)) could improve diagnostic efficiency. This supports our practice of applying a higher threshold (such as 3.7 (ng/dL)/(mU/L)) in the elderly population. However, there is a significant divergence among different studies regarding whether age should modify the ARR cutoff value. Yin et al. [ 8 ]argued that there was no significant difference in the optimal cutoff value between the <40‐year‐old and ≥40‐year‐old groups, and they advocated for no age stratification. While Peng et al.’s [ 19 ] latest study, based on direct renin concentration (DRC), concluded that the optimal cutoff values for each age group (17.49–21.01 (pg/mL)/(μIU/mL)) did not increase with age and were all lower than the commonly accepted consensus threshold, suggesting that a relatively lower unified threshold might be applicable. Meanwhile, gender is a key variable that cannot be ignored. The research by Solanki et al. [ 20 ] provides strong evidence for this. They found that among individuals aged 20–39, the median aldosterone level for women was 51% higher than that of men, and the renin level was 38% lower, resulting in an increase of 101% in ARR, despite the lower blood pressure in women. This strongly suggests that the observed increase in ARR in young women may be physiological, and using a uniform threshold would lead to an increase in the false positive rate for this population. Moreover, even among individuals aged 60 and above, the ARR for women was significantly higher than that for men. This study clearly indicates that the ARR detection based on DRC particularly requires the establishment of age‐ and gender‐specific reference ranges. Despite these differences, all the studies still reveal important commonalities and clinical implications: In EH patients: Renin and aldosterone both decrease with age, and the decrease in renin is more significant, resulting in a pseudo‐hyperactivity of ARR with age. This is the fundamental reason for the increase in false positives in the elderly population. Ma et al. [ 18 ] also advocated for a cut‐off of 3.7 however, in their study, 18% of PA patients (24/135) had ARR values below this threshold. Moreover, 209 patients with ARR <3.7 were classified as EH without confirmatory testing, potentially underestimating the sensitivity of this cut‐off. Although some studies propose lower ARR thresholds for patients under 60 years, such values may increase the need for additional testing and have not been widely adopted. Despite variations in proposed age‐specific ARR values across studies, a consensus exists that a single uniform threshold is inadequate, and higher cut‐offs should be applied in older populations. Our study provides clinically practical values aligned with guideline frameworks, thereby potentially improving diagnostic efficiency. Further prospective validation is warranted. The exact numerical value needs to be determined based on the local population, detection methods and clinical verification. In the future, large‐scale, prospective, multi‐center studies are urgently needed. Under the condition of strictly controlling confounding factors (including menstrual cycle, drugs, salt intake, etc.), a reference range for ARR based on age and gender should be established, and its effect on the accuracy of PA diagnosis and clinical outcomes should be evaluated, so as to truly achieve individualized precise screening.\n\nThe PA patients included in this study constituted 17.6% of the total cohort, consistent with established prevalence rates and supporting the representativeness of our sample. In this study, the guideline‐recommended cut‐off value (ARR ≥ 2.4) commonly used in clinical practice was adopted as the starting point for the diagnostic process. However, subsequent clinical follow‐up and spironolactone testing revealed that a small proportion of these patients had features mimicking primary aldosteronism, including adrenal adenomas or hypokalemia. In our study cohort, 26 patients with an ARR value less than 2.4 were clinically suspected of having primary aldosteronism. The age range was from 33 to 72 years old, and patients under 50 years old accounted for 65.4% of this group, with the ARR of 1.47 (IQR: 0.93–1.87) was observed at the median (Supporting Information  1 ). Previous studies have shown that renal artery stenosis [ 21 ,  22 ], accessory renal arteries [ 23 ], and chronic kidney disease [ 24 ] can increase renin levels and leading to an underestimation of the PA with ARR < 2.4 in this study. Among the 26 patients, the pathological findings comprised renal artery stenosis (5 cases), accessory renal arteries (14 cases), and chronic kidney disease (7 cases). In the context of these concomitant pathologies, the ARR appears to be independent of age. However, this study does not attempt to establish a brand‐new cut‐off. Instead, it shows how to adjust the interpretation of results by age, using 2.4 as the starting point in the current diagnostic process. We acknowledge that this design may lead to a potential bias in the definition of the “non‐PA” (EH) group, especially possibly missing some PA patients with ARR values slightly lower than 2.4 (i.e., false negatives). For such patients, confirmatory testing may be considered when strong clinical features such as hypokalemia or adrenal nodules are present. Additionally, Ng et al. highlighted significant intra‐individual variability in ARR, noting that over one‐third of PA patients had at least one ARR measurement below the diagnostic cut‐off [ 17 ]. Thus, repeated ARR measurements are recommended to reduce false negatives.\n\nThis study is a retrospective investigation and may have selection bias. All patients included in this study were from the hypertension specialty department of a large tertiary medical center and usually had more refractory, complex conditions or were suspected of having secondary hypertension. This may lead to a more severe condition in the hypertensive patient group in this study compared with the general community hypertensive population, thereby potentially overestimating the prevalence of primary aldosteronism (PA) or the overall level of the aldosterone‐to‐renin ratio (ARR). Fifty‐eight patients with saline infusion test (SIT) results in the indeterminate range (5–10 ng/dL) were excluded, which may have affected the accuracy of ARR measurement. Second, the number of eligible participants over 50 years of age was relatively small compared with the younger group. Finally, in this study, only one ARR screening was performed after adequate medication adjustment before conducting confirmatory tests, and the individual variability of ARR was not fully considered.\n\nOur findings clarify the impact of age on the ARR as a screening tool for PA and support the use of a higher ARR cut‐off value in patients aged 50 years and older. In alignment with current guidelines, we recommend an ARR threshold of 2.4 for individuals under 50 years old, and a value of 3.7 for those aged 50 years or older. It is important to note that these age‐stratified ARR cut‐off values are primarily used as an initial tool to enhance screening efficiency. In clinical practice, the interpretation of ARR must be combined with the specific circumstances of the patient for individualized assessment.\n\nThis study was performed in line with the principles of the Declaration of Helsinki. Approval was granted by the Ethics Committee of The First Affiliated Hospital of Dalian Medical University.\n\nThe Ethics Committee judged that written informed consent for patients was not required since there was no given the non‐intrusive and non‐experimental character in this study.",
    "mesh_query": "public health[MeSH Terms]",
    "source_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12915590/",
    "data_crawling": "2026-02-19T18:21:57.915645"
  },
  {
    "pmc_id": "12916085",
    "title": "Cost-effectiveness of One-Time Universal Childhood Hepatitis C Screening in the United States",
    "abstract": "Background: Hepatitis C Virus (HCV) in pregnancy has increased, leading to increased perinatally exposed infants. Although universal HCV screening in pregnancy is recommended, pediatric cases remain undiagnosed. We examine the cost-effectiveness of universal HCV screening among children at age 2 and 10, when other routine blood testing is recommended. Methods: An HCV natural history Markov model evaluated the cost-effectiveness of universal HCV screening independently at ages 2 and 10 compared to the currently recommended risk-based screening of children born to those with HCV. Based on previous literature, we assumed a 0.05% pediatric HCV chronic prevalence (0.73% chronic prevalence among pregnant persons and 7.2% vertical transmission). In the status-quo scenario, we assumed 23% of children with prenatal HCV exposure were screened. We assessed costs (USD), quality-adjusted life years (QALYs), and the incremental cost-effectiveness ratio (ICER, $ per QALY gained) compared to a willingness-to-pay threshold (WTP) of $50,000/QALY. We explored parameter uncertainty, including pediatric HCV chronic prevalence and screening rates, in multiple sensitivity analyses. Results: Universal HCV screening at age 2 was cost-effective (ICER=$8,774/QALY gained) compared to the status-quo risk-based screening. The lowest pediatric HCV chronic prevalence in which universal screening remained cost-effective under a WTP of $50,000/QALY was 0.007%. At age 10, universal screening was cost-effective compared to risk-based screening (ICER=$4,404/gained) and was cost-effective at the lowest HCV prevalence in children of 0.006%. Models at both age 2 and 10 were robust to sensitivity analyses. Conclusions: Universal HCV screening in childhood is cost-effective. Guidelines should consider recommending universal screening nationally, particularly if it can be conducted along with other routine pediatric blood draws.",
    "text": "Chronic hepatitis C virus (HCV) in the United States has increased substantially over the last two decades among reproductive-aged people. 1  This has led to a corresponding increase in HCV in pregnancy and infants that are perinatally exposed. From 2000-2019, HCV infection in pregnancy increased ten-fold. 2  Approximately 7-9% of infants perinatally exposed to HCV will develop HCV infection and 60-75% of these infants will develop a chronic infection. 3 – 5  Direct-acting antiviral treatment for HCV is now widely available and approved in children starting at 3 years of age.\n\nRecognizing rising rates of HCV, the Centers for Disease Control and Prevention (CDC) released guidelines in 2020 recommending universal testing among adults over age 18 as well as in all pregnant people during each pregnancy; 1  the US Preventive Services Task Force (USPSTF) also released recommendations in 2020 for universal screening among all persons aged 18-79, including pregnant people. 6  Despite these recommendations, there are still significant gaps in testing and diagnosis. Following universal screening recommendations, it is estimated that only 39-45% of pregnant people are tested for HCV. 7 , 8\n\nSignificant gaps also remain in identifying children with perinatal HCV infection. Most children with perinatal HCV exposure do not complete postnatal HCV testing. A recent retrospective cohort study utilizing an electronic medical record database including 133 million patients across 55 healthcare organizations found that between 2010-2020, fewer than half of children with perinatal HCV exposure completed any HCV testing and only 42.1% of those completed appropriate testing. 9  In 2023, the CDC released updated recommendations for HCV testing among perinatally exposed infants. These guidelines recommend testing for all perinatally exposed infants with HCV RNA at 2-6 months of age and, for children with positive HCV RNA, referral to subspecialty care. 7  These updated guidelines shorten the time between birth and recommended testing, but the challenge of communicating prenatal HCV exposure to pediatric providers remains, which has likely, in part, accounted for historically low rates of pediatric testing. 10\n\nGiven gaps in testing and awareness of infection both in pregnant people and their infants, universal screening of children may offer an opportunity to identify children perinatally exposed to HCV with risk of chronic HCV infection. Prior studies have estimated the cost-effectiveness of universal screening specifically in adolescence in order to account for later life exposure to HCV, but to date there has been few evaluations of universal screening approaches in younger children. 11 , 12\n\nWe sought to estimate the cost-effectiveness of universal HCV screening among children when performed during routine pediatric care at ages 2 and 10, at times in which routine blood testing is already recommended.\n\nWe assessed the cost-effectiveness of universal HCV screening conducted at age 2 and age 10 compared to the current status-quo which entails risk-based screening of children born to individuals with known chronic HCV. Age 2 was selected to correspond with routine lead and iron for anemia testing recommended by the American Academy of Pediatrics (AAP) to be performed at ages 1-2. 13  Age 10 was selected to correspond to cholesterol screening routinely performed as recommended by the AAP between ages 9-11. 13  We used a healthcare provider perspective 14  and a lifetime horizon (100 years) in both models. Research is reported in line with the Consolidated Health Economic Evaluation Reporting Standards (CHEERS) Statement.\n\nA closed cohort, HCV natural history Markov model of HCV disease progression and treatment was adapted from a previously published model of universal HCV screening during pregnancy 15  ( Figure 1 ). In the pediatric model of screening at age 2, we assumed all children were screened and were either not infected (antibody negative), spontaneously cleared their acute infection (antibody positive, RNA negative), or were chronically infected (antibody and RNA positive). If chronically infected, we assumed all children started at fibrosis stage 0 (F0). We assumed treatment was received within one year of diagnosis in both the universal and risk-based scenarios. In the model at age 10, fibrosis distribution at age 10 was calculated based on childhood fibrosis progression rates with no treatment 16  ( Table 1 ). All other assumptions and parameters remained the same as the model at age 2.\n\nCosts, as measured in 2023 US dollars (USD), and health utilities, as measured by quality-adjusted life years (QALYs), were attached to each health state. Both costs and health utilities were discounted by 3% per year. To account for parameter uncertainty, we used a probabilistic uncertainty analysis where parameters were randomly sampled from probabilistic distributions ( Table 1 ) for 10,000 parameter sets. We reported the mean and 2.5-97.5% intervals of cost and health utilities across model runs. The mean incremental cost-effectiveness ratio (ICER, $ per QALY gained) was calculated by diving the mean incremental costs by the mean incremental QALYs for the universal screening compared to the status-quo risk-based screening. We assessed cost-effectiveness under a willingness-to-pay threshold of $50,000/QALY and $100,000/QALY. 17\n\nWe assumed a chronic HCV prevalence among pregnant persons of 0.73% (95% CI=0.71-0.75%) 18  and a vertical transmission rate of 7.2% (95% CI=5.6-8.9%). 3  This results in a pediatric chronic prevalence of approximately 0.05%. Given a 33.6% spontaneous clearance rate (95% CI=17.0-51.8%), 4  we assumed an antibody HCV prevalence in children to be approximately 0.08%.\n\nWhile universal HCV screening of pregnant persons is now recommended in the US, it is estimated that only 45% (95% CI=25-65) are screened. 7  Similarly, estimates range for HCV screening among children born to those with HCV, and we used a conservative estimate of 23% (95% CI=18-26%). 19  In the risk-based scenario, we calculated the proportion of HCV-infected children diagnosed from rates of screening during pregnancy and children born to persons with known HCV, resulting in approximately 9.9% children with HCV diagnosed in the risk-based scenario.\n\nFibrosis stage-specific transition rates among children and adults were obtained from a published meta-analysis of progression rates for chronic HCV 16  ( Table 1 ). For individuals in F0-F3 who were treated and attained a sustained virologic response (SVR), we assumed no further fibrosis progression. For individuals in F4 or later who achieved SVR, we assumed continued progression but at a lower rate than those with HCV  20 , 21 . Non-HCV related mortality rates (background mortality) was time-varying by age and obtained from the World Health Organization life tables. 22  HCV and liver-related mortality rates associated with F4, decompensated cirrhosis (DC), and hepatocellular carcinoma (HCC) were obtained from published studies. 23\n\nBased on published studies, 24  we assumed 95% of those treated with direct-acting antivirals (DAA) achieved SVR for all genotypes.\n\nThe costs of HCV antibody and RNA confirmatory testing was obtained from the 2023 Clinical Lab Fee Schedule by the U.S. Centers for Medicare and Medicaid Services 25  ( Table 1 ). We assumed all individuals are screened with an HCV antibody test, followed by an RNA confirmatory test if the antibody test is positive. As we choose ages 2 and 10, when other routine blood testing is conducted, 13  we did not incorporate additional outpatient visit or consultation costs for testing into the model.\n\nBased on the 2023 Federal Supply Schedule, 26  we used a wholesale acquisition cost for DAA of $55,000 per treatment course of sofosbuvir/velpatasvir for pediatric patients <17 kg. These costs were similar for <30 kg and we used the $55,000 treatment cost in models at both age 2 and 10. Due to the substantially different costs of a pediatric course of glecaprevir/ pibrentasvir of $15,800 (Roxann Stubbs/Abbvie, email communication, September 2024), we used the higher cost of sofosbuvir/velpatasvir in the base models and conducted a sensitivity analysis with the lower costs. Similarly, in the model at age 10, we conducted a sensitivity analysis of increased sofosbuvir/velpatasvir treatment costs for those >30 kg of $105,000. 26  Additional treatment delivery costs (pre-treatment and on-treatment monitoring) were included in all costs based on Infectious Diseases Society of America (IDSA) guidelines. 15\n\nHealth utilities, as measured in QALYs, were obtained from published studies 27 , 28  ( Table 1 ). Consistent with previous studies, 15 , 29  we incorporated an incremental increase of 0.05 health utilities for those who achieved SVR.\n\nNumerous sensitivity analyses were conducted with the model at both age 2 and age 10. In both models, we performed a one-way sensitivity analysis varying the HCV chronic prevalence in children. We also conducted one-way sensitivity analyses in both models to examine changes in the ICER with: increased HCV screening uptake to children born to HCV-infected mothers in the risk-based scenario (23% to 75%) given increased testing within the first year of life; increased uptake of HCV testing in pregnancy to be comparable to prenatal screening rates of hepatitis B virus (45% to 85%); 41  lower DAA treatment costs ($55,000 to $15,800) for a pediatric course; lower fibrosis progression in children from F0-F1 and F1-F2 to match rates of adults ( Table 1 ); increased adult HCV treatment rate from 35% to 65% given higher reported rates among those with private insurance; 42  varying discounting rates for costs and health utilities (3% to 0% and 6%); and shorter time horizons (lifetime horizon with 100 years to 50 and 20 years). In the model at age 2, we conducted an additional sensitivity analysis with varying SVR rates of 80-100%, given some evidence of lower treatment efficacy among those aged 5 or younger. 43 , 44  In the model at age 10, we conducted an additional sensitivity analysis increasing DAA treatment costs to $105,000 to consider the higher costs per treatment course among those >30kg. 26\n\nCompared to the status-quo risk-based scenario, universal HCV screening at age 2 was associated with a mean incremental cost of $20.45 (2.5-97.5% interval=$7.16-27.90) and mean incremental QALYs of 0.002 (2.5-97.5% interval=0.001-0.004) per child screened ( Table 2 ). Universal pediatric HCV screening was cost-effective with an ICER of $8,773.71 per QALY gained, falling below the WTP of $50,000 per QALY. The ICER remained below this WTP in 99.6% of stimulations ( Supplemental Figure 1 ).\n\nHCV universal screening in children remained cost-effective for HCV pediatric chronic prevalences at or above 0.007% (ICER= $48,102.59/QALY gained) under a $50,000 WTP threshold and 0.004% (ICER=$82,354.75/QALY gained;  Figure 2 ) under a $100,000 WTP.\n\nResults were robust to all sensitivity analyses ( Figure 3 ;  Supplemental Table 1 ). Universal screening remained cost-effective with increased pediatric screening in the risk-based scenario, increased screening in pregnancy, increased adult treatment rates, lower DAA treatment costs, lower F0-F1 and F1-F2 progression rates in children, and varying SVR achievement rates for those age 5 or younger. Only the sensitivity analysis examining a 20-year time horizon, compared to 100 years in the base model, was not cost-effective under a $50,000 WTP (ICER=$61,008/QALY gained).\n\nCompared to the status-quo risk-based scenario, universal HCV screening conducted at age 10 was also cost-effective (ICER=$4,404.49/QALY gained) under a WTP of $50,000 for 100% of the stimulations ( Table 2 ;  Supplemental Figure 2 ). Universal screening at age 10 was associated with a mean incremental cost of $10.76 (2.5-97.5% interval=$-3.03-18.81) and mean incremental QALYs of 0.002 (2.5-97.5% interval=0.001-0.004) per child. Universal screening remained cost effective for an HCV pediatric prevalence at or above 0.006% (ICER= $49,136.74/QALY gained) under a $50,000 WTP and 0.003% (ICER= $99,201.33/QALY gained) under a $100,000 WTP ( Figure 2 ).\n\nSimilar to the model at age 2, results of the model at age 10 were robust to all sensitivity analyses ( Figure 3 ;  Supplemental Table 2 ). Universal screening remained cost-effective with all varying parameters, including increased DAA costs to $105,000 (ICER=$10,323/QALY gained). Only the analysis examining a 20-year time horizon was not cost-effective under a $50,000 WTP (ICER=$54,501/QALY gained).\n\nOur cost-effectiveness analysis found universal HCV screening to be cost-effective in the pediatrics population, when screening was performed either at 2 years of age or at 10 years of age, time points during which routine bloodwork is already being recommended by the AAP. 13  We found universal HCV screening to be cost-effective unless pediatric HCV prevalence was less than 0.004%. Our findings were robust to multiple sensitivity analyses including varying HCV testing rates in pregnancy, rates of adherence to risk-based screening recommendations, DAA treatment costs, fibrosis progression rates, and adult DAA treatment rates. These findings add to the body of cost-effectiveness studies that have been conducted that currently support universal HCV screening in the adult population. 45  Furthermore, the demonstrated cost-effectiveness in our study helps to complete the cost-effectiveness assessment throughout the perinatal-pediatric cascade of care, by adding to the recognized cost-effectiveness in pregnant people. 15 , 46  Our findings, coupled with prior work evaluating cost-effectiveness among adolescents, 11  should inform future guidelines for HCV testing in the pediatric population.\n\nTo our knowledge, our study is one of the first to evaluate the implementation of universal HCV screening in the pediatric, non-adolescent population. 12  Prior research has confirmed that universal screening in adults is cost-effective which contributed to updated recommendations by the CDC and USPSTF. 6 , 47  Furthermore, Assoumou et al. 11  evaluated cost-effectiveness of one-time HCV screening in adolescents specifically, evaluating testing at one time point among 15- to 30-year-olds seen at urban community health centers by looking at three different testing scenarios. Per their assessment, they found routine testing to be cost-effective when the prevalence of people who inject drugs among adolescents was greater than 0.59%. Of note, at the time Assoumou et al.’s study was performed, estimated costs of DAA treatment was significantly higher (up to $137,820 per treatment course) compared to the current estimated treatment cost included in our model, which may have contributed to a higher barrier to cost-effectiveness. Probabilistic sensitivity analysis which evaluated different test results, test characteristics, treatment cost/efficacy and quality weights was performed to evaluate different testing/outcome scenarios, although aspects such as reinfection rates as well as selective availability of reflex testing may not be relevant to a contemporary pediatric population under age 15, as is the case in our study. We similarly performed multiple sensitivity analyses but evaluated additional factors such as HCV testing rates in pregnancy, adherence to risk-based screening guidelines, and adult HCV treatment rates to complement assessment of cost-effectiveness specifically in a pediatric population. Our model was robust to these sensitivity analyses, finding that even when adherence to prenatal screening and pediatric risk-based screening guidelines were improved, universal testing still remained cost-effective.\n\nFindings from our study have the potential to change guideline recommendations. The current guidelines for HCV in children are risk-based and recommend screening of infants born to people with HCV. 48 , 49  Prior recommendations focused on testing for HCV antibodies at 18 months of age, and more recent CDC recommendations have recommended testing at 2-6 months of age, in an effort to increase testing rates. 49  However, testing is largely incomplete – with evidence clearly showing that less than half of the infants born to persons with HCV are being tested adequately. 50  Given testing is cost-effective universally, there is the potential to increase HCV detection and linkage to care among all affected infants and children, even if maternal risk is underrecognized.\n\nWhile universal HCV screening recommendations would increase uptake and diagnosis in the pediatric population, achieving universal guideline adherence may face substantial implementation challenges. Given limited awareness among pediatric providers, inadequate insurance coverage, retention issues as children transition in care settings, and HCV stigma, universal pediatric screening may be especially difficult to implement. 51 – 53  As we modeled in our study, pairing HCV testing with other pediatric blood testing, such as lead or cholesterol testing, and at well-child visits may be advantageous and increase HCV screening uptake. However, these tests also have relatively low uptake. 54 , 55  Implementation science methods to identify setting-specific barriers and facilitators and develop evidence-based implementation strategies, such as age-specific electronic health record prompts, community and school-health screening programs, and parent education, are needed. 52 , 56 , 57  Moreover, increasing awareness of updated HCV screening guidelines among pediatricians and other family health providers is essential. 50 , 57 , 58\n\nIncreasing uptake of pediatric HCV screening leads to increased diagnosis and treatment needs, in which access varies greatly. For instance, rural counties often have disproportionate maternal HCV rates, 59  and may require expansions of treatment programs and infrastructure if universal pediatric HCV screening were implemented. With limited access to pediatric hepatologists or infectious disease specialists that are needed to treat HCV in pediatric populations, alternative strategies, such as telemedicine and training rural pediatricians and primary care providers in HCV treatment, have been suggested. 60 , 61  In particular, telemedicine in rural areas increases access to specialists and has led to increased HCV treatment and viral clearance in adult populations 62  and increased healthcare retention and improved health outcomes in pediatric patients with other conditions. 63  Reducing barriers in healthcare settings is critical to receiving timely HCV testing and treatment in children.\n\nOur study has a few limitations. First, a key limitation is uncertainty in parameters, which we explore through extensive sensitivity analyses. For example, the prevalence of HCV in children at the ages we assess is uncertain and likely varies by setting – however our threshold analyses identified the threshold prevalence which could ensure cost-effectiveness; this could be used to guide setting-specific recommendations. Additionally, we recognize that the recent change in recommendations for testing among infants born to people with HCV as well as recent FDA approval of point of care RNA testing for HCV for adults could increase the likelihood that pregnant persons with HCV are identified and that their children are screened. However, our sensitivity analysis indicates that universal screening in children is cost-effective even with substantial increase in the efficacy of risk-based pediatric screening and prenatal screening. Finally, we examine the potential cost-effectiveness of a universal screening program but acknowledge that real-world implementation of universal screening has yielded less than universal screening uptake. If implemented, future studies could examine the real-world cost-effectiveness of such a program. Additionally, we do not account for any reinfection or transmission risk in our models, and future studies could assess this. Limitations notwithstanding, this is the first pediatric cost-effectiveness study to specifically evaluate HCV testing at routine pediatric visits with bloodwork included, which provides a practical path to implementation in a cost-effective and convenient approach.\n\nOur findings should guide implementation of cost-effective HCV testing among children in routine pediatric care. Implementation of HCV testing can increase HCV detection and linkage to care, as well as lead to reduction in the risk of liver disease progression in children due to earlier HCV recognition. Future work should focus on evaluating how best to implement universal HCV testing across health systems, particularly in higher prevalence HCV settings.",
    "mesh_query": "mass screening[MeSH Terms]",
    "source_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12916085/",
    "data_crawling": "2026-02-19T18:22:04.375473"
  },
  {
    "pmc_id": "12915870",
    "title": "Evidence from Europe on implementation, participation and performance of self-collection for cervical cancer screening",
    "abstract": "ABSTRACT Cervical cancer screening programs reduce the number of cervical cancer cases and deaths, but the success of any screening program is dependent on high participant uptake and coverage and many European countries are observing declining cervical cancer screening coverage to below national targets. Self-collection of vaginal samples for human papillomavirus testing, also termed self-sampling, is one strategy which is being introduced to try to increase screening coverage by removing barriers to participation and it has attracted growing interest and support globally. Informed by peer-reviewed and gray literature, this narrative review starts with a case study from the Netherlands and outlines the self-collection landscape in Europe within the themes of program implementation and relative test performance. It highlights some of the current evidence gaps needed to inform policy decisions on the use of self-collection within screening programs.",
    "text": "Cervical cancer screening programs have reduced the number of cervical cancer cases and deaths by identifying and treating precancerous cervical lesions [ 1 , 2 ]. However, the success of any screening program is dependent on participant uptake and coverage. Many countries are observing declining cervical cancer screening coverage to below national targets [ 3–10 ]. Self-collection of vaginal samples for HPV testing, also termed self-sampling, is one strategy being introduced to try to increase screening coverage by removing barriers to participation whether social, structural or personal [ 11 , 12 ], particularly in individuals who are not being screened according to schedule, a group which has a higher risk of high-grade cervical intraepithelial neoplasia (CIN2+) and cancer [ 13–15 ]. Sample self-collection for HPV testing has attracted growing interest and support globally, fueled in part by experiences during the COVID pandemic, since it provided a practical solution to minimize in-person appointments [ 16 ]. HPV self-collection is now recommended by the World Health Organization as an additional option for cervical cancer screening programs to reach the 2023 coverage target [ 17 ].\n\nInformed by peer-reviewed and gray literature, this narrative review aims to outline the self-collection landscape in Europe. It highlights some of the current evidence gaps needed to inform policy decisions on the use of self-collection within screening programs.\n\nThere are different ways in which self-collection can be offered within the routine screening schedule or opportunistically to targeted groups. There are also different options for how self-sampling is provided in programs and for the screening pathway (algorithm) used.\n\nIn countries without established screening programs [ 18 ], self-collection can support the scale-up of HPV testing to meet global targets [ 17 ]. For established, effective cervical screening programs, HPV self-collection has the potential to increase access and coverage and reduce screening costs [ 16 , 17 ]. The priorities are to widen participation, optimize program cost–effectiveness, and maintain coverage, without reducing overall effectiveness. These priorities raise challenges to implementing self-collection in high-uptake settings, as highlighted by Rebolj et al. [ 19 ].\n\nIn Europe to date, six countries have implemented self-collection within their screening programs, with several other countries offering it within pilot schemes ( Table 1 ). As some screening programs consider whether to introduce self-collection, now is an ideal time to evaluate the impact of introducing self-collection in countries where there is already high screening coverage of in-clinic cervical cancer screening.\n\nCountries in Europe offering self-collection within their national routine cervical cancer screening programs and pilot programs.\n\nFrom 2022. Before this, self-collection was only offered to those who did not respond to a screening invite (underscreened).\n\nA provisional policy allowing self-collection was passed in Sweden during the COVID-19 pandemic.\n\nSelf-collection kits typically include a collection device, transport tube with or without media, packaging and instructions for use plus a return postage envelope allowing the sample to be sent by post to the laboratory for testing. Kits can be mailed to people’s homes or given opportunistically within a community outreach setting or a clinical setting, such as primary care, to allow for self-collection supported by a healthcare professional on-site or privately in the clinic or at home.\n\nThere are two general approaches used for sample self-collection: dry self-collection without media, which is sent dry in the mail and suspended in a buffer in the laboratory, or wet self-collection, in which the sample is resuspended in media at the time of collection. The collection of first-void urine for self-sampling is an alternative option being explored [ 32 , 33 ] but has yet to be used in any national cervical screening programs.\n\nSelf-collection can be offered as opt-in or opt-out. In an opt-in strategy, women are typically sent a letter inviting them to screen, but they must then respond (online, by telephone or email) to receive a self-collection kit. In opt-out strategies, without being requested, kits are given in person or sent in the post with a letter inviting the woman to screen, either as part of systematic or opportunistic screening.\n\nThe Netherlands has had a national cervical screening program since 1996 [ 34 ]. In 2017, the program switched from cervical cytology to HPV primary screening and simultaneously introduced the option of self-collection for non-responders, becoming the first European country to introduce self-collection. As an early adopter with 6 years of real-world data published annually [ 35 ], it allows us to examine the impact of introducing self-collection on disease detection, loss to follow-up (LTFU) and screening uptake in a high-coverage setting.\n\nScreening in the Netherlands is offered once every 5 years to women aged 30–60 (with a 10-year interval for women who test high-risk HPV [hrHPV] negative at ages 40 and 50). Initially, self-collection kits had to be requested online or by telephone or email (opt-in). Since 2022, self-collection has been offered as an option at the initial screening invite for all screen-eligible individuals, instead of only to non-responders. In July 2023, the program transitioned to an ‘opt-out’ approach. At age 30, when first offered screening, participants are sent an HPV self-collection kit. For all subsequent screens (from age 35), they are offered the choice (via a letter) of clinician-collected sampling or receiving a self-collection kit (opt-in). If they do not respond to the letter, they are sent a self-collection kit and a repeat invite (opt-out) [ 36 ].\n\nData from the screening program indicates that the overall screening uptake has not increased since the introduction of self-collection (see coverage data presented in  Figure 1 ). Between 2012 and 2015, the uptake (i.e., the proportion of people offered a screen who accepted it) was high (65%) and stable [ 37 ], but it dropped to 60% in 2016 [ 28 ], possibly due to ongoing changes to the program’s organization in preparation for the national roll-out of HPV testing [ 34 ]. Since 2017, the participation rate has remained <60% each year [ 35 ], but the 5-year coverage (i.e., the percentage of women with at least one screen during the past 5 years) was above 70% [ 38 ] until 2020, when screening was temporarily paused due to the COVID-19 pandemic, which reduced the absolute number of women invited and screened.\n\nScreening uptake in the Dutch cervical screening program since the introduction of self-collection in 2017.\n\nData for 2022 were taken from the screening surveillance report published in 2023 (40). Data for 2021 were taken from the annual surveillance report published in 2022 (36) and data for 2020 and earlier were taken from the 2021 report (38).  1 Due to the COVID-19 pandemic, the screening programme was temporarily paused in March 2020 and fewer people were invited and participated during that year. The programme was restarted slowly from July 2020.  2 The participation rate is calculated as n/N: where n is the number who participated within 15 months of the start of their invitation year, and N is the number invited to screen.  3 Women eligible for screening who have participated in screening during the five years before the end of the reporting period.\n\nAlthough the participation rate (screening coverage) has not increased, the proportion of women choosing self-collection did, from around 7% when it was first introduced, increasing to 19.5% in 2020 amid the COVID-19 pandemic (when in-person appointments were minimized) [ 35 ]. In 2022, 22.2% of women screening chose self-collection [ 39 ]. One unexpected finding was that the percentage of people positive for hrHPV was consistently at least 1.5% lower in self-collected samples than in clinician-collected samples (within each age group, in years 2017, 2018 and 2019) [ 38 ]. While still meeting international screening criteria, the difference in hrHPV positivity is attributed to a higher cycle threshold (Ct) value in PCR-based HPV testing of self-collected samples than in clinician-collected samples. This difference in Ct value (which refers to the number of PCR cycles needed and relates to the amount of viral material present in the sample) results in lower sensitivity and higher relative specificity for CIN3+ detection in self-collected samples compared with clinician-collected samples [ 40 ].\n\nCervical screening data from the Netherlands has been carefully analyzed [ 34 , 38 , 41 ]. A detailed analysis of screening participation in those who regularly screen and those who do not indicates that there are multiple reasons for the slight fall in participation rate since 2017 ( Figure 1 ) [ 34 ]. Specifically, a change to invitations being issued from the screening organization rather than the GP and the requirement to bring the invitation letter to appointments or order a self-collection kit from a website presents barriers to participation. In line with previous studies, socioeconomic factors impact the uptake of screening, with lower-income and migrant people being less likely to access screening [ 34 ]. Aitken et al. observed that a higher proportion of not previously screened women chose self-collection compared with previously screened women (15.5% vs. 5.5%, respectively) [ 41 ].\n\nSurveillance reports from the screening program provide useful data on LTFU at different points in the screening pathway and show that LTFU differs considerably for self and clinician-collected pathways [ 35 ]. Clinician-collected samples can be tested directly using liquid-based cytology (LBC), whereas women positive for hrHPV on self-collection must attend an appointment for a clinician-collected sample for LBC. This is because cervical cellular abnormalities are most often detected at the transformation zone around the cervical squamocolumnar junction, and this is most reliably sampled by clinician collection of the visualized transformation zone. In contrast, the HPV test is less anatomically dependent, as the virus is more widespread around the ectocervix. In 2022, LBC was performed on samples from 99.4% of hrHPV-positive women for clinician-collected sampling, but for those who used self-collection, 82.0% of hrHPV-positive women had LBC [ 39 ], presumably because 18% did not provide a second sample, required for LBC. It has been estimated that due in part to the additional LTFU in the self-collection pathway [ 40 ], the relative program detection rate for CIN2+ for self-collection could be as low as 60–70% that of clinician-collected sampling [ 19 ].\n\nA modeling study before the introduction of self-collection concluded that self-collection in the Netherlands could be cost-effective if three criteria were met: the relative sensitivity of self-collection was at least 0.95 of that of clinician-collection, there was a low rate of switching to self-collection (of <40%) among routine screeners and overall participation increased by at least 6% following introduction of self-collection, the last of which has not yet been met [ 42 ]. It is difficult to know to what extent screening uptake was affected by how self-collection was first introduced or by the impact of external factors such as the COVID-19 pandemic. Long-term coverage of cervical screening remains high and self-collection has been integrated into the national routine screening program.\n\nThis real-world example indicates that introducing self-collection into a well-established high-coverage screening program does not automatically increase overall screening uptake and highlights the need for further consideration as to how best to communicate to women being offered screening the importance of screening and that it is of utmost importance to ensure follow-up of any positive HPV results. It also highlights the importance of careful consideration of all aspects of how self-collection is offered to reduce barriers to participation, and careful monitoring and evaluation of LTFU and positivity of self-samples which are required to ensure that the benefits of offering women choices and widening participation in screening programs are realized in practice.\n\nIn the next section, the wider evidence on self-collection is explored in terms of program implementation, HPV test performance, sample adequacy and laboratory workflow.\n\nIn the countries where HPV self-collection is currently being offered, it is principally targeted to under-screened women, in other words, those who have not been screened via traditional methods in line with European screening guidelines for cervical cancer screening [ 43 ]. It is well suited for opportunistic screening to supplement routine screening, particularly in settings where under-screened women attend a healthcare setting. Pilot studies in the UK [ 44 , 45 ] examined whether offering self-collection kits to under-screened women in person during a healthcare appointment could increase participation in this group. In one study, of the women eligible (overdue for screening by >6 months), 21% (652/3131) were offered, 443 accepted and 292 returned a self-collected sample (45% (292/652))[ 44 ]. Results of a larger UK pilot, where women were offered self-collection in primary care, are anticipated later this year [ 46 ].\n\nAny opportunistic screening needs to ensure appropriate screening intervals and clear communication of the screening process including how the results, follow-up diagnosis and treatment are provided.\n\nTheoretically, for women who would not otherwise screen, screening using self-collection will increase the detection of CIN2+ cases, even with high LTFU. However, when HPV self-collection is offered to groups who would have regularly received in-clinic screening, choosing self-collection may result in a lower CIN2+ detection rate (largely due to the lower follow-up after screen-positive results). Interpreting data from the Netherlands suggests that even when self-collection is targeted to non-responders, some women who would otherwise have participated in in-clinic screening may ignore the initial invitation (to screen via clinician-collected sampling) so that they are offered self-collection in a second invitation [ 35 ].\n\nResearch on the impact of opt-in and opt-out self-sampling strategies on coverage and follow-up of underscreened groups has been undertaken within national screening programs. In large-scale randomized studies in Slovenia [ 5 ] and Estonia [ 7 ] an opt-out approach resulted in higher participation than opt-in. In Slovenia, the uptake (proportion of samples received) in 26,556 underscreened women offered one of three screening options: 34.0% (n = 14,000) for opt-in self-collection at home (samples sent to laboratory); 37.7% (n = 9556) for opt-out self-collection at home and 18.4% (n = 2600) for clinician-collection at a health center ( p  < 0.050) [ 5 ]. In Estonia, uptake in 12,000 underscreened women was 26% for opt-out self-sampling, where women received a kit in the post or 11% for opt-in self-sampling, where women received an email invite to order a kit which was then posted to them ( p  < 0.05) [ 7 ]. A study in Italy [ 47 ] compared three strategies among 2995 underscreened women in two rounds with a 3-year interval. The control group received a reminder invitation to undergo in-clinic screening, while the intervention groups either received a kit in the mail (opt-out) or collected from a local pharmacy (opt-in). The percentage of women screened was 30.3% in the control group, 44.6% in the opt-out home self-sampling group and 32.3% in the opt-in pharmacy self-sampling group. The difference in uptake between the opt-out home self-sampling group and the control was statistically significant ( p  < 0.05) but the difference between the opt-in pharmacy self-sampling group and the control group was not. Two smaller studies in Sweden where uptake of clinician-collected sampling is as high as 82% reported similar results (among 8000 and 741 underscreened women per study) for opt-out self-collection screening offered to underscreened women with opt-out uptake of 18.7 and 20.2%, respectively [ 48 , 49 ].\n\nThe above studies indicate that for underscreened groups, the offer of opt-out self-collection resulted in a moderate uptake of 18–45%, consistently higher than for opt-in self-collection [ 5 , 7 , 47 ]. However, opt-out screening is likely to result in more waste, due to unused kits, particularly if sample return rates are relatively low. A meta-analysis of randomized control trials evaluating different self-collection strategies concluded that while opt-out strategies were more effective than opt-in strategies for self-collection, the approaches where self-collection was offered in person resulted in the highest uptake. A direct comparison of different strategies was impeded because, within the trials included in the meta-analysis, the opt-out mail-to-all trials were all in high-income settings, and community-based approaches were all in low- and middle-income settings. The authors also noted significant inter-study heterogeneity which precluded direct comparison of all self-collection strategies [ 50 ].\n\nA key consideration for the provision of self-collection, which is particularly relevant if self-collection replaces clinician-collected sampling in routine screening, is the relative performance of the HPV test in detecting CIN2+.\n\nSensitivity and specificity are metrics used to quantify test performance. The 2 × 2 table below ( Figure 2 ) is a useful tool to present the four possible outcomes of using a diagnostic test with a binary result (e.g., test positive or test negative).\n\nSensitivity is the proportion of people with a disease (or infection) that have a positive test result (a/a+c). Specificity is the proportion of people with no disease (or infection) that have a negative test result (d/b+d).\n\nAdditional useful measures of test performance are affected by the prevalence of disease in the tested population. Positive predictive value is the proportion of positive tests which are true positive (a/a + b). Negative predictive value is the proportion of negative tests which are true negatives (d/c + d).\n\nRelative performance measures (e.g., relative sensitivity) are reported with respect to the performance of a reference methodology type and are reported as a ratio. In this case, clinician-collected sampling is considered the reference against which the self-collected sampling is compared (e.g., sensitivity self-collection/sensitivity clinician-collection) for disease detection.\n\nEvaluations to assess relative test performance are more complex within the context of cervical screening for several reasons. Firstly, for HPV primary screening protocols, LBC is used as a reflex test following a positive hrHPV test result, so the combined performance of both tests must be considered for some types of evaluations. Secondly, there are two ways to consider the performance of an hrHPV assay, analytic performance – its ability to correctly detect the presence of hrHPV and clinical performance, its ability to detect disease (i.e., CIN2+). In this context, clinical performance is normally more relevant, but these different ways of assessing test performance are required for different purposes (i.e., for clinical effectiveness, cost–effectiveness evaluations etc..). For some comparisons, it is also useful to consider the ‘programme performance’, in other words, a measure of accuracy that accounts for other aspects of the screening pathway which impact the probability of detecting CIN2+, such as the proportion of invalid samples or people LTFU.\n\nTesting two samples from the same person, one collected using self-collection and one using clinician-collection, allows direct comparison of the clinical performance of testing self- and clinician-collected samples. However, if self-collection occurs within a healthcare setting, as is the case in many comparative studies, the comparison does not account for any differences in sample adequacy or quality introduced when samples are collected at home (as discussed in a later section). Comparing HPV or CIN2+ prevalence in two separate groups, one of which uses self-collection and the other uses clinician-collection, is a valid comparison if people are randomized to which sampling method they use (to avoid selection bias) and uptake in both cohorts is high enough to avoid introducing selection bias. Alternatively, for studies where the sampling method is self-selected (including large population-based studies), the underlying differences between the groups must be taken into account using statistical techniques.\n\nPeople who do not complete screening would not typically be included when calculating the sensitivity and specificity of the HPV test (or the HPV test plus LBC combined). However, when comparing different screening algorithms, it may be useful to consider the LTFU when calculating the sensitivity and specificity of the screening pathway, in other words, if people with CIN2+ (with disease) who initially provide a sample but disengage before completing screening are considered false negatives (group c in the diagnostic 2 × 2 table) this reduces the sensitivity of the pathway for CIN2+. We have referred to this as pathway effectiveness.\n\nIn studies directly comparing the relative clinical performance of testing self- and clinician-collected samples, there is evidence of non-inferiority of self-collection. An influential meta-analysis by Arbyn et al., 2014 [ 51 ] and updated in 2018 [ 8 ], compared test clinical performance from 56 studies where participants provided two samples (self and clinician-collected). Both samples were tested with the same HPV assay and participants with positive samples had colposcopy and histology to confirm their CIN2+ status. For a pooled analysis of PCR (polymerase chain reaction) HPV assays, there was a similar sensitivity for detecting CIN2+ in self- and clinician-collected samples (relative sensitivity 0.99, 95% CI: 0.97–1.02) but self-collection had a slightly lower relative specificity (0.98, 95% CI: 0.97–0.99). A later meta-analysis by Arbyn et al. was published in 2022; however, it evaluated the agreement and concordance between the two sampling modalities rather than relative sensitivity and specificity for CIN2+ end points [ 52 ].\n\nFor pragmatic reasons, many of the women included in comparison studies are recruited from colposcopy clinics, including women attending for follow-up of cervical abnormalities [ 8 ], and as such they have a higher prevalence of HPV and CIN2+ than found in screening populations [ 19 , 53 , 54 ]. While the sensitivity and specificity of an hrHPV test to detect the presence of hrHPV are independent of hrHPV prevalence, there is evidence that the specificity for detecting CIN2+ is reduced when the prevalence of hrHPV is high [ 52 ] (in both sample types). The implications of using higher prevalence populations for comparison studies are not clear and may contribute to the different conclusions drawn from experimental studies, in other words, that sample type does not significantly impact sensitivity for detecting CIN2+, and the real-world evidence from the Netherlands which finds lower hrHPV positivity and lower odds of CIN2+ and CIN3+ detection in self-collected samples even when age differences, screening history and sociodemographic characteristics are taken into account [ 41 ].\n\nThe use of self-collection within the Netherlands national screening program and recent studies in screening populations report some differences from the findings of the meta-analysis. There have been two large Dutch studies in screening populations, the first, referred to as IMPROVE, was a randomized, paired non-inferiority trial conducted in 2015–2016 [ 54 ]. It found no statistically significant difference in the relative sensitivity and specificity for detecting CIN2+ in self and clinician-collected samples (relative sensitivity 0.96, 95% CI: 0.91–1.03, and specificity 1.00, 95% CI: 0.99–1.01). It used the GP5+/GP6+ PCR enzyme immunoassay, which is not currently recommended for use in national screening programs [ 55 ].\n\nData from high-income countries on the effectiveness of HPV self-collection during actual implementation are limited. An observational study conducted in the Netherlands from 2017 to 2019 [ 41 ] found that self-collection resulted in significantly lower hrHPV positivity (adjusted odds ratio [aOR] 0.65; 95% CI, 0.63–0.68) and lower odds of CIN2+ detection (aOR 0.76, 95% CI: 0.70–0.82) and CIN3+ detection (aOR 0.86, 95% CI: 0.78–0.95) compared with clinician-collected sampling, after adjusting for screening history, age, region and sociodemographic factors [ 41 ].\n\nIt is important to consider LTFU (sometimes referred to as ‘pathway retention’) when considering the implications of self-collection on the effectiveness of a screening pathway to detect CIN2+. The more steps in the pathway that require patient engagement, the higher the chance of LTFU, which then reduces the probability of detecting CIN2+, and the overall pathway effectiveness. Compared with clinician-collected sampling, self-collection pathways typically have an additional step after an HPV-positive result where a second sample collection (clinician-collected) is needed for LBC triage to inform referral to colposcopy. To mitigate the risk of LTFU, some programs have removed the LBC triage step, by directly referring to colposcopy or using genotyping to refer higher-risk HPV genotypes (e.g., HPV 16/18/45) to colposcopy, and referring patients with other HPV genotypes to LBC triage (such as in the Netherlands). Direct referral to colposcopy for all HPV-positive tests could potentially result in harm due to over-referral and may not be feasible in many settings based on cost and capacity [ 9 , 48 ].\n\nMany studies of self-collection report LTFU and uptake at different stages of the pathway. In a model developed to assess the cost–effectiveness of different uptake rates and various levels of switching (from clinician- to self-collection) within the Netherlands, it was assumed that 92% of women who are HPV-test positive by self-sampling would then return to the clinic to provide a second sample for LBC testing [ 42 ]. Although this percentage was based on reported data [ 9 ], it is considerably higher than reported in a study from France which found a 35% uptake of LBC post-HPV-positive self-collection result [ 56 ]. The Dutch model also assumed the same LTFU rate (of 8%) for regular screeners who switched to self-collection as for people who would not otherwise have screened. Additional data are needed to better understand the follow-up rates following an HPV-positive result from a self-collected sample. Full consideration of pathway effectiveness across all steps incorporating test performance, uptake and LTFU is needed to meaningfully compare screening algorithms with and without self-collection.\n\nIssues with sample adequacy, quality and sample collection/laboratory testing procedures could be the reason for, or contribute to the lower hrHPV positivity of self-collection observed in some real-world settings, such as in the Netherlands [ 41 ]. An audit of cervical screening data from Denmark (in 2018) found that the percentage of invalid self-samples sent for HPV testing was low (0.18%), but double that of clinician-collected samples (0.05–0.10%) in the same time period (no statistical analysis was performed) [ 57 ]. The Arbyn 2018 meta-analysis reported that 0.7% (95% CI: 0.4%–1.0%) of self-collected samples were invalid, but no percentage was reported for clinician-collected samples [ 8 ]. The reason for invalid tests, whether user error, leakage or sample collection/testing processes, is not clear. Validation is typically dependent on the detection of internal endogenous controls that differ between the various HPV assays [ 57 ]. The criteria for molecular HPV testing sample adequacy for clinician-collected samples are well defined, but there is variation in criteria currently used for self-collected samples.\n\nClinician-collected samples are stored under standardized conditions, generally using LBC-compatible storage media containing fixatives, and are regularly and promptly sent to the laboratory. In contrast, the conditions under which samples are collected and stored using self-collection could vary immensely. Furthermore, there is the risk of extended delays (at home or in the postal system) in returning samples to the laboratory.\n\nIt is not clear to what extent collecting a sample at home impacts the sample quality because although studies have investigated self-collection performed in primary care [ 58 , 59 ] and in homes [ 40 , 54 , 60 ], the collection location is often varied alongside other factors [ 60 ]. Some studies have found that the quantity of transport material used (for wet samples) or the amount of media used to resuspend the sample at the laboratory before testing affects the positivity rates of HPV testing in self-collected samples [ 19 , 40 , 41 , 61 ]. There are few studies which examine the stability of samples collected at home and returned in person or sent via the post. One such study from Denmark found that dry-collected samples were stable for up to 32 weeks when collected using an Evalyn brush and kept between 4 and 30°C [ 62 ]. Another found that, despite a decrease in DNA yields in mailed self-collected compared with clinic-collected samples, this had little effect on the PCR amplification process or test results [ 63 ]. A study in Sardinia found high agreement in HPV-DNA results when dry self-collected samples were tested at 2–6 days and at 35–39 days after elution [ 64 ]. While these studies indicate good stability over extended periods, there may be opportunities for further optimizing the way that self-collected samples are stored and transported.\n\nFor programs currently providing an efficient screening service, maintaining high quality and efficiency are important. Many laboratories testing cervical screening samples perform a large number of tests each day, achieved through automated processes allowing multiple samples to be tested simultaneously. Any small change in process which cannot be automated would disrupt the laboratory workflow and potentially impact throughput and costs. For instance, efficiencies can be reduced considerably if hands-on time of laboratory staff is required for suspending self-collected samples in buffer solution for testing following dry self-collection or if samples need transferring from self-collection tubes which do not fit into standard racks [ 65 ]. While dry self-collected samples may have advantages for storage and transportation, there may be disadvantages for laboratory workflow. There is very little publicly available documentation reporting the laboratory workflow for HPV screening with clinician-collected samples or the potential impact of switching to a self-collected sample. Where there is a change in the HPV assay, collection device or type of sample used for screening, screening programs would typically carry out an internal validation and further internal validation and verification are also required by laboratories to meet external accreditation requirements. It is also not clear whether all laboratories involved in processing screening samples have the ability or capacity to process self-collected devices or whether new laboratory standard operating procedures and technologies are required. Health economic assessments which include explicit consideration of laboratory workflow on overall pathway effectiveness and cost are needed.\n\nThe use of self-collection for cervical screening has the potential to increase uptake in women who face barriers to clinician-collected screening. However, caution should be taken in deciding how and whether to introduce self-collection in programs with existing high-coverage screening, particularly for women who routinely screen [ 19 ]. Changes to national programs must account for the need to maintain and improve health gains achieved through existing screening programs.\n\nMany countries are considering implementing primary HPV testing with clinician-collected screening and/or considering wider changes to their screening algorithms such as the frequency of screening and how to best utilize new technologies such as HPV genotyping. Any changes to national programs must account for the need to maintain and improve the health gains achieved through existing screening programs. If overall screening rates are not shown to increase with the introduction of self-collection, particularly among underscreened individuals, the introduction of self-collection could risk the effectiveness and accuracy of existing screening programs if it disrupts laboratory workflow or reduces CIN2+ detection (either due to LTFU, or sample quality issues) in women who would otherwise have screened.\n\nFor regularly screened women, the scientific and cervical cancer community have called for further research to avoid any potential disadvantages of introducing self-collection including evaluating which protocols are best suited for self-collection [ 66 , 67 ]. The optimal duration of the screening cycle may need to be re-evaluated for self-collection. A better understanding of the impact of self-collection on laboratory flow and pathway effectiveness for CIN2+ detection is crucial. Theoretical modeling and health economic evaluations informed by real-world data on uptake and LTFU and incorporating self-collection and clinician-collection to calculate the pathway effectiveness of different screening algorithms would provide meaningful evidence to inform national policy decisions. Formal implementation science evaluations from settings where self-collection is introduced would offer valuable insights to other national screening programs on outcomes such as acceptability, costs and sustainability [ 68 ].\n\nThe same HPV self-collection outreach strategies are unlikely to work in all settings, since postal systems, healthcare provision and access, laboratory infrastructure and screening systems are heterogeneous, as are patient preferences and the public acceptance of screening. It is important to consider the feasibility and optimal approach in each setting, bearing in mind that even where screening programs are well established, the prevalence of HPV and CIN2+ will gradually reduce over the next 20–30 years as the screening cohort includes an increasing percentage of HPV vaccinated people [ 69 , 70 ].",
    "mesh_query": "mass screening[MeSH Terms]",
    "source_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12915870/",
    "data_crawling": "2026-02-19T18:22:05.458851"
  },
  {
    "pmc_id": "12911743",
    "title": "Effectiveness and acceptability of interventions to improve faecal immunochemical test (FIT) return in both asymptomatic (screening) and symptomatic populations: protocol for a systematic review of qualitative and quantitative evidence",
    "abstract": " Abstract Introduction Colorectal cancer (CRC) is the fourth most common cancer in the UK and second leading cause of cancer-related deaths. The faecal immunochemical test (FIT) is a non-invasive home-based test used for both symptomatic assessment and population-based screening. However, approximately 30% of screening FIT kits and 10% of symptomatic FIT kits are never returned. Under-served populations, including ethnic minorities, socioeconomically deprived communities and those with mental health conditions, experience particularly low FIT return rates, contributing to health inequalities in CRC outcomes. This systematic review aims to synthesise evidence on the effectiveness and acceptability of interventions to improve FIT returns in both asymptomatic screening and symptomatic populations, with particular focus on under-served communities. Methods and analysis We will conduct a systematic review of qualitative and quantitative evidence. We will search Scopus, MedLine via Ovid, CINAHL via Ebsco and Cochrane Central Register of Controlled Trials from September 2010 onwards, supplemented by reference screening and trial registry searches. Eligible studies will include randomised controlled trials, quasi-experimental studies, observational studies, qualitative studies, mixed-methods studies and implementation studies examining FIT interventions in screening or symptomatic populations. Two reviewers will independently screen search results for eligible studies. Data extraction will capture study characteristics, population demographics, intervention components and outcomes including FIT return rates, acceptability, feasibility and implementation factors. Quantitative data will undergo systematic tabulation and meta-analysis where appropriate, with narrative synthesis for heterogeneous studies. Qualitative data will be analysed using framework-based thematic analysis, mapping findings to both the theoretical domains framework and theoretical framework of acceptability. A mixed-methods synthesis will integrate quantitative and qualitative findings to identify intervention characteristics, implementation strategies and contextual factors associated with improved outcomes across different population groups. Ethics and dissemination Ethics approval is not required as this systematic review will analyse published studies. Findings will be disseminated through peer-reviewed publication and conference presentations. PROSPERO registration number CRD420251111663.",
    "text": "This will be the first systematic review to synthesise both quantitative and qualitative evidence specifically on interventions to improve faecal immunochemical test returns, using a robust mixed-methods approach.\n\nThe review will employ theoretical frameworks (theoretical domains framework and theoretical framework of acceptability) to provide comprehensive understanding of both behavioural mechanisms and acceptability factors.\n\nThe use of automated screening carries a small risk that relevant but poorly indexed studies, particularly qualitative or implementation studies that use non-standard terminology, could be missed by the algorithm.\n\nThe search is limited to papers published from September 2010 onwards, which excludes older behavioural or implementation insights from the era of guaiac faecal occult blood tests that might still be applicable today.\n\nColorectal cancer (CRC) is the fourth most common cancer in the UK, and the second leading cause of cancer-related deaths. 1  An expedited diagnosis and early treatment of CRC improve survival and cure rates, with 95% of patients diagnosed at stage 1 surviving 5 years or more, compared with only 10% of those diagnosed at stage 4. 2 4\n\nThe faecal immunochemical test (FIT) is a non-invasive, dipstick test that is completed at an individual’s home and sent for analysis. The test detects the early degradation products of blood in the faeces. 5 7  It is now widely used as a surrogate marker for bowel cancer in two distinct primary care pathways. First, FIT is used for the assessment of symptomatic individuals presenting with potential bowel cancer symptoms, such as changes in bowel habits, abdominal pain, rectal bleeding and weight loss. Second, FIT is used as part of population-based screening programmes for people over the age of 50, regardless of whether or not they are experiencing any symptoms. 5 8  This is a protocol for a systematic review of evidence about the effectiveness and acceptability of interventions to improve FIT returns in both contexts.\n\nApproximately 30% of CRC screening FIT kits and 10% of symptomatic FIT kits are never returned. 9 10  This represents about 2.1 million unreturned FIT kits annually, resulting in missed opportunities for early CRC detection and a financial loss of approximately £11.1 million to the NHS, 7  as well as creating unnecessary non-recyclable waste. To address low FIT returns, programmes and interventions are being developed to identify and overcome barriers to correct FIT completion.\n\nOne example of such an intervention is the DETECT-CRC (Early DETECTion of ColoRectal Cancer in Yorkshire) feasibility study, which aims to increase FIT returns in socioeconomically deprived areas through a pharmacy-based active case finding service. 11  This approach aims to identify individuals who may not recognise their symptoms or who face barriers to accessing healthcare services like primary care by offering FIT kits within community pharmacy settings, which can be completed at home.\n\nA number of systematic reviews have previously examined interventions to improve CRC screening uptake. 12 15  However, these studies have primarily focused on broader population-level interventions or specific screening modalities, without comprehensively examining the evidence specific to underserved groups, or exploring the acceptability and implementation of FIT within these populations. This represents a significant gap in the literature, particularly given the known disparities in CRC outcomes among underserved communities. Understanding both the effectiveness and acceptability of existing interventions is essential for developing strategies that can improve CRC outcomes equitably and efficiently across all population groups.\n\nComparable with many other cancers, 16  inequalities are prevalent across the entire CRC pathway: underserved populations, including ethnic minority communities, socioeconomically deprived populations, individuals from certain religious backgrounds, people with serious mental health conditions, those with low educational attainment and transgender individuals, have reduced screening engagement, delayed diagnoses and inequitable access to treatment and follow-up care, which ultimately leads to poorer CRC outcomes. 16 21\n\nThe aim of this systematic review will be to synthesise the evidence on the effectiveness and acceptability of interventions to improve FIT returns in both asymptomatic screening and symptomatic populations.\n\nWe will review  any  intervention designed to improve FIT return rates in either screening or symptomatic pathways. This intentionally broad scope reflects the heterogeneity of approaches in the literature, which may include (but are not limited to): behavioural interventions (eg, reminders, educational materials, motivational messaging); service delivery modifications (eg, pharmacy-based distribution, community outreach); kit design or distribution method changes (eg, simplified instructions, modified sample collection devices) or multi-component complex interventions combining multiple strategies. One aim of our synthesis will be to develop a typology of interventions from the included studies using the TIDieR (template for intervention description and replication) checklist. 22\n\nTo evaluate the effectiveness of interventions to improve FIT completion/return rates and subsequent follow-up in asymptomatic (screening) or symptomatic populations.\n\nTo assess the acceptability and feasibility of these interventions from the perspective of both service users and providers.\n\nTo examine how intervention effectiveness varies across different population subgroups, with particular attention to under-served communities (defined below).\n\nTo examine the implementation of interventions to improve FIT returns, including barriers and facilitators to implementation, implementation strategies used, and contextual factors that influence successful implementation across different healthcare settings and populations.\n\nWe will undertake a systematic review of qualitative and quantitative evidence about the acceptability and effectiveness of interventions to increase FIT returns. This protocol is reported in line with the preferred reporting items for systematic review and meta-analysis protocols reporting guidelines 23  ( online supplemental file 1 ) and has been prospectively registered on PROSPERO (CRD420251111663).\n\nWe will specifically evaluate interventions within under-served populations, defined according to the UK government framework as groups experiencing socioeconomic deprivation, any protected characteristics under the UK 2010 Equality Act, barriers to healthcare registration, housing instability, immigration status barriers, institutional barriers, or complex health and social needs. 24\n\nWithin this broader definition, we will specifically target the communities most commonly experiencing bowel cancer inequities, including older adults, people from Black, Asian and minority ethnic backgrounds, those living in areas of high socioeconomic deprivation, men (who have lower screening participation rates), and people with learning disabilities or mental health conditions who face additional barriers to screening access and follow-up care.\n\nStudies will be included if they target these populations specifically, or provide data allowing for assessment of differential intervention effects in different population groups.\n\nAsymptomatic populations: adults (age 18 years or over) who are eligible for CRC screening according to national guidelines.\n\nSymptomatic populations: adults (age 18 years or over) presenting with symptoms in primary care that warrant FIT as part of diagnostic evaluation.\n\nExclusion criteria: non-human studies; studies conducted exclusively in secondary or tertiary care settings; studies focusing on populations with established CRC diagnosis; studies where FIT is used for post-treatment surveillance rather than screening or diagnostic purposes.\n\nIn cases where studies report mixed populations, we will include papers provided data for the eligible population of interest can be extracted separately, or, where this is not possible, where the majority of the study sample is eligible according to our criteria.\n\nInclusion criteria: studies examining the effectiveness and/or acceptability of interventions designed to improve FIT returns. Eligible studies may include: (a) Controlled comparisons (eg, randomised controlled trials (RCTs) or quasi-experimental studies comparing intervention vs usual care/control), (b) Pre-post comparisons (eg, FIT return rates before vs after intervention implementation) or (c) Studies without formal comparators (eg, qualitative studies examining acceptability, implementation studies describing barriers/facilitators). Our primary outcome of “intervention effectiveness (FIT return rates)” will be extracted from controlled or pre-post comparisons where available; however, we will also include studies without quantitative comparators that provide valuable evidence on acceptability, feasibility, and implementation factors.\n\nExclusion criteria: studies focusing on other CRC detection methods (eg, guaiac faecal occult blood tests, colonoscopy) without specific data on FIT; studies that do not evaluate intervention effectiveness or acceptability (FIT return rates).\n\nThere will be no geographical restrictions on study inclusion. Studies conducted in any country or healthcare setting will be eligible, regardless of income classification or healthcare system structure. This approach recognises that valuable insights on interventions for under-served populations may emerge from diverse global contexts.\n\nInclusion criteria: RCTs, quasi-experimental studies, observational studies (cohort, case-control, cross-sectional), qualitative studies, mixed-methods studies and implementation studies. It is anticipated that quantitative study designs (RCTs, quasi-experimental and observational studies) will primarily contribute to effectiveness objectives, while qualitative studies will contribute to implementation and acceptability objectives.\n\nExclusion criteria: commentary or opinion publications that do not present new data.\n\nSearches will be limited to papers published from September 2010 onwards (when FIT started to replace older detection methods).\n\nAcceptability of interventions designed to improve FIT returns to service users and providers.\n\nThe following bibliographic databases will be searched from 2010 to present: Scopus, MedLine via Ovid, CINAHL via Ebsco and Cochrane Central Register of Controlled Trials. The electronic database search will be supplemented by scanning reference lists of included studies and searching trial registries (ClinicalTrials.gov, WHO International Clinical Trials Registry Platform (ICTRP).\n\nThe search strategy will be designed in consultation with the review team and peer-reviewed by using the peer review of electronic search strategies checklist. See  online supplemental file 2  for a sample search strategy.\n\nStudy selection will be done in two stages using Rayyan software. 25  Two reviewers will independently screen titles and abstracts against the eligibility criteria for the first 1000 abstracts. After that, we will employ the Rayyan SVM-based classifier to undertake priority screening. We will auto-eliminate abstracts below 40% likelihood of inclusion, recalculating after every 50 screening decisions made on abstracts that are above the 40% threshold for auto-elimination. The likelihood rating is derived from the distance from the learnt boundary between included and excluded papers within the training set. Abstracts that meet the criteria or provide insufficient information to determine eligibility will proceed to full-text review. Two reviewers will then independently assess the full-text articles for final inclusion. Any disagreements at either stage will be resolved through discussion or consultation with a third reviewer if needed. The reasons for exclusion at the full-text stage will be documented.\n\nA standardised data extraction form will be developed and piloted on a sample of included studies. Two reviewers will independently extract data from each study, with discrepancies resolved through discussion or a third reviewer. The form will be iteratively updated to ensure all relevant data are captured. If critical information is missing from any studies, the authors will be contacted by email with a follow-up reminder after 2 weeks if no response is received.\n\nQuantitative results will be extracted for any direct measures of intervention effectiveness relating to FIT uptake and/or return rates.\n\nFor qualitative data, we will extract interpretative text and overarching themes as reported by study authors. The extraction of qualitative findings will be structured according to the theoretical framework of acceptability (TFA) 26  to support subsequent framework analysis. Data will be organised within the TFA’s seven domains: affective attitude, burden, ethicality, intervention coherence, opportunity costs, perceived effectiveness and self-efficacy. Additionally, we will categorise the acceptability outcomes reported in each study as prospective (anticipated acceptability), concurrent (experienced acceptability during intervention delivery) or retrospective (acceptability reflected on after intervention completion).\n\nIntervention characteristics (screening or symptomatic pathway, intervention components, implementation strategies) using the TIDieR checklist. 22\n\nThe mixed methods appraisal tool will be used for qualitative, quantitative and mixed methods studies.\n\nTwo reviewers will independently assess quality, with disagreements resolved through consensus or a third reviewer. Quality assessment results will be presented in tables and considered in the interpretation of findings. All eligible studies will be included in the synthesis regardless of quality, but sensitivity analyses may be conducted to explore the impact of quality on results.\n\nQuantitative data will be synthesised through systematic tabulation and, where appropriate, meta-analysis. Where meta-analysis is not appropriate due to heterogeneity in study designs, populations or interventions, we will conduct a narrative synthesis. Meta-analyses will calculate risk ratios or ORs with 95% CIs for dichotomous outcomes (eg, FIT return rates) and mean differences with SD for continuous outcomes (eg, time to return).\n\nQualitative data will undergo framework-based thematic analysis. Data will be deductively mapped to the theoretical domains framework to identify behavioural determinants influencing FIT uptake and return. 27  Concurrently, qualitative findings will be cross-mapped to the TFA to examine intervention acceptability. 26  This dual mapping will allow us to understand both the behavioural mechanisms underlying intervention effects and the acceptability factors that influence implementation success.\n\nWe will conduct a mixed-methods synthesis to harness the strengths of both quantitative and qualitative evidence. We will use a convergent synthesis strategy. In a convergent approach, we will simultaneously analyse quantitative and qualitative data independently before merging the findings to create an integrated understanding. This mixed-methods approach is designed to produce a more comprehensive and nuanced understanding than would be possible through isolated examination of either quantitative or qualitative evidence alone.\n\nThe strength of evidence will be assessed using Grading of Recommendations, Assessment, Development and Evaluation (GRADE) for quantitative outcomes 28  and CERQual for qualitative findings. 29\n\nThe findings from both syntheses will be integrated by linking intervention characteristics, implementation strategies and effectiveness outcomes. This integration will help identify which implementation approaches are associated with better outcomes in which contexts and for which populations.\n\nContext-mechanism-outcome configurations to explain what works, for whom, in what circumstances.\n\nThe findings from this systematic review will directly inform the development of the intervention package and implementation plan in the PRO-FIT project, ensuring that it builds on existing evidence of what works in promoting FIT use among underserved communities.\n\nA patient and public involvement panel for the wider PRO-FIT project will review the findings of the systematic review.\n\nA mixed-methods systematic review incorporating qualitative and quantitative evidence is warranted to comprehensively assess the effectiveness, acceptability and implementation of FIT in underserved groups. This approach will allow for a holistic understanding of the barriers and facilitators to FIT uptake and return, as well as the identification of strategies to improve participation and reduce inequities. The integration of qualitative and quantitative evidence will provide a more nuanced understanding of the complex factors influencing FIT use in these populations, which is essential to inform the development of targeted interventions.\n\nOur systematic review will make several important contributions to the existing evidence base. First, it will provide the first comprehensive synthesis of interventions specifically designed to improve FIT returns in underserved populations, addressing a significant gap in the current literature. Unlike previous reviews that have focused on broader population-level interventions or specific screening modalities, this review will specifically examine how interventions perform across different underserved groups, providing crucial insights into health equity considerations. Second, by employing a mixed-methods approach, the review will generate evidence on both what works (effectiveness) and why it works (acceptability and feasibility), providing relevant insights for intervention development. This will also enable the identification of intervention components that are both effective and implementable in real-world settings. Thirdly, the review will produce a typology of interventions that could be used by healthcare providers, policymakers and researchers to design more effective and equitable FIT interventions.\n\nThere are important implications for clinical practice and healthcare policy by undertaking this review. As evidence is being collected across diverse contexts, the review will help healthcare providers and policymakers understand what interventions are most effective in different underserved populations. This evidence could inform guidelines on FIT implementation. In addition, our review will identify which approaches are most successful for addressing specific barriers, which are faced by different underserved populations.\n\nAdditionally, the review findings will directly inform the development of an intervention package and implementation plan within the National Institute for Health Research funded PRO-FIT project. This ensures a clear pathway from evidence synthesis to intervention development and implementation.\n\nIn conclusion, this systematic review will provide crucial evidence to inform the development of more effective and equitable FIT interventions for underserved populations. By understanding both what works and why it works, the review will support the translation of research evidence into practice and policy, ultimately contributing to the reduction of CRC inequalities.",
    "mesh_query": "mass screening[MeSH Terms]",
    "source_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12911743/",
    "data_crawling": "2026-02-19T18:22:06.516881"
  },
  {
    "pmc_id": "12911692",
    "title": "Start4All protocol for a Bayesian cost-effectiveness model of tuberculosis screening and diagnosis in seven high burden low-income and middle-income countries",
    "abstract": "Abstract Introduction High costs of screening and diagnostic tests remain a major barrier to timely tuberculosis (TB) identification in resource-limited settings. Evidence on the cost-effectiveness of scalable screening algorithms is limited. Start4All is a research project aimed at developing and evaluating algorithmic approaches to TB screening and diagnosis, with the goal of optimising technical and allocative efficiency when expanding diagnostic coverage to primary healthcare and community settings. Methods and analysis Five screening and diagnostic tests will be evaluated: a capillary blood-based assay (C-reactive protein (CRP)), sputum-based rapid molecular tests (PCR; individual and pooled Xpert MTB/RIF Ultra assay (Xpert Ultra, Cepheid®, California, USA)), a lateral-flow urine-based test for lipoarabinomannan (LF-LAM), and digital chest X-rays with artificial intelligence-based computer-aided detection (CXR-CAD). A microbiological reference standard of positive culture using the mycobacteria growth indicator tube will be used to confirm TB disease. We will compare the cost and effectiveness of concurrent and sequential positive serial combinations (screening algorithms) of CRP, CXR-CAD, LF-LAM, individual and pooled Xpert Ultra. Diagnostic performance will be estimated using sensitivity, specificity, predictive values and proportions of positive results, with Bayesian inference used to derive these estimates. The analysis will include adults (15 years and older) only and will be stratified by HIV status and level of care, including facility and community-based case finding. Effectiveness will be assessed based on the number of people with TB detected. Cost analysis will be conducted from the provider perspective, incorporating commodity and implementation costs. A decision tree model will be developed to assess the cost per number of persons with confirmed TB detected across all countries. Probabilistic sensitivity analysis will be conducted to account for uncertainty in model parameters, incorporating willingness-to-pay and willingness-to-accept thresholds. Ethics and dissemination WHO ethical review committee approval ERC.0003921. Data will be available on reasonable request to the principal investigator of the consortium. Trial registration number NCT05845112 .",
    "text": "Modelling several screening algorithms across a wide range of possible test combinations and accounting for uncertainty.\n\nThe proposed model provides a flexible framework that can be adapted or used by countries for decision-making.\n\nTuberculosis (TB) is the leading cause of death from a single infectious disease globally. Despite progress in diagnosis and treatment, 2.7 million people who developed TB disease were not diagnosed or notified in 2023, and over 1.25 million died of TB. 1  TB disproportionately impacts vulnerable populations with limited healthcare access, including those living in poverty, men, people living with HIV (PLHIV), rural residents, urban poor, displaced persons, older persons and children. The WHO recently reported that 47% (range 27%–92%) of TB-affected households face catastrophic costs. 2  Furthermore, funding for TB services in low-income and middle-income countries (LMICs) has been chronically insufficient with current levels at only 26% of the targeted US$ 22 billion/year by 2027. 1\n\nEffective TB screening and diagnostic tests are crucial for improving population health outcomes by minimising delays in diagnosis and linkage to care, and reducing transmission, as well as lowering patient costs and unnecessary treatment expenditures. 1  However, barriers such as limited access to accurate and timely diagnostic tools behove development of cost-effective approaches, particularly for vulnerable populations in LMICs. Since 2011, rapid molecular tests have revolutionised TB diagnosis, replacing the traditional reliance on microscopy and culture methods. While newer molecular tests are recommended by WHO as highly effective, rapid and reliable diagnostic tools for TB, 3  their high cost limits widespread use and sputum smear microscopy remains the primary TB diagnostic method for over half of the TB-affected population in high-burden countries. 1  A recent review highlighted that cost-effectiveness (CE) of TB screening and diagnostic tools may be a function of several contextual factors, including population risk and associated prevalence, as well as testing volumes, yet evidence on the cost and CE of alternative TB screening and diagnostic tools is lacking. 4\n\nWHO defines an algorithm for systematic TB screening as combining one or several screening tests and a separate diagnostic evaluation for TB disease. 5  Evidence suggests that the introduction of initial screening tests, especially symptom-agnostic tools, such as chest X-rays (CXR) with artificial intelligence (AI)-based computer aided design (CAD) software and pooling of samples, may not only improve diagnostic accuracy, but also lead to large cost savings by reducing the need for more expensive confirmatory tests, 6 8  potentially enabling large-scale TB screening programmes where they are most needed. 9  Additionally, a marginal reduction in diagnostic accuracy may be considered acceptable if it facilitates the use of decentralised and/or more cost-efficient tests that can increase overall case detection at the population level 10  . The Start4All consortium is a 4-year Unitaid-funded research project to evaluate the optimal algorithmic approaches to TB screening and diagnosis across seven countries (Bangladesh, Brazil, Cameroon, Kenya, Malawi, Nigeria and Vietnam). The aim of Start4All is to determine the most accurate, feasible, acceptable, scalable and cost-effective solutions to expand TB diagnostic coverage to primary healthcare (PHC) and community settings.\n\nThis protocol paper outlines the methods used for the economic modelling analysis conducted alongside the Start4All diagnostic evaluation study, the protocol for which is planned to be published in this journal and is summarised in the statistical analysis plan (SAP) published elsewhere. 11  This initial analysis presents a modelling study that may support countries in identifying algorithms to be prioritised in their specific contexts—an essential step given the evolving diagnostic landscape, diverse target populations and the varying capacities of health systems across settings.\n\nOur study aims to address a critical lack of evidence on the CE and implementation of multi-step TB screening and diagnostic pathways, particularly across diverse and vulnerable populations, by generating multi-country data that can inform TB policy, optimise screening strategies and guide future research.\n\nOur primary objectives are to identify the most cost-effective algorithms relative to individual sputum Xpert Ultra (Cepheid®, California, USA) testing to detect TB through facility-based case finding (FBCF):\n\nAt PHC facilities across all countries (aggregate analysis) and all individuals, regardless of HIV status.\n\nAt district hospitals (DH) across all countries (aggregate analysis) and for all individuals, regardless of HIV status.\n\nOur secondary objectives are to identify the most cost-effective algorithms relative to individual sputum Xpert Ultra testing to detect TB through:\n\nFBCF at PHC facilities across all countries (aggregate analysis), for PLHIV and people without HIV or not known to have HIV.\n\nFBCF at DH across all countries (aggregate analysis), for PLHIV and people without HIV or not known to have HIV.\n\nFBCF at PHC facilities in individual countries (Bangladesh, Brazil, Cameroon, Kenya, Malawi and Nigeria) for all individuals, regardless of HIV status.\n\nFBCF at DH in individual countries (Bangladesh, Cameroon, Kenya, Malawi, Nigeria and Vietnam) for all individuals, regardless of HIV status.\n\nCommunity-based case finding (CBCF) in informal settlements in Bangladesh for all individuals, regardless of HIV status.\n\nCBCF in internally displaced people (IDPs) in Nigeria for all individuals, regardless of HIV status.\n\nCBCF in urban, low-income populations in Vietnam for all individuals, regardless of HIV status.\n\nThe study took place between 1 September 2022 and 28 May 2025. The health economic study is being conducted alongside multi-country (n=7), multi-centric cross-sectional studies (n=20), with different population groups and at different health system levels (see  online supplemental appendix S1  for overview). Publication of the protocol for the multi-country studies is planned in this journal, while the SAP has been published elsewhere. 11  At stationary healthcare facilities, including PHC facilities and DH, passive case-finding (PCF) was implemented for individuals with presumptive TB (see  online supplemental appendix S1 ). In some cases, intensified case finding (ICF) was adopted for people who were considered by WHO definitions to have a higher risk of TB, for example, PLHIV. 5  PCF and ICF will be considered jointly in the analysis as FBCF. Individuals were also identified through CBCF in pre-specified population groups, including informal settlements, IDPs or nomads.\n\nFive screening and diagnostic tests were evaluated on each participant in the study ( table 1 ): a capillary blood-based assay (C-reactive protein (CRP)), sputum-based rapid molecular tests (PCR; individual and pooled Xpert Ultra testing), a lateral-flow urine-based test for lipoarabinomannan (LF-LAM), and digital CXRs with AI-based CAD software. Pooled sputum testing involved multiple sputum samples from different individuals being combined and tested together, usually in pools containing four individual samples, by Xpert Ultra. A microbiological reference standard of positive culture using the mycobacteria growth indicator tube was used to confirm TB disease.\n\nPerformance metrics will be combined for all CRP tests and the economic evaluation will consider the cost of quantitative CRP only, as it makes up the majority of CRP tests in the study\n\nCAD, computer-aided detection; CRP, C-reactive protein; CXR, chest X-ray; LF-LAM, lateral-flow urine-based test for lipoarabinomannan; POC, Point of Care.\n\nData from the study will inform estimates of diagnostic performance—including sensitivity, specificity, predictive values and diagnostic yield per test attempted—for individual tests, as well as combinations of CRP, CXR-CAD, LF-LAM and individual or pooled Xpert Ultra tests. The proportion of positive results from each test or test combination will also be calculated.\n\nGiven a large number of possible combinations and permutations, a framework was developed to select a set of individual tests, as well as concurrent (simultaneous testing) and positive sequential (stepwise testing following a positive result) test combinations (screening algorithms), for modelling and evaluation ( online supplemental appendix S2 ). In total, 20 algorithms were pre-selected for FBCF and 40 for CBCF. As with individual tests, key performance metrics will be assessed for each selected algorithm.\n\nBayesian statistical inference will be used to estimate overall diagnostic accuracy metrics for each algorithm.\n\nThe CHEERS framework was used as a guide to develop this protocol ( online supplemental appendix S3 ). 12\n\nData collection of health outcomes took place in seven high-TB-burden countries, where country partners conducted a variety of community engagement activities to ensure appropriate participation of the public in the study. These activities included the organisation of consensus building meetings at national and local levels, during which feedback on the design and usefulness of the study was received and incorporated. CBCF events in a subset of countries were also accompanied by awareness building and community mobilisation efforts as described in table 3. The consortium constituted and continually engaged a scientific advisory board, who were informed and provided feedback on the progress of the study and its findings. Finally, the study included qualitative research elements that enabled the collection of insights and commentary from key stakeholders such as public health officers, policymakers and key affected groups.\n\nThe health economic analysis outlined in this protocol will compare the modelled costs and accuracy of selected algorithms with the individual Xpert Ultra test through a CE analysis. The single specimen individual Xpert Ultra was selected as the comparator for the economic evaluation because it is recommended by WHO.\n\nAnalysis will be performed at aggregate and individual country level for FBCF (PHC and DH) and at individual country level for CBCF.\n\nThe analysis will be performed from the provider perspective, as no primary data was collected from patients. The time horizon is approximately 1 year, though this varies between countries based on duration of the recruitment period. For FBCF, only the costs incurred at the point of testing, including the diagnostic process and any immediate resources used, will be considered, excluding follow-up care or treatment costs. For CBCF, the costs associated with mobilising individuals for screening, including community engagement efforts and communication materials used during the recruitment phase, will be captured.\n\nThe primary analysis will focus on aggregated CE results across multiple countries for FBCF. The secondary analysis will provide a more detailed examination of individual country-level CE results, covering both FBCF and CBCF approaches.\n\nUnit costs were defined as the cost per person tested and will be calculated for each algorithm at aggregated and individual country level. For the aggregated analyses, only commodity costs will be considered. This is because there is significant variability in implementation costs across countries, making the inclusion of health system costs non-comparable and analytically meaningless. Individual country analyses will account for commodity and implementation costs.\n\nCommodity costs will be based on ex-works costs only and be calculated as a function of several parameters, including cost inputs (capital and non-capital items) and a set of underlying assumptions ( table 2 ). Input cost values will be based on available literature and Start4All procurement costs ( online supplemental appendix S5 ). Capital items are defined as a durable resource with a useful life typically exceeding 1 year and value greater than US$ 100, whose costs are depreciated over time. Capital item costs will be annuitised over the lifetime of the item at a 3% discount rate 13  and apportioned per test, based on testing volumes for each test. Warranty and service level agreement costs will be included. Non-capital items are defined as resources with a useful life of less than 1 year that are consumed during routine service delivery, for example, Xpert Ultra cartridges.\n\nCAD, computer-aided detection; CBCF, community-based case finding; CRP, C-reactive protein; CXR, chest X-ray; DH, district hospitals; FBCF, facility-based case finding; PHC, primary healthcare.\n\nTesting volumes for individual Xpert Ultra will be based on assumptions from the literature. 14  At CBCF, volumes will be calculated by applying a 15% rate to the total number of individuals screened daily, 14 16  representing those who proceed to confirmatory testing with individual Xpert Ultra. For CXR-CAD and CRP at facility level, testing volumes will be determined using capacity and utilisation assumptions. For CBCF, daily screening volumes will be drawn from existing literature. 14 16\n\nFor FBCF, sputum specimen transportation costs will be assumed to be zero, based on the assumption that Xpert Ultra is available onsite at all facilities. For CBCF, transport of specimen will be costed based on information on running CBCF screening camps from the four countries implementing CBCF (Bangladesh, Cameroon, Nigeria and Vietnam).\n\nFor FBCF, country-specific implementation costs will be determined by the cost per outpatient visit. 17  The cost per FBCF algorithm will be estimated by summing the cost of individual tests included in the algorithm and assuming one outpatient visit to complete the algorithm.\n\nCBCF implementation costs will be collected directly from the four countries where CBCF activities take place (Bangladesh, Cameroon, Nigeria and Vietnam), based on assumptions regarding the typical implementation of a CBCF programme, excluding research costs ( table 3 ). The number of people screened per month or campaign will be estimated based on literature. 14 16  The total cost per campaign or per month of running the CBCF programme will be divided by the estimated number of people screened to calculate the cost per person screened in each country. The cost per CBCF algorithm will be determined in a manner similar to facility-based algorithms, but by combining commodity costs with primary programme implementation cost.\n\nThe following costs were not included: programme management and coordination, any costs related to culture\n\nACF implementation approach costed is a ‘low cost’ cost, where screening events take place relatively close to healthcare facilities, including laboratories; the Ministry of Health in Bangladesh is also exploring a more costly option where screening activities take place further afield\n\nACF, Active Case Finding; CBCF, community-based case finding; HR, Human Resource.\n\nCost per outpatient visit is available in 2017 international dollars (Int$) 17  and will be adjusted to 2024 dollars using global inflation figures provided by the World Bank. 18  CBCF implementation costs will be converted to 2024 Int$ using Purchasing Power Parity (PPP) conversion factors. 18  For consistency, commodity costs published in 2024 will be used. Project procurement costs are from 2023/2024.\n\nEffectiveness will be assessed by the number of persons with confirmed TB detected by each algorithm, simulating for a hypothetical cohort of 1000 participants. Data from the study will be used to estimate, for each algorithm, country and population setting under evaluation, the conditional probabilities of both positive results at each testing step, as well as the positive and negative predictive values (PPV and NPV). Each of these parameters is a probability parameter, and as such, the posterior distributions are estimated as beta distributions, the estimated distributional parameters of which are used as input to the CE model.\n\nAggregate analyses will combine parameters from all countries for PHC and DH populations, disaggregated by HIV status. Individual country analyses will use population-specific and site-specific accuracy parameters.\n\nThe number of individuals with a false positive result and the number of missed cases will also be estimated following the methods described above and analysed alongside CE results.\n\nWe will use a Bayesian decision tree model to simulate the cost and persons with TB detected for all algorithms selected a priori. Separate decision trees will be built in Amua V.0.3.1 software (Dr Zachary Jonathan Ward, Center for Health Decision Science at Harvard T.H. Chan School of Public Health, Boston, MA, USA) 19  for FBCF and CBCF, following standard methodology. 13  An example decision tree is provided in  online supplemental appendix S6 .\n\nThe expected cost of each algorithm will be calculated as a function of the unit cost of individual tests included and the (conditional) probability of positive/negative test results at each screening step and PPV and NPV of each algorithm (detail provided in  online supplemental appendix S6 ).\n\nA total of 22 separate and independent model runs will be performed across a range of different settings, accounting for HIV status for aggregated analyses ( table 4 ). Each time, all algorithms selected a priori for facility-based and CBCF settings will be included.\n\nAggregated analyses will be performed with aggregated accuracy data and commodity costs only\n\nCBCF, community-based case finding; DH, district hospitals; FBCF, facility-based case finding; IDP, internally displaced people; PHC, primary healthcare; PLHIV, people living with HIV.\n\nFor each model run, point estimates for expected costs, persons with TB detected and the cost per TB case detected will be tabulated and results displayed on a CE plane ( figure 1 ).\n\nThree types of uncertainty will be considered in the decision model, following standard guidelines. 20\n\nFirst, heterogeneity of individuals, including varying levels of TB prevalence in the population and differences in cost will be reflected through repeated model runs for different country and population settings.\n\nSecond, parameter uncertainty will be addressed through probabilistic sensitivity analysis (PSA), using Monte Carlo simulation (100,000 iterations). Uncertainty of the performance parameters will be propagated into the model using the posterior distributions estimated in the performance analysis described above. Uncertain cost parameter inputs will be included in the model as fitted distribution functions, based on most likely, optimistic and pessimistic scenarios ( online supplemental appendix S4 ). Where no data can be found on varying test prices, ±20% of the price point estimate will be assumed for the optimistic and pessimistic scenario, respectively, except for LF-LAM, where available literature will be used to define a reasonable range. 21  Xpert Ultra cartridge costs will be varied by±20%. Gamma or Poisson distributions will be fitted using R (V.4.1.0 (The R Foundation for Statistical Computing, Vienna, Austria)), depending on data characteristics. The analysis will use the ‘optimistic’ and ‘pessimistic’ scenarios to set cost distribution parameters. Poisson distributions will be bounded to avoid unrealistic values in the PSA as a result of long tails. All costs in the decision model will be included as a beta distribution function. For aggregate analyses, results will include a point estimate and a 95% credible interval, defined as the 2.5th and 97.5th percentiles of results across these simulations.\n\nThird, structural uncertainty will be partially addressed by modelling FBCF and CBCF separately. Costing assumptions will also be varied across different population settings, adjusting for the number of tests performed a day.\n\nUncertainty surrounding the cost and number of persons with TB detected will be summarised through scatter plots and CE acceptability curves (CEACs). Decision uncertainty will be reflected by varying the willingness-to-pay (WTP) and willingness-to-accept (WTA) thresholds ( figure 1 ). WTP will be assessed where algorithms fall in quadrant I. WTA will be assessed where algorithms fall in quadrant III. Separate CEACs will be generated for WTA and WTP analysis.\n\nSince there is no recognised WTP or WTA threshold for our outcome indicator (cost per person with TB detected), the estimated cost of treating one person with drug-susceptible TB (DS-TB) 21  will be used as a minimum threshold value to ground the analysis and varied in the sensitivity analysis (SA).\n\nIn the absence of established guidance, a minimum threshold of a 60% probability of being cost-effective will be set as a starting point and varied in the SA.\n\nEffective and timely screening algorithms are essential for improving TB case detection and reducing transmission globally. This health economic modelling study compares the cost and number of persons with TB detected across various country and population settings and case-finding strategies, relative to individual Xpert Ultra testing. The goal is to identify a subset of the most promising algorithms for further real-world evaluation by examining trade-offs between cost and accuracy, and potential cost savings through additional screening tests that can reduce the number of individual Xpert Ultra tests required without impairing the number of persons with TB detected.\n\nWhile the application of a follow-on diagnostic test after a negative screening result has the potential to improve diagnostic accuracy—particularly by reducing false positives—it also increases overall costs and may pose challenges to implementation feasibility, especially in resource-limited settings. Therefore, only positive concurrent and sequential screening algorithms are included since these have potential to exclude individuals who do not have TB early in the pathway and thus save resources. Applying follow-on diagnostic tests after a negative screening result would counteract this objective by limiting the opportunity to reduce confirmatory testing and associated costs.\n\nThe analysis will be conducted at two levels. First, aggregated analyses will identify a set of promising algorithms across several countries for FBCF in PHC and DH settings. Second, country-specific and population-specific analyses will generate contextual results to inform national policy and practice. This dual approach provides practical support for decision-making at country, regional and global level.\n\nThe analysis will use Bayesian methods to evaluate CE, accounting for uncertainty at different levels. Though our modelling framework will not account for detailed patient pathways or provider delays (eg, turnaround time), it will provide a high-level view of CE to help inform country selection of appropriate diagnostic strategies for their context. Distinct approaches will be applied for FBCF and CBCF, but models will remain generic and applicable across countries. Results will be stratified by level of care (PHC and DH) and HIV status (PLHIV, people without HIV or not known to have HIV, and all individuals, regardless of HIV status) for aggregated analyses. Extensive sensitivity analyses will account for parameter, model and decision uncertainty, ensuring robust findings across different demographic and epidemiological settings.\n\nA limitation of the study is the exclusion of patient-incurred costs, due to the lack of primary data and limited up-to-date WHO TB Patient Cost study coverage in the recruiting countries. 2  While this restricts patient-level cost inclusion, the number of false positives and missed cases will be reported to support algorithm prioritisation. Another limitation is that costs associated with FBCF will not distinguish between PCF and ICF, as comparing them falls outside the scope of this study. However, this will not impact key outcomes, since no direct comparison is being made between these approaches or across diagnostic settings. As our aggregate analysis seeks to support the evaluation of algorithms rather than provide a nominal estimate of health system costs across all countries, implementation costs will be included for individual country analyses only. A final limitation is that this study is not measuring patient-important outcomes including time to treatment initiation or treatment outcomes, but such analysis is planned for in further studies under Start4All.\n\nAlthough primary cost data are limited, cost assumptions will vary by care level and diagnostic setting. Commodity and implementation costs will be considered in country analyses and CBCF implementation costs will be estimated based on data obtained from four countries (Bangladesh, Cameroon, Nigeria and Vietnam). Robust cost estimates will be generated using procurement data combined with literature, with PSA accounting for heterogeneity and uncertainty at three levels: costing assumptions, cost per test and cost per algorithm.\n\nOur cost estimates include many uncertainties, such as over-estimating non-capital costs, but underestimating commodity costs overall by using ex-works estimates. However, since the same approach will be applied across all model runs, this limitation is unlikely to affect comparative results. CBCF implementation cost per person will be held constant across algorithms, assuming test-specific cost differences cancel out. Multiple model runs will reflect variations in costs across settings.\n\nThe model does not include socioeconomic equity, gender disaggregation, drug-resistance disaggregation or child populations due to scope and data limitations. However, specific analyses will be conducted for CBCF settings in four countries, including informal settlements, nomadic populations and refugees/IDPs. Separate models for PLHIV will help identify algorithms particularly suitable for this group.\n\nWTP and WTA thresholds will be applied based on the position of each algorithm on the CE plane. This allows for a nuanced evaluation of trade-offs between cost and diagnostic performance. Algorithms with lower performance, but substantial cost savings may be attractive, especially in resource-limited settings. The benchmark used for WTP and WTA in this study is the provider cost of treating one person with DS-TB, reflecting the trade-off between early detection and the higher costs of treating advanced or drug-resistant TB. This approach aligns with global policy goals prioritising early detection to reduce transmission and avoid costly treatments.\n\nAn initial threshold of 60% probability of CE will guide decision-making, balancing uncertainty with a reasonable level of confidence for policy-making. Sensitivity analyses will explore the impact of lower thresholds to assess the robustness of conclusions under greater uncertainty.\n\nThe study does not include treatment costs or model TB transmission effects, which are important limitations. However, WTP and WTA thresholds implicitly account—at least in part—for long-term treatment and transmission costs, offering a practical way to evaluate economic trade-offs. These thresholds allow exploration of policy priorities and budget constraints, making the model relevant across diverse settings.\n\nWhile our modelling results are intended to inform policy, guide implementation and support further research on the CE of TB diagnostic algorithms, we acknowledge that CE is only one of several important considerations. Factors such as feasibility, scalability and acceptability are equally critical for successful real-world implementation.\n\nEthics approval has been granted for phase I only. Oversight is provided by country ethical research committees, LSTM and Stop TB Partnership, as shown in  table 5 .\n\nInformed consent was obtained from all participants, and all data were anonymised prior to being accessed by the study authors.\n\nThe results of this study will be disseminated through publication in peer-reviewed open-access journals, presentations at national and international scientific conferences, and summaries shared with relevant stakeholders and participants where appropriate.",
    "mesh_query": "mass screening[MeSH Terms]",
    "source_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12911692/",
    "data_crawling": "2026-02-19T18:22:07.641112"
  },
  {
    "pmc_id": "12911741",
    "title": "Feasibility and cost-effectiveness of at-home self-sampling screening for type 2 diabetes: a pilot screening study in Denmark",
    "abstract": "Abstract Objective The increased use of HbA1c as a diagnostic criterion, novel methods to collect and store blood samples as well as the quality of national registers enable screening targeted individuals, who are not screened opportunistically. The study evaluates whether screening for type 2 diabetes using at-home self-sampling HbA1c tests, targeted people who have not had their HbA1c measured the last 2 years, is feasible and cost-effective. Research design and methods During a period of 9 months, the Danish Diabetes Association mailed free capillary at-home self-sampling HbA1c tests to 8,000 randomly selected individuals aged 50–75 years who had not had their HbA1c measured in the past 2 years. The screening costs per screen-detected derived from the pilot study and estimated cost savings across HbA1c levels derived from a Danish simulation study was used to estimate cost-effectiveness. Results About 38% returned a blood sample. The share of participants with HbA1c of 48 mmol/mol (6.5%) and above was 1.7% (50/2913). The screening costs were 1.207 per screen-detected with type 2 diabetes. A national screening program targeting individuals aged 50 to 75 years, who have not had their HbA1c measured within the previous 2 years, is estimated to reduce healthcare costs and productivity losses by €1514 per screen-detected, if diagnosis is moved forward by 3 years, as derived from a Danish simulation study. Hence, the total social return ratio of the screening program is estimated to €1514/ €1183=1.28. Conclusions The study suggests that screening for type 2 diabetes using at-home self-sampling HbA1c tests, targeting individuals aged 50–75, who have not had their HbA1c measured the past 2 years, is feasible and cost-effective in Denmark.",
    "text": "Detecting undiagnosed people with type 2 diabetes accelerates the time of diagnosis and reduces morbidity and mortality.\n\nScreening for type 2 diabetes using at-home self-sampling HbA1c, targeted individuals aged 50–75 who have not had their HbA1c measured in the last 2 years, is feasible and estimated to be cost-effective in Denmark.\n\nAt-home self-sampling HbA1c tests can complement existing opportunistic screening strategies and help reduce the incidence of diabetes complications and excess mortality.\n\nThe number of individuals living with type 2 diabetes is expected to rise globally, while treatment of type 2 diabetes poses a substantial economic burden worldwide. In Denmark, approximately 7% of the adult population, 333 000 individuals, lived with type 2 diabetes in the first quarter of 2025 1  and the number is expected to increase to 9.5%, by 2040. 2  People with type 2 diabetes in Denmark incur an estimated healthcare cost of €595 989, including treatment, care and medication, while the indirect costs due to productivity loss, including labor market absence and premature mortality, amount to approximately €478 487. 3\n\nA substantial share of the additional cost is driven by diabetes complications and the containment of these complications would reduce costs. HbA1c levels are the most significant predictor of the development and progression of type 2 diabetes complications. Compared with a HbA1c value of 48–58 mmol/mol (6.5%–7.5%), individuals with a HbA1c level of 70–80 mmol/mol (7%–9.5%), within the first year after diagnosis, have a threefold increased risk of developing microvascular complications. This risk is sixfold for those with HbA1c levels of 90–100 mmol/mol (10.4%–11.3%). 4\n\nConsequently, therapeutic inertia with delayed treatment of hyperglycemia is related to significant healthcare and labor market costs. 5  While therapeutic inertia is a major problem in Denmark affecting about 50% of the individuals diagnosed with type 2 diabetes, approximately 165 000 cases, 5  a critical concern is the large proportion of undiagnosed and untreated cases. It is estimated that 25% of the individuals with type 2 diabetes, about 100 000 cases in Denmark, are undiagnosed. 6\n\nThe early-stage type 2 diabetes is often asymptomatic, 7  which contributes to delayed diagnosis, thereby increasing the likelihood of developing complications. One third of people diagnosed with type 2 diabetes already have complications at the time of diagnosis. 6 8  This is the main reason why screening is recommended for type 2 diabetes. 9\n\nType 2 diabetes meets many of the established criteria for screening to be effective (10). It has (1) a long asymptomatic phase during which complications may develop, (2) simple, reliable and cost-effective diagnostic tests and (3) well-established interventions for glucose, blood pressure and lipid management.\n\nScreening is generally approached within three broad screening schemes: (1) universal or population-based screening through a general direct outreach, (2) targeted screening approaching selected groups who are at high risk of having type 2 diabetes using risk assessment surveys or biomarkers such as hypertension and (3) opportunistic screening examining individuals who are presenting to health services for other reasons.\n\nThe scarce randomized controlled trial (RCT) studies are unable to document that population-based screening for type 2 diabetes reduces neither morbidity nor mortality in the general population. 10 12  This is, however, not surprising, as screening for type 2 diabetes identifies only a small fraction of the individuals in the general population, who will experience CVD or advanced mortality. Hence, the impact of screening on population-level CVD and mortality remains minimal, even if early detection improves outcomes for those diagnosed, 13  as suggested by non-randomized controlled studies. 14 15\n\nTo identify an effect of population-based screening, the number needed to invite in both trial arms is substantial, while healthcare records are required to identify type 2 diabetes as the cause of comorbidities and mortality of the individuals with undiagnosed type 2 diabetes. 16 17  In Denmark, the number needed to invite exceeds the number of individuals in the population, not currently screened opportunistically. In addition, despite high-quality healthcare data, type 2 diabetes remains under-reported as the underlying cause of diabetes comorbidities and mortality. 6\n\nIn contrast, non-randomized simulation studies as well as Systematic Review and Meta-Analysis suggest that targeted and opportunistic screening reduces morbidity and mortality, 14 15  improve quality-adjusted life-year and are cost-effective at screening every 3–5 years in adults aged 45 and above with normal blood glucose levels, but every 2 years in high-risk groups. 18 20  Accordingly, the WHO and the National Institute for Clinical Excellence recommend targeted and opportunistic screening of the population at risk of having type 2 diabetes. 21 23  Non-systematic opportunistic screening in clinical settings is on the rise in Denmark. In 2018, approximately 50% of 60-year olds, 60% of 70-year olds and 68% of 80-year olds had at least one HbA1c measurement recorded within the last year. 24  However, opportunistic screening may not screen less health-conscious and vulnerable individuals. 25  Moreover, individuals living with unknown type 2 diabetes may not regularly visit their general practitioner (GP).\n\nWe suggest a fourth screening scheme using at-home self-sampling HbA1c test targeting individuals without a HbA1c measurement within the last 2 years. The quality of the national registers allows screening programs to target individuals, not tested opportunistically, while novel and cheaper test and storage methods allow sampling to take place outside the clinic settings. The aim of the study is to evaluate whether screening for type 2 diabetes using at-home self-sampling HbA1c tests, targeted individuals between 50 and 75 years of age who have not tested their HbA1c level within the last 24 months, is feasible and cost-effective in Denmark.\n\nThe study employed a two-stage design. First, an at-home self-sampling screening pilot study was conducted to evaluate the feasibility and to obtain estimates of the participation and detection rates as well as the distribution of the HbA1c level among participants. Second, the estimates were applied to calculate the healthcare and labor market cost savings associated with advanced type 2 diabetes detection through self-sampling. This analysis leveraged existing simulations on the economic cost of therapeutic inertia at varying HbA1c levels 1-year, 3-year and 5-year post-diagnosis.\n\nOver a period of 9 months, free at-home capillary blood sampling kits were distributed to 8000 randomly selected individuals aged 50–75 years without an HbA1c measurement over the past 2 years. The population was identified using the Danish Clinical Laboratory Information Register (LABKA), which compiles laboratory test results from hospitals and GPs. The decision to target individuals aged 50–75 years was based on the current level of opportunistic screening for HbA1c by age group in Denmark, 3  while the 2-year threshold prioritized identifying individuals at elevated risk of developing type 2 diabetes, 20  although at the expense of a lower overall detection rate and cost-effectiveness. The sample size was restricted due to practical reasons, that is, resources and time.\n\nParticipants received an invitation letter together with a leaflet with (1) instructions on how to collect a capillary blood sample using the provided self-sampling kit, (2) information on how to return the sample for laboratory HbA1c analysis using a prepaid envelope and (3) information about how to access the test results. The test results were recorded in LABKA and made accessible within 2 weeks to the participants and their GPs through their electronic health record. Participants with a HbA1c level of 48 mmol/mol (6.5%) and above were contacted by the doctor before they were able to access their results.\n\nThe at-home capillary blood sampling kit consisted of a Hemoglobin Capillary Collection System (HCCS), integrating collection, preparation and transport of blood samples for HbA1c analysis, and two 1.5 mm single-release finger-stick blood lancets. Sample stability was verified in an internal pilot verification study, in which 26 individuals from an outpatient clinic provided a total of 47 capillary samples analyzed immediately and after mailing at day 6, 11 and 13. No significant differences were observed in HbA1c levels across days, confirming that the HCCS samples remained stable for at least 13 days without refrigeration. The blood sample’s HbA1c was analyzed in the Lab with a Tosoh analyzer. The Tosoh analyzer uses ion-exchange high-performance liquid chromatography, which is regarded as the gold standard due to its high precision, rapid analysis times and the ability to separate HbA1c from other hemoglobin fractions for reliable results. 26\n\nDescriptive statistical analyses were conducted to examine participation and detection rates as well as HbA1c results by sex and age groups. Similarly, the sex and age distributions among invited and participants were compared with the general population, while the sex and age distribution of the screen-detected was compared with individuals diagnosed with type 2 diabetes through the Danish Health Care System (DHCS) in 2023. The sex and age distribution of the general population is derived from Statistics Denmark, while the sex and age distribution of those diagnosed through DHCS is derived from the Register of Selected Chronic Diseases and Severe Mental Disorders. Comparisons were performed using the χ 2  test, with a significance threshold of p<0.05.\n\nTo estimate the cost savings associated with earlier type 2 diabetes diagnosis through screening using at-home self-sampling, we applied a Danish simulation study on the healthcare and labor market cost of therapeutic inertia for individuals with type 2 diabetes across HbA1c targets as defined by the Danish Endocrinological Society, 4  including the HbA1c targets of 48 mmol/mol, 53 mmol/mol (7%) and 63 mmol/mol (7.9%). Moreover, we weighted the estimated costs of therapeutic inertia for each HbA1c target group derived from Lindvig and colleagues 5  by the distribution of HbA1c observed among screen-detected participants in the screening study, assuming that the HbA1c distribution of the participants is representative for a national screening program in Denmark. The calculations were repeated across the different scenarios of earlier diagnosis.\n\nLindvig and colleagues 5  apply a type 2 diabetes model developed by the Swedish Institute for Health Economics (IHE) to calculate the increased incidence of microvascular and macrovascular complications as well as mortality over a 40-year time horizon due to therapeutic inertia for 1, 3, 5 and 7 years following diagnosis, compared with full glycemic control of 6.5% (48 mmol/mol). Subsequently, they calculate the societal costs of diabetes complications and mortality applying Danish register-based estimates provided by Kjellberg  et al 27  on the direct healthcare expenditures for treatment of type 2 diabetes and comorbidities as well as indirect labor market costs incurred the first 3 years following an incident of each of the 17 most common diabetes complications. The indirect costs further include absenteeism associated with complications, provided by Sørensen and Ploug. 28  Income statistics were sourced from Statistics Denmark. Hence, the model inputs, including clinical and cost parameters, are user-defined. For detailed model inputs, settings and cost estimates, see Lindvig  et al . 5  We further deduct the treatment costs due to additional patient years following advanced diagnosis from the estimated cost savings. The treatment costs per patient year comprise of GPs chronic fee, costs to yearly tests and self-paying for foot screening.\n\nThe calculations are repeated across three scenarios of early diagnosis, including 1, 3 and 5-year earlier diagnosis. Based on evidence from Denmark 8 14  indicating that identifying undiagnosed individuals with type 2 diabetes advances the time of diagnosis by about 3 years, we have chosen this as our preferred scenario.\n\nFinally, the net cost savings were compared with total screening costs per screen-detected to estimate the return on investment of type 2 diabetes screening using at-home self-sampling. The total screening costs per screen-detected are calculated as (I+S*P)/(P*D), where I is the invitation costs per invited, S is the screening costs per screened, P is the participation rate and D is the detection rate. The invitation costs per invited include the letter, packaging, postage and the at-home self-sampling equipment. The costs of identifying eligible individuals are not included, as it is possible to fully automate this process. The screening costs per screened encompass return postage and the laboratory analysis cost. We apply the participation and detection rate reported in the at-home self-sampling screening study to calculate screening costs.\n\nWe assume that the screening costs per screen-detected are reduced in a screening program organized by the National Health authorities. First, evidence from Denmark suggests that a national screening program increases participation rate compared with pilot screening studies while the share of screen-detected remains constant as the participation rate increases. Second, with a larger volume of samples, it is possible to reduce purchase prices and fixed costs, for example, associated with data extraction, while workflows can be automated and thus reduce working time per sample. A plausible but cautious scenario is to consider a 10 percentage point higher participation rate and 10 percentage reduction in total costs due to scale effects.\n\nKraka Economics was contracted to conduct the economic calculations based on the results on the HbA1c distribution among participants in the screening study and the existing simulation study.\n\nThe participants received information about the pilot study, an informed consent form, with relevant information about data analysis and storage and the ethical aspects of participating in a screening campaign that they needed to fill with personal information, including personal ID number and contact details. A filled and signed consent was required for the invited individuals to participate in the study.\n\nAll data were anonymized and processed in accordance with Danish Law and the General Data Protection Regulation 2016/679 of The European Parliament and of The Council. The participants took part in the study voluntarily and consented on their data to be used anonymized in the study.\n\nAlthough there are no physical side effects in screening for type 2 diabetes with a capillary test, screening for a disease that is asymptomatic in early stages may lead to stress and/or anxiety. Participants with a HbA1c value of 48 mmol/mol (6.5%) and above were contacted by a medical doctor associated with the project to ensure that participants were informed about the implications of the result and how to handle the situation, before being able to access their result.\n\nThe participation rate was 38%. However, due to insufficient blood samples, the successful participation rate was 36%, 2913 individuals. As central administrative register data were applied for the selection of eligible individuals, information on sex and age was complete for all invitees, participants, and screen-detected individuals.\n\nOf the successfully tested participants, 1.7% (50 individuals) were detected with HbA1c of 48 mmol/mol (6.5%) or above, while an additional 10.2% (298 individuals) had HbA1c of 42–47 mmol/mol (6%–6.4%). Half of the screen-detected had HbA1c values from 48 to 53 mmol/mol (6.5%, 7%), 24% a HbA1c level of 53 to 63 mmol/mol (7%, 7.9%), and 26% a HbA1c of 63 mmol/mol (7.9%) or above.\n\nInvited individuals were evenly distributed by sex but not across age groups. Younger age groups were overrepresented among those invited, compared with the general population, reflecting the lower likelihood of having had an HbA1c measurement within the past 2 years. Specifically, 30.1% of all invited individuals were aged 50–54 years; 18.4% aged 60–64 years and 10.1% aged 70–75 years. The age distribution of the successfully tested participants mirrored that of the invited individuals for both men and women, as participation rates were similar across all age groups, but slightly higher among women than men (38% and 35%) ( table 1 ).\n\nThe share of screen-detected with HbA1c of 48 mmol/mol (6.5%) or above was 2.32% among men and 1.14% among women. Individuals aged 50–59 years constituted 50% of the screen-detected compared with 40% in the same age group among those diagnosed by the DHCS. While this represents a substantial difference in the age distribution, it is not statistically significant ( table 2 ).\n\nIf screening detects individuals with type 2 diabetes 3 years earlier, the cost savings constitute €97 in direct costs and €168 in indirect cost for every screen-detected individual with a HbA1c of 48–53. For screen-detected with a HbA1c of 53–63 and above 63, the cost savings amount to €217 and €376 in direct cost as well as €377 and €652 in indirect cost, respectively,  table 3 , as derived from Lindvig  et al . 5  The total direct costs constitute €690, while the indirect costs constitute €1197, when weighted by the HbA1c distribution among screen-detected participants in the screening study resulting in total cost savings of €1887. The costs of additional patient years are deducted from the gains equivalent to €124 per patient year. Hence, the socioeconomic gain constitutes €1514 per screen-detected,  table 4 , if screening results in at least 3 years earlier diagnosis. If the screening detects individuals with type 2 diabetes 1 or 5 years earlier, the total socioeconomic gain constitutes €575 and €2219, respectively.\n\nWe apply the invitation costs per invited (€5.89), the screening cost per screened (€9.67) as well as the participation rate (0.36415) and detection rate (0.0171644) obtained from the at-home self-sampling screening pilot study to estimate the screening costs per screen-detected, while assuming that a national screening program provides both a 10 percentage point increase in the participation rate and a 10% reduction in screening costs due to scale effects. Hence, the screening costs per screen-detected of a national screening program are estimated to\n\nIf screening results in a 3-year earlier diagnosis of type 2 diabetes, it provides a societal return rate of €1514/€1183=1.28, corresponding to approximately 28%. For every euro invested in the screening program, the society saves 1.28 on treatment and lost income for people with type 2 diabetes as the screening program advances the time of diagnosis and treatment and thus reduces the risk of diabetes complications that require costly treatment and lead to sick leave. This assessment excludes the substantial health benefits for quality of life and the value of this (QALY). Recent Danish cost-effectiveness studies for type 2 diabetes therapies often use a willingness-to-pay threshold of approximately DKK 224 000 per QALY (€30 000 per QALY) as a benchmark for cost-effectiveness. 29\n\nOur findings, supported by existing evidence, suggest that screening for type 2 diabetes using at-home self-sampling HbA1c tests, targeted to individuals aged 50–75 years without a HbA1c measurement within the last 2 years, is feasible and cost-effective in Denmark.\n\nThe share of screen-detected with HbA1c of 48 mmol/mol (6.5%) or above is consistent with the estimated share of individuals with unknown type 2 diabetes in Denmark in the same age group. 6  The distribution of HbA1c among screen-detected participants is not statistically different from the distribution among patients diagnosed through the DHCS. Half of the newly diagnosed with type 2 diabetes through the DHCS in 2018 had a HbA1c level of 53 mmol/mol (7%) or above, while 31% had a HbA1c level over 62 mmol/mol (7.9%). 30  However, individuals aged 50–59 are slightly over-represented of the participants compared with the individuals diagnosed by the DHCS (51% vs 40%).\n\nFinally, we estimate that a targeted screening program advancing diagnosis by 3 years across all HbA1c levels results in an average cost savings of €1.514 per screen-detected and a total social return ratio of 1.28.\n\nA key strength of the study is the novelty of the screening method. To our knowledge, this is the first study investigating screening for type 2 diabetes by applying an at-home self-sampling HbA1c test kit. In contrast, most evidence concerns screening in clinical settings using fasting plasma glucose or oral glucose tolerance tests. Another strength is the accuracy in identifying the individuals without a HbA1c measurement within the last 2 years. This is possible due to LABKA’s quality and completeness.\n\nThe cost-effectiveness results require cautious interpretation due to study limitations. First, while the estimated cost savings related to undiagnosed type 2 diabetes derived from Lindvig  et al 5  are based on the IHE model, 31  the estimated cost savings have a long time horizon. Similarly, the estimated cost savings are for simplicity provided within HbA1c intervals, reducing the accuracy of the estimated cost-effectiveness. We further assume that the screening program advances the time of diagnosis by 3 years. Previous literature suggests that screening advances type 2 diabetes diagnosis by 3–6 years, 8  while two later studies report an average of 2.2 years compared with clinically detected cases. 14 32\n\nFurthermore, some of this study’s assumptions likely result in an underestimation of the true economic burden of undiagnosed type 2 diabetes and the potential cost savings from earlier diagnosis. First, in line with Lindvig  et al , 5  we assume constant HbA1c levels in the prediagnosis period. However, HbA1c levels increase over time due to a declining β-cell function decreasing patients’ ability to maintain glycemic control. Second, other biomarkers associated with diabetes progression and risk-enhancing behaviors, including systolic blood pressure, dyslipidemia, body weight, smoking and alcohol intake, are assumed to remain constant before and after diagnosis. However, type 2 diabetes diagnosis and treatment may lead to improvements in other biomarkers and risk-related behaviors. 11  Third, the healthcare cost does not include municipal expenses such as rehabilitation, nursing care and diabetes aid costs. 16  Prior research indicates that municipal nursing care constitutes a significant cost associated with diabetes complications. 33  Moreover, the study does not include non-pecuniary effects of early diagnosis and treatment, such as an increased quality of life for the screen-detected and their relatives. Finally, the study does not estimate the optimal frequency of screening. The 2-year threshold prioritizes identifying individuals at elevated risk of developing type 2 diabetes, 20  although at the expense of a lower overall cost-effectiveness.\n\nConversely, other assumptions may result in an upward bias in the estimated cost savings. First, screen-detected participants are assumed to lower their HbA1c to 48 mmol/mol (6.5%), following diagnosis. This is a simplification of real-world clinical practice and may overestimate the cost savings from early detection. The Danish type 2 diabetes treatment guidelines 4 34  define four therapeutic HbA1c targets allowing for individualized treatment considering disease progression and comorbidities. Furthermore, HbA1c levels may increase over time despite of treatment, while treatment may intensify, as suggested by evidence from real-life clinical practice. 34 35  Second, costs associated with disease management during additional patient years are deducted from the economic cost savings of early detection. It includes GPs chronic fees, costs of yearly tests and self-paid foot screening. Type 2 diabetes drug costs and adverse drug-specific health outcomes are not included as disease management cost. Sortsø  et al 36  find, however, that most direct treatment costs are due to diabetes complications while diabetes treatment accounts for only 8% of total direct costs.\n\nFinally, alternative screening approaches and voluntary self-measurement should be acknowledged for further research. Lower cost methods, such as urine dipstick testing, may increase participation and reduce screening costs per screen-detected. However, inherent limitations such as lower sensitivity and diagnostic specificity may lead to missed cases or increased need for confirmatory testing. Hence, empirical evidence directly comparing the cost-effectiveness of at-home HbA1c self-sampling with other low-cost screening strategies are needed. In parallel, the increasing availability of personal glucose measurement technologies necessitates consideration of voluntary self-measurement. While such technologies may expand testing among health-conscious individuals, voluntary use is subjected to self-selection and is less likely to reach populations at highest risk of undiagnosed type 2 diabetes. Hence, future evaluations of screening programs should consider integrating self-reported measures with appropriate clinical follow-up, while maintaining a focus on under-screened populations.\n\nThis study contributes to the existing literature by being the first to evaluate the feasibility and cost-effectiveness of screening for type 2 diabetes using at-home self-sampling HbA1c tests. The study targets individuals without a HbA1c measurement, filling a gap in our understanding of diabetes screening strategies.\n\nThe study suggests that the at-home self-sampling screening setup is cost-effective. Importantly, this method should be viewed as complementary rather than competing with other screening strategies.\n\nBy implementing a diverse array of screening strategies tailored to different population segments, healthcare providers can identify a broader spectrum of individuals with type 2 diabetes at earlier stages. Hence, reducing the share of diabetes complications and excess mortality and consequently lowering the direct and indirect economic burden of type 2 diabetes.\n\nWhile these results are promising, future research is needed to evaluate the feasibility and the long-term cost-efficiency of different screening initiatives across national settings.",
    "mesh_query": "mass screening[MeSH Terms]",
    "source_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12911741/",
    "data_crawling": "2026-02-19T18:22:08.794795"
  },
  {
    "pmc_id": "12910275",
    "title": "Developing a Multimodal Screening Algorithm for Mild Cognitive Impairment and Early Dementia in Home Health Care: Protocol for a Cross-Sectional Case-Control Study Using Speech Analysis, Large Language Models, and Electronic Health Records",
    "abstract": "Background Mild cognitive impairment and early dementia (MCI-ED) are frequently unrecognized in routine care, particularly in home health care (HHC), where clinical decisions are made under time constraints and cognitive status may be incompletely documented. Federally mandated HHC assessments, such as the Outcome and Assessment Information Set (OASIS), capture health and functional status but may miss subtle early cognitive changes. Speech, language, and interactional patterns during routine patient-nurse communication, together with information embedded in unstructured clinical notes, may provide complementary signals for earlier identification. Objective This protocol describes the development and evaluation of a multimodal screening approach for identifying MCI-ED in HHC by integrating (1) speech and interaction features from routine patient-nurse encounters (verbal communication), (2) large language model–based extraction of MCI-ED–related information from HHC notes and encounter transcripts, and (3) structured variables from OASIS. Methods This ongoing cross-sectional case-control study is being conducted in collaboration with VNS Health (formerly Visiting Nurse Service of New York). Eligible participants are adults aged ≥60 years receiving HHC services. Case/control assignment uses a 2-stage process: electronic health record (EHR) prescreening followed by clinician-reviewed cognitive assessment (Montreal Cognitive Assessment and Clinical Dementia Rating) for consented participants without an existing mild cognitive impairment diagnosis. For Aim 1, each participant contributes 3 audio-recorded routine patient-nurse encounters linked to EHR data, including OASIS and free-text clinical notes. Aim 1 extracts acoustic, linguistic, emotional, and interactional features from patient-nurse verbal communication. Aim 2 uses a schema-guided large language model pipeline to extract and normalize MCI-ED–related symptoms, lifestyle risk factors, and communication deficits from HHC notes and encounter transcripts, supported by a human-annotated gold-standard dataset. Aim 3 integrates speech, extracted text variables, and OASIS predictors using supervised machine learning with stratified nested cross-validation; evaluation will include discrimination, calibration, and subgroup performance checks across race, sex, and age. Results Between February 2024 and July 2025, a total of 114 HHC patients completed study-administered cognitive assessments and were classified as 55 MCI-ED cases and 59 cognitively normal controls. Audio-recorded patient-nurse encounters had a median duration of 19 (IQR 12-23) minutes and a median of 56 (IQR 31-80) utterances per encounter; nurses contributed more words than patients (median 842, IQR 461-1218 vs median 589, IQR 303-960). In exploratory feasibility analyses, multimodal models integrating speech, interactional features, and structured EHR/OASIS variables outperformed single-source models. Conclusions This protocol describes a reproducible multimodal framework for MCI-ED screening in HHC using routinely generated data streams. Initial implementation results support feasibility of data collection and end-to-end processing and suggest potential value of integrating interactional speech features with clinical text and OASIS variables. Final model evaluation, subgroup analyses, and validation will follow the prespecified analytic procedures on the finalized study dataset. International Registered Report Identifier (IRRID) DERR1-10.2196/82731",
    "text": "Alzheimer disease and Alzheimer disease–related dementias are among the most pressing global public health challenges. In 2021, an estimated 57 million people were living with dementia worldwide, with over 60% residing in low- and middle-income countries and nearly 10 million new cases occurring each year [ 1 ]. Dementia is associated with substantial disability, caregiver burden, and rapidly rising health-system costs [ 2 ]. In the United States, an estimated 7.2 million adults aged ≥60 years were living with Alzheimer dementia in 2025, underscoring the scale of need in high-income settings as well [ 3 ]. Alongside treatment advances and growing interest in risk reduction, global guidance continues to emphasize the importance of timely detection and prevention-oriented interventions across diverse populations and care contexts [ 4 , 5 ].\n\nDespite this urgency, a large proportion of cognitive impairment, particularly mild cognitive impairment (MCI) and early-stage dementia, remains unrecognized or undocumented in routine care, contributing to delayed diagnosis and missed opportunities to tailor care planning [ 6 , 7 ]. In home-based care settings, documentation gaps can be especially consequential because care teams must make clinical decisions in the context of multimorbidity, limited visit time, and incomplete prior cognitive history [ 8 , 9 ]. Recent evidence from skilled home health care (HHC) shows that dementia is frequently undocumented in home health records, illustrating how care transitions and documentation practices can impede recognition of cognitive impairment [ 10 ]. These realities motivate scalable screening approaches that can operate within routine workflows, rather than relying solely on specialist evaluation or resource-intensive testing.\n\nSpeech and language have emerged as promising noninvasive, low-burden digital biomarkers for cognitive impairment [ 11 - 14 ]. A recent systematic review and meta-analysis focused on MCI specifically concluded that speech-based biomarkers show meaningful diagnostic use, while also highlighting methodological heterogeneity and the need for validation in diverse settings and populations [ 15 ]. At the same time, much of the speech-based Alzheimer disease and Alzheimer disease–related dementias detection literature remains anchored in structured elicitation tasks [ 16 ] (eg, cookie-theft picture description) and benchmark corpora (eg, DementiaBank-derived shared tasks such as ADReSSo [ 17 ]), which enable comparability but may not capture interactional and pragmatic markers expressed during everyday clinical communication [ 15 , 18 ]. Multilingual and cross-cultural work further indicates that generalization across languages and contexts cannot be assumed; for example, multilingual spontaneous speech studies (eg, Italian and Spanish) demonstrate feasibility outside English-centric benchmarks but also reinforce the importance of ecologically valid sampling and external validation [ 13 , 19 - 21 ].\n\nThese limitations are particularly relevant for MCI and early dementia, where impairments can be subtle, context-dependent, and potentially expressed through conversational dynamics (eg, timing, turn-taking balance, and discourse coherence) rather than only through content produced during structured tasks [ 16 ]. This motivates studying routine patient-clinician conversations, where interactional features may provide an additional signal for early-stage cognitive change in real-world contexts [ 22 ].\n\nA complementary and underused source of early cognitive signals is unstructured clinical documentation, including HHC nursing notes [ 10 , 23 ]. Recent reviews show that natural language processing (NLP) approaches applied to electronic health record (EHR) notes can identify cognitive impairment with strong median performance across studies, but variability in diagnostic criteria, data sources, and external validation remains a key barrier to translation [ 24 ]. In parallel, large language model (LLM) methods are increasingly being evaluated for detecting cognitive decline from clinical notes, including large clinical language model approaches (eg, CD-Tron) and comparative studies of LLMs in real-world clinical text [ 25 - 29 ]. These developments suggest that LLM-enabled extraction can help capture both explicit and implicit mentions of symptoms, risk factors, and functional concerns that are inconsistently represented in structured fields, an especially relevant issue in home-based care workflows.\n\nHHC is therefore a compelling setting for scalable, equity-oriented screening because it provides repeated encounters and routinely generates multiple complementary data streams, including standardized assessments (eg, Outcome and Assessment Information Set [OASIS] in US Medicare–certified home health agencies), narrative nursing notes, and patient-nurse verbal communication. Our prior work in HHC has shown that combining structured assessment data with information extracted from clinical notes can improve risk identification (HomeADScreen [ 12 ]). More recently, we demonstrated the potential value of leveraging audio-recorded patient-nurse verbal communication as an additional signal beyond EHR data for early cognitive screening in HHC [ 16 ]. However, few studies have jointly leveraged (1) standardized home-care assessments, (2) unstructured home-care clinical notes, and (3) routine patient-clinician conversations within a single integrated screening framework for mild cognitive impairment and early dementia (MCI-ED) in home-based care.\n\nAccordingly, this study describes an ongoing protocol to develop and evaluate a multimodal screening approach for identifying MCI and early dementia in HHC using routinely generated data streams: standardized assessment data (OASIS), HHC nursing notes, and audio-recorded patient-nurse verbal communication. We aim to (1) model speech, language, emotion, and interaction patterns from patient-nurse conversations using automated speech analysis, (2) apply NLP/LLM methods to identify MCI/early dementia–related symptoms, lifestyle risk factors, and communication deficits from both clinical notes and verbal communication, and (3) integrate these signals with standardized assessment variables to improve screening performance compared with models based on any single data stream.\n\nThis protocol describes an ongoing cross-sectional case-control protocol in collaboration with VNS Health (formerly Visiting Nurse Service of New York), one of the largest HHC systems in the United States. The study population includes adults aged 60 years and older who receive HHC services from VNS Health. The protocol is designed to develop and evaluate a multimodal screening algorithm for identifying MCI-ED in HHC.\n\nAim 1 focuses on modeling speech, language, emotional expression, and interaction patterns during routine patient-nurse encounters (verbal communication) as markers of early cognitive decline in HHC. The primary analytic cohort includes non-Hispanic Black and non-Hispanic White patients receiving HHC services from VNS Health. These groups were selected because they are highly represented in the study setting, enable adequately powered comparisons within a single HHC system, and facilitate evaluation of model performance across racial groups—particularly important given well-documented disparities in dementia diagnosis and care for Black patients.\n\nPotential participants are identified through EHR-based screening and clinician referral workflows within VNS Health. Prespecified EHR indicators are used to identify likely cases (eg, documented symptoms of cognitive decline) and likely controls (no evidence of impairment). Eligible patients are approached during an active episode of HHC. Recruitment is monitored to achieve representation of both racial groups and, when feasible, balance across key characteristics, age, sex as a biological variable, and education.\n\nEligible participants are aged ≥60 years, plan to receive VNS Health services during the study period, have sufficient English proficiency to communicate independently with HHC nurses, have adequate vision/hearing to complete cognitive testing, and can provide written informed consent. Patients are excluded if they (1) are unable to communicate independently with the HHC nurse in English and (2) have speech or language disorders due to neurological conditions other than MCI-ED (eg, Parkinson disease or seizure disorders). Full eligibility criteria are provided in  Multimedia Appendix 1 .\n\nWe use a 2-stage approach to identify patients for case and control groups. First, we identify potential cases using available  ICD-10  ( International Statistical Classification of Diseases, Tenth Revision ) diagnoses in the EHR ( ICD-10  G31.84 for MCI) and identify potential controls as patients without documented cognitive impairment. Second, all consented participants without an existing MCI diagnosis complete cognitive assessments—the Montreal Cognitive Assessment (MoCA) [ 30 , 31 ] and Clinical Dementia Rating (CDR) [ 31 ]—in their homes, administered by a trained research assistant who audio-records responses to support final group assignment.\n\nA study clinician with expertise in cognitive impairment detection reviews the recorded cognitive assessments together with relevant clinical context (medical history and nurse assessment information from OASIS) to confirm group assignment. Based on prespecified criteria, participants are classified as MCI-ED cases when findings are consistent with early cognitive impairment (anticipated CDR 0.5-1 and MoCA ~16-25, with consideration of EHR evidence when available) and as cognitively normal controls when findings are within normal limits (CDR 0 and MoCA ≥26), and there is no EHR evidence of cognitive impairment. Participants meeting criteria for moderate to severe impairment (eg, CDR 2-3 or MoCA <16) are excluded because the protocol focuses on MCI-ED.\n\nAfter group allocation, patients in both the case and control groups are invited to provide additional consent for the next phase of Aim 1, which includes audio-recording routine patient-nurse encounters.\n\nThis study was reviewed and approved as human participant research by the Columbia University Irving Medical Center Institutional Review Board (Protocol AAAU3168). The study is conducted in collaboration with VNS Health and complies with all applicable institutional, federal, and regulatory requirements for research involving human participants.\n\nWritten informed consent is obtained from all participating patients prior to enrollment. Consent includes permission for the administration of cognitive assessments, including the MoCA and the CDR, audio-recording of patient-nurse encounters, and linkage of audio recordings and assessment data with EHR information. HHC nurses also provide informed consent for participation, including consent for audio-recording of patient-nurse encounters. Participants are informed of the study purpose, procedures, potential risks, and their right to withdraw at any time without affecting their care or employment.\n\nAll study data are handled in accordance with Health Insurance Portability and Accountability Act and institutional data protection policies. Audio recordings, transcripts, cognitive assessment data, and clinical text are deidentified prior to analysis, with direct identifiers removed. Data are stored on secure, access-controlled servers at Columbia University and VNS Health with role-based permissions and audit logging. Access to identifiable data is restricted to authorized study personnel only. Deidentified datasets are used for analysis, and results are reported in aggregate to minimize the risk of participant reidentification.\n\nEach participating patient receives a US $50 incentive for completion of cognitive assessments (MoCA and CDR) and an additional US $50 incentive for participation in audio-recording of patient-nurse encounters. HHC nurses also receive a US $50 incentive for participation in audio-recording of patient-nurse encounters. Incentives are provided in accordance with institutional review board–approved procedures and are not contingent on study outcomes.\n\nFor each enrolled participant, 3 routine patient-nurse encounters are audio-recorded during the HHC episode of care. Recording multiple encounters provides repeated observations to capture within-person variability in speech, language, and interactional patterns across visits, while minimizing participant and clinician burden. When both patient and nurse provide consent, a trained research assistant attends the visit and operates a Saramonic Blink audio-recording device [ 32 ], minimizing burden on clinical staff. This portable device, with dual wireless microphones that attach to clothing, provides clear speech transmission to devices like an iPod and offers dual-channel storage.\n\nAudio-recorded encounters are linked to EHR data extracted from the VNS Health system, including the OASIS [ 33 , 34 ]—a federally mandated HHC assessment capturing patient health status, functional status, and living arrangements—as well as supplemental structured data (eg, medications) and free-text clinical notes. Free-text notes include visit notes, documenting each nurse encounter, and care coordination notes, capturing communications with other clinicians, physicians, and family members.\n\nPrior to this protocol, we conducted a series of pilot studies to establish the feasibility of audio-recording patient-nurse verbal communication and applying automated speech and machine learning methods in the HHC setting [ 22 ]. First, we evaluated several commercially available audio-recording devices in laboratory and real-world HHC settings, assessing usability, transcription quality, and acceptability among HHC nurses and patients. Based on System Usability Scale scores and transcription accuracy measured by word error rate, the Saramonic Blink device [ 35 ] demonstrated the best overall performance and was selected for use in the current study. Semistructured interviews with HHC nurses and patients further indicated that audio-recording was acceptable and had minimal perceived impact on routine care delivery.\n\nIn a second pilot study, we demonstrated the feasibility of automated speaker type identification in recorded HHC encounters using machine learning models trained on acoustic and lexical features, achieving satisfactory classification performance [ 36 ]. In a third pilot study, we developed and validated an end-to-end analytic pipeline for modeling spoken language in cognitive impairment [ 11 ] ( Figure 1 ). The pipeline included (1) audio preprocessing for noise reduction; (2) automated speaker type identification to separate patient and clinician speech; (3) extraction of acoustic features capturing phonetic motor planning and voice characteristics (eg, fluency, frequency/spectral measures, intensity, and instability) using OpenSMILE [ 37 ] and PRAAT [ 38 ]; (4) modeling of emotional expression using the Geneva Minimalistic Acoustic Parameter Set [ 39 ] (GeMAPS) complemented by lexicon-based psycholinguistic markers [ 40 ] (using Linguistic Inquiry and Word Count [LIWC]); (5) modeling of language organization using transcript-derived lexical and syntactic measures—using Natural Language Toolkit (NLTK) [ 41 ]—and contextual language representations (using distilled RoBERTa [ 42 ]); and (6) machine learning–based classification with internal validation. We evaluated this pipeline on a benchmark dataset (DementiaBank [ 43 ] “Cookie Theft” picture descriptions) and observed strong discrimination between cognitively impaired and cognitively unimpaired participants, supporting the feasibility of extracting informative speech-derived markers and training predictive models. Collectively, these pilot studies informed the design decisions, data collection procedures, and analytic pipelines in this protocol.\n\nAnalytic pipeline for modeling spoken language. ADRD: Alzheimer disease and related dementias; LIWC: Linguistic Inquiry and Word Count; NLTK: Natural Language Toolkit.\n\nEarly cognitive decline affects multiple aspects of spoken communication, including speech motor control, language organization, emotional expression, and social interaction. In HHC settings, these changes are expressed during spontaneous patient-nurse verbal communications rather than structured speech production tasks (eg, reading task). Building on our preliminary feasibility and pilot work, Aim 1 focuses on systematically modeling these communication patterns using an automated speech analysis system. The objective is to extract complementary acoustic, linguistic, emotional, and interactional features from naturally occurring patient-nurse communications that may signal MCI-ED. The analytic framework for Aim 1 consists of 5 components, summarized in  Table 1 , which together capture core dimensions of speech production and interaction relevant to cognitive decline.\n\nModeling mild cognitive impairment and early dementia (MCI-ED) patient-nurse verbal communication in the home health care setting (components 1-4).\n\nSilent pauses: number of speechless intervals at the beginning of and between words [ 45 ].\n\nWithin-word disfluency: within-word silent pauses and sound prolongations [ 46 ].\n\nPairwise variability index: durational variability in successive acoustic-phonetic intervals [ 48 ].\n\nVowel duration: proportion of time of vocalic intervals in a sentence and the standard deviation of inter-vowel intervals [ 49 ].\n\nFundamental frequency: average number of oscillations originating from the vocal folds per second [ 50 ].\n\nFormant frequencies (F1-F4): acoustic resonances of the vocal tract due to changes in the positions of vocal organs [ 51 ].\n\nSpectral center of gravity: amplitude-weighted mean of harmonic peaks averaged over sound duration [ 52 ].\n\nLong-term average spectrum: composite signal representing the spectrum of the glottal source and resonant characteristics of the vocal tract [ 53 ].\n\nMel-frequency cepstral coefficients: energy variations between frequency bands of a speech signal [ 54 ].\n\nHarmonics-to-noise ratio: relative amount of additive noise in the voice signal [ 50 ].\n\nVoice breaks: reduced ability in vocal cord execution resulting in voice breaks [ 57 ].\n\nAcoustic voice quality index: weighted combination of time-frequency and quefrency-domain metrics developed to measure the severity of dysphonia [ 58 ].\n\nHammarberg index: articulatory effort computed as the difference between maximum energy in the 0-2 kHz band and the energy in the 2-5 kHz band [ 59 ].\n\nBandwidth of formants 1-3. Formant frequencies are acoustic resonances of the vocal tract caused by changes in vocal organ positions [ 51 ].\n\nLoudness: estimate of perceived signal intensity from an auditory spectrum [ 61 ].\n\nHarmonics-to-noise ratio: relative amount of additive noise in the voice signal [ 50 ].\n\nHarmonic difference H1-H2: difference between first and second harmonic amplitudes [ 62 ].\n\nHarmonic difference H1-A3: difference between H1 and A3 (energy of the highest harmonic in the third formant range) [ 62 ].\n\nMel-frequency cepstral coefficients: see frequency and spectral domain in component 1.\n\nMoving average type-token ratio: total number of unique words divided by the total number of words for each successive fixed-length window [ 63 ].\n\nBrunet index: variation in word types marked by part-of-speech tagging relative to the total number of words in a sentence [ 64 ].\n\nHonore index: proportion of words used only once relative to the total number of words [ 65 ].\n\nIncomplete (fragment) sentences: identified using an automatic detection algorithm based on syntactic parse trees and part-of-speech tagging [ 68 ].\n\nIdentification of filled pauses (eg, “um”) in the patient’s spoken language [ 69 - 72 ].\n\nUncertainty in patient language: computed using the linguistic approximator introduced by Ferson et al [ 73 ].\n\nMemory-related terms: proportion of sentences containing memory-related terms relative to the total number of sentences, computed using the NimbleMiner toolkit [ 74 , 75 ].\n\nQuestion ratio: proportion of interrogative sentences relative to the total number of sentences, identified using the NLTK a  Python package [ 74 ].\n\nTotal number of patient turns indicates frequency of information exchange [ 76 ].\n\nDialog interactivity: defined as the total number of patient turns divided by the total length of the encounter [ 76 ].\n\nLength of time of the patient’s turn; longer durations have been associated with difficulty in turn monitoring in MCI-ED [ 76 ].\n\nDiscernible pause rate: proportion of discernible speechless intervals at the start of patient turns relative to total utterances [ 77 ].\n\nCross-over speaking rate: proportion of patient-nurse utterances with cross-over speaking relative to total utterances during the interaction [ 77 ].\n\nFor reproducibility, all speech- and interaction-based parameters extracted in Aim 1 are explicitly specified in  Table 1 , organized by analytic component (phonetic motor planning, emotional expression, syntactic and semantic language organization, and patient-nurse interaction).  Table 1  provides the operational definition and measurement domain for each parameter. Acoustic features are computed using established toolkits (OpenSMILE [ 78 ] and PRAAT [ 38 ]); linguistic features are derived from automatically transcribed speech using NLTK, LIWC [ 79 ], and distilled RoBERTa; and interactional features are computed from speaker-labeled timestamps generated by Amazon Web Services (AWS) Transcribe.\n\nImpairment in phonetic motor planning is a well-documented consequence of neurodegenerative disorders, including MCI-ED, and manifests as reduced articulation precision, altered speech rhythm [ 44 , 80 , 81 ], and increased disfluency. To characterize these changes, we analyze acoustic parameters across six domains ( Table 1 , component 1): (1) speech fluency [ 45 , 46 , 82 ], (2) rhythmic structure [ 48 , 49 , 83 ], (3) frequency and spectral characteristics [ 45 , 84 , 85 ], (4) voice instability [ 45 , 69 , 86 ], (5) voice quality [ 87 - 89 ], and (6) voice intensity [ 45 , 90 ]. These measures quantify temporal and spectral aspects of speech that reflect the patient’s ability to plan and execute vocal motor actions.\n\nAlterations in emotional expression often develop alongside cognitive decline and can negatively affect communication quality and interpersonal interaction [ 91 , 92 ]. Emotion is conveyed both through nonverbal vocalization and semantic content [ 93 - 96 ]. To model vocal expression of emotion, we use the GeMAPS [ 39 ], which captures affect-related changes in autonomic arousal and vocal musculature via frequency-, energy-, and spectral-domain parameters ( Table 1 , component 2). To capture the semantic expression of emotion, we extract linguistically encoded emotional indicators using the LIWC dictionary. Emotion-related linguistic markers (eg, sadness and anxiety) have been associated with cognitive dysfunction [ 97 ] and adverse health outcomes [ 98 ].\n\nLanguage impairment in MCI-ED is characterized by reduced lexical diversity, simplified syntax [ 44 , 97 , 99 ], word-finding difficulties [ 44 , 100 ], and impaired memory-related discourse [ 44 , 101 ], which together contribute to reduced coherence. We model language organization using features in four domains ( Table 1 , component 3): (1) lexical richness [ 64 , 65 , 102 ], (2) syntactic complexity and grammaticality [ 66 , 103 ], (3) semantic fluency [ 104 ], and (4) patient recall ability [ 73 , 74 ]. Linguistic features are derived from automatically transcribed speech and processed using standard NLP tools. In addition, we will use distilled RoBERTa [ 42 ] to generate contextual language representations that capture semantic relationships beyond surface-level lexical features.\n\nCognitive impairment also affects social communication, including turn-taking, timing, and responsiveness [ 105 , 106 ]. Patients with MCI-ED may show recurrent interactional patterns, such as longer turns, delayed responses, or reduced interactivity [ 107 ]. To capture these phenomena, we will model patient-nurse interaction using easily measurable dialogue features [ 76 , 77 , 108 ] ( Table 1 , component 4), including patient turn counts, dialog interactivity, turn density, turn duration, and relative timing of turns. These interactional measures reflect how patients engage with clinicians in real-world care encounters and provide information beyond speech content alone.\n\nAcoustic parameters for components 1 and 2 will be computed at the utterance level (continuous blocks of uninterrupted patient speech) using OpenSMILE [ 37 ] and PRAAT toolkits [ 38 ]. Verbal communications are automatically transcribed using AWS Transcribe, after which linguistic features for component 3 are computed at the encounter level using the NLTK toolkit and distilled RoBERTa. Interactional features for component 4 are derived from AWS Transcribe metadata, including speaker labels and time stamps. All extracted features will be aggregated at the patient level for downstream integration with clinical data in Aim 3.\n\nMany clinical indicators of MCI-ED—including symptoms, lifestyle risk factors, and communication difficulties—are documented in free-text clinical notes or expressed during patient-nurse conversations [ 109 ], but are not captured in structured EHR fields [ 6 , 110 ]. Aim 2 uses LLMs to systematically extract this information from HHC clinical notes and transcripts of patient-nurse verbal communication. The goal is to convert unstructured text into standardized, patient-level variables that can be integrated with speech features (Aim 1) and structured assessment data (OASIS) for multimodal screening (Aim 3).\n\nFor reproducibility, all MCI-ED–related information identified in Aim 2 is defined using an information schema summarized in component 1 (Information targets and schema). The schema specifies the target information families (clinical symptoms, lifestyle risk factors, and communication deficits) and associated attributes, and is applied consistently across human annotation and LLM-based identification. MCI-ED–related information is normalized to standard clinical terminologies—Unified Medical Language System (UMLS) [ 111 ] concepts, when available—and represented in a structured patient-level format, supporting reproducible integration with Aim 1 features and OASIS variables.\n\nWe define a schema for the 3 MCI-ED–related risk factor categories, including clinical symptoms, lifestyle risk factors, and communication deficits. For each identified item, the system records (1) the related terms; (2) a normalized clinical concept identifier when available (UMLS [ 111 ] concepts); (3) clinically relevant attributes: assertion (present/absent/possible), temporality (current/historical), and experiencer (patient/caregiver); (4) severity and frequency; and (5) duration. The schema is used consistently across clinical notes and transcripts of patient-nurse communication.\n\nUsing the information schema defined in component 1, we create a human-annotated gold-standard dataset to support LLM adaptation and evaluation. The schema specifies the target information categories (clinical symptoms, lifestyle risk factors, and communication deficits) and associated attributes, including assertion, temporality, and experiencer.\n\nTwo trained nurse annotators independently annotate a stratified sample of HHC clinical notes and encounter transcripts according to this predefined schema. The annotation sample is stratified by race, sex, and visit type to ensure representation of diverse documentation patterns. Interannotator agreement is assessed using Cohen κ [ 112 ] for each information category and attribute, calculated on double-annotated samples prior to adjudication. Discrepancies are resolved through adjudication meetings to produce a finalized gold-standard dataset. The annotated corpus is subsequently partitioned into training, development, and test sets to enable LLM fine-tuning and unbiased performance evaluation.\n\nWe use a hybrid LLM-based strategy that combines prompted extraction and instruction tuning [ 113 ], both aligned with the information schema (component 1) and supervised by the human-annotated gold-standard dataset (component 2). First, prompted extraction (baseline): as a baseline approach, we apply structured prompts that explicitly define the 3 information families (clinical symptoms, lifestyle risk factors, and communication deficits) and required attributes (assertion, temporality, and experiencer). Prompts instruct the model to produce schema-compliant JSON outputs and to provide a supporting text span for each identified item to ensure evidence-grounded extraction. This prompted approach provides an interpretable, rapidly adjustable method for early experiments and error analysis. Second, instruction tuning (schema-guided supervised adaptation): to improve reliability on HHC-specific language and documentation patterns, we perform instruction tuning using the training split of the human-annotated gold-standard dataset. Training examples pair the input text (note or transcript segment) with the target output formatted as schema-compliant JSON, including the identified item type, attributes (assertion, temporality, and experiencer), and supporting span. This teaches the model to follow the extraction instructions consistently and to produce outputs that match the schema across diverse note styles and conversational phrasing. Instruction tuning is implemented using parameter-efficient methods (eg, Low-Rank Adaptation [ 114 ]/ Quantized Low-Rank Adaptation [ 114 , 115 ]) to reduce computational burden in the secure environment.\n\nFor each identified item in the text, we normalize the item (mention) to a standardized clinical concept identifier (UMLS, when available) using a controlled vocabulary lookup supplemented by string similarity matching for common variants. When multiple items map to the same concept within a document, we merge them into a single record while preserving the schema attributes (assertion, temporality, experiencer, and—when present—severity and frequency/duration) and retaining the supporting text spans for traceability. We then aggregate document-level outputs to the patient level to produce predictors for Aim 3. Patient-level variables summarize the presence of each normalized concept and its attributes across the patient’s available HHC notes and encounter transcripts. The final deliverable is a structured patient-level table of normalized MCI-ED–related symptoms, lifestyle risk factors, and communication deficits for integration with OASIS data and Aim 1 speech and interaction features.\n\nWe evaluate performance against the human-annotated gold-standard dataset using (1) span-level precision/recall/ F 1  under exact and overlap matching, (2) attribute performance (assertion, temporality, experiencer, and severity/frequency when applicable), and (3) concept normalization accuracy. We report results overall and stratified by race, sex, and age group to monitor for systematic performance differences. We conduct routine error analysis (eg, common false positives from templated note language, negation errors, or transcript artifacts) and use findings to refine prompts, update normalization resources, and adjust fine-tuning settings. Low-confidence outputs are flagged for targeted review during development to guide iteration and reduce systematic errors.\n\nThe final Aim 2 deliverable is a structured, patient-level dataset summarizing clinical symptoms, lifestyle risk factors, and communication deficits identified from HHC notes and transcripts, including normalized concept identifiers and clinically meaningful attributes. These variables are used as candidate predictors and complementary signals in the multimodal screening algorithm developed in Aim 3.\n\nThe objective of Aim 3 is to develop and evaluate a multimodal screening algorithm for identifying HHC patients with MCI-ED. The algorithm integrates complementary information from three routinely generated data sources: (1) speech and interaction features extracted from patient-nurse communication (Aim 1), (2) MCI-ED–related information identified from clinical notes and transcripts (Aim 2), and (3) structured assessment data from the OASIS.\n\nInput variables include (1) acoustic, linguistic, emotional, and interactional features derived from patient-nurse verbal communication (Aim 1); (2) normalized clinical symptoms, lifestyle risk factors, and communication deficits identified from clinical notes and encounter transcripts (Aim 2); and (3) structured OASIS variables capturing sociodemographic characteristics, diagnoses, medications, functional status, and related clinical information.\n\nPrior to model development, we assess data quality and address missingness, inconsistency, and integrity issues using a predefined data quality framework [ 116 ]. Continuous variables are transformed and scaled as appropriate to ensure comparability across modalities. To reduce dimensionality and mitigate overfitting in the presence of a large number of candidate predictors, we apply Joint Mutual Information Maximization [ 117 ] as a feature selection method. Joint Mutual Information Maximization is selected for its suitability in small to moderate sample settings with high-dimensional data, where it balances relevance to the outcome with redundancy among features.\n\nWe develop screening models using supervised discriminative machine learning algorithms that are appropriate for tabular and multimodal clinical data, including logistic regression, support vector machines (SVMs) [ 118 ], and ensemble tree-based methods [ 119 - 122 ]. These models are chosen for their interpretability, robustness, and reduced risk of overfitting in clinical datasets. Multimodal integration is performed by combining features from speech, clinical text, and OASIS data within a unified modeling framework. Models are trained to estimate the probability of MCI-ED at the patient level. Temporal aspects of speech-derived features are summarized at the patient level prior to modeling, rather than modeled using complex sequence architectures, to maintain feasibility and stability given sample size considerations.\n\nModel training and hyperparameter tuning are conducted using stratified nested cross-validation, with inner loops for parameter selection and outer loops for performance estimation. Model performance is evaluated using the area under the receiver operating characteristic curve (AUC-ROC) and area under the precision-recall curve, along with calibration measures to assess agreement between predicted risk and observed outcomes. To evaluate equitable performance, we assess model metrics stratified by race, sex, and age group. Fairness-related measures, including group-wise differences in sensitivity and specificity and calibration across subgroups, are examined. When systematic performance differences are observed, we explore mitigation strategies such as reweighting or threshold adjustment and reassess model performance.\n\nIn the final step, the selected model is evaluated on an independent validation dataset to provide an unbiased estimate of performance. The screening algorithm produces a patient-level risk score indicating the likelihood of MCI-ED, which is intended to support clinical awareness and referral for further cognitive evaluation rather than serve as a diagnostic tool. The resulting algorithm and associated feature sets are prepared for downstream evaluation of clinical use and integration into HHC workflows.\n\nBetween February 2024 and July 2025, we enrolled 114 HHC patients who met eligibility criteria and completed study-administered cognitive assessments. Following standardized review of the cognitive assessments and prespecified group-allocation procedures, 55 participants were classified as MCI-ED cases and 59 as cognitively normal controls. The cohort had a balanced sex distribution (n=58, 51% female) and was racially and ethnically diverse (n=63, 55.3% Black). Most participants were insured by Medicare (n=75, 66%), and 44% (n=50) lived alone.\n\nIn descriptive comparisons, participants classified as cognitively impaired had a higher prevalence of urinary incontinence (21/55, 36.8% vs 13/59, 21.4%), anxiety (32/55, 57.9% vs 23/59, 39.3%), and impaired vision (14/55, 26.3% vs 4/59, 7.1%), as well as greater dependence in activities of daily living (41/55, 73.7% completely dependent vs 38/59, 64.3%). These results are reported to characterize the cohort and should be interpreted descriptively rather than as definitive group differences.\n\nAudio-recorded patient-nurse encounters had a median duration of 19 (IQR 12-23) minutes and a median of 56 (IQR 31-80) utterances per encounter. Across encounters, nurses contributed more words than patients (median 842, IQR 461-1218 vs 589, IQR 303-960), consistent with the structure of routine HHC visits and motivating inclusion of interactional features.\n\nWe conducted exploratory modeling analyses to evaluate the feasibility of distinguishing MCI-ED cases from cognitively normal controls using (1) speech-derived measures, (2) clinical text, and (3) structured EHR/OASIS variables, as well as multimodal combinations of these data sources. These analyses are intended to assess feasibility and inform subsequent model refinement and validation, rather than to provide definitive estimates of performance.\n\nAcoustic and temporal speech features were encoded using SpeechDETECT, including parameters related to phonetic motor planning ( Table 1 , component 1). Vocal emotion-related cues were encoded using GeMAPS ( Table 1 , component 2). Linguistic features included handcrafted measures capturing lexical richness, syntactic complexity, and semantic/fluency markers (eg, repetition and filler words;  Table 1 , component 3), and psycholinguistic indicators were extracted using LIWC 2015. In addition, we evaluated pretrained transformer language models for transcript-based representations.\n\nWhen modeling patient speech alone, DistilBERT achieved the strongest performance among evaluated BERT-based models ( F 1 =69.39; AUC-ROC=69.36). For clinical notes, BioClinicalBERT yielded the best performance among evaluated language models ( F 1 =64.29; AUC-ROC=69.17). Among traditional classifiers, a linear SVM performed well using patient speech features ( F 1 =75.0; AUC-ROC=75.94). Models using structured EHR/OASIS variables achieved their best performance with logistic regression ( F 1 =75.56; AUC-ROC=79.70).\n\nIncorporating nurse speech and interactional measures ( Table 1 , component 4) resulted in improved discrimination (SVM  F 1 =85.0; AUC-ROC=86.47), suggesting that patient-nurse interaction captures complementary information beyond patient speech alone.\n\nIn multimodal analyses integrating speech features, interactional measures, and structured EHR/OASIS variables, the SVM achieved the highest overall performance ( F 1 =88.89; AUC-ROC=90.23). Examination of model contributions suggested that reduced lexical diversity, longer patient pauses, increased nurse dominance in conversation, selected psycholinguistic markers, and specific EHR variables (eg, non–insulin-dependent diabetes, pressure ulcers, and living alone) contributed to discrimination.\n\nOverall, these results support the feasibility of extracting and integrating multimodal signals for MCI-ED screening in HHC. Final model evaluation, subgroup performance assessment, and fairness analyses will be conducted using the prespecified validation procedures after completion of recruitment and the finalized analytic dataset.\n\nThis study protocol describes a multimodal screening approach for identifying MCI-ED in HHC using routinely generated data streams. The central hypothesis is that spontaneous patient-nurse conversations, combined with structured HHC assessment data (the federally mandated OASIS instrument) and information extracted from free-text clinical documentation, can provide complementary signals for earlier identification of cognitive impairment than any single data stream alone.\n\nThe implementation results reported in this manuscript demonstrate the feasibility of an end-to-end workflow in HHC, including audio capture during routine visits, automated transcription and speaker labeling, extraction of acoustic, linguistic, emotional, and interactional features, and linkage to clinical notes and OASIS variables. Exploratory analyses in the analyzed cohort suggest that incorporating nurse speech and interactional features can improve discrimination beyond patient speech alone, consistent with the premise that conversation structure (eg, timing, pauses, turn-taking balance, and interactivity) contains clinically relevant information. These findings should be interpreted as feasibility and proof-of-concept evidence, rather than definitive estimates of model performance.\n\nA substantial body of prior work has demonstrated that speech markers can differentiate individuals with Alzheimer disease from cognitively unimpaired controls, often using structured or semistructured tasks (eg, picture description, verbal fluency, and reading) collected in controlled environments [ 13 ]. While these approaches have been valuable for benchmarking and understanding underlying patterns, they may be less sensitive to the subtle and heterogeneous manifestations of MCI-ED and may not reflect communication behaviors during real-world clinical encounters.\n\nThis study extends this literature in 3 important ways. First, it shifts the speech signal from standardized tasks to naturally occurring clinical interactions in HHC, where pragmatic, temporal, and turn-taking patterns can be observed at scale. Second, it models not only patient speech characteristics but also interactional dynamics (including nurse speech), which may reflect clinician adaptation to support patients and/or patient difficulty maintaining conversational flow. Third, it advances a multimodal framework by integrating conversational speech features with (1) structured assessment variables from OASIS and (2) MCI-ED–related information embedded in free-text documentation or spoken conversation but not consistently represented in structured EHR fields. Together, these extensions aim to improve the practical relevance of screening in HHC settings where comprehensive cognitive evaluations may be limited.\n\nRecent advances in minimally invasive approaches to Alzheimer disease detection, including blood-based biomarker testing in symptomatic individuals, reflect increasing clinical emphasis on earlier identification. However, biomarker confirmation alone does not characterize how cognitive decline affects communication during routine care. Changes in speech and language, such as reduced fluency, disrupted discourse organization, and altered vocal control, often emerge early and reflect functional impairment that is not captured by biological measures. The screening approach described here targets this complementary dimension by modeling communication behaviors observed in routine patient-nurse encounters, providing ecologically valid indicators of cognitive change. Integrating speech-based indicators with other clinical information, including biomarker evidence when available, may support a more comprehensive assessment of cognitive decline and its impact on real-world functioning.\n\nSeveral limitations should be considered when interpreting these results. First, findings are based on data from a single HHC organization and a modest sample, which may limit generalizability and yield performance estimates that are sensitive to sampling variability. Second, audio quality, background noise, and automated transcription/speaker-labeling errors can affect the accuracy of extracted acoustic, linguistic, and interactional features. Third, interactional measures may reflect both patient cognitive-linguistic status and clinician communication style or workflow constraints, which can introduce confounding if not explicitly modeled. Fourth, interactional measures may reflect both patient cognitive-linguistic status and clinician communication style or workflow constraints, which can introduce confounding if not explicitly modeled. Fourth, the protocol focuses on English-speaking participants with sufficient hearing/vision to complete cognitive testing; results may not generalize to other language groups or to patients with sensory limitations, who are common in HHC. Finally, cross-sectional classification does not establish whether speech and interaction markers predict future cognitive trajectories, underscoring the need for longitudinal evaluation.\n\nSeveral avenues for future research could strengthen both the scientific rigor and clinical use of this approach. First, prospective longitudinal studies are needed to move beyond cross-sectional classification and evaluate whether speech and interactional markers can predict cognitive decline trajectories or functional deterioration over time. Such studies would clarify whether these features capture progressive change in addition to baseline differences. Second, external validation across diverse HHC agencies, geographic regions, and care delivery models will be essential to assess model transportability and identify when recalibration is necessary. Third, more granular analysis of conversational dynamics could distinguish clinically meaningful interaction patterns—such as repair sequences, prompting behaviors, and topic maintenance difficulties—from structural features that primarily reflect workflow or documentation practices. Fourth, incorporating clinician feedback through human-in-the-loop development cycles can help identify model failure modes, enhance interpretability of predictions, and establish safe deployment thresholds informed by real-world use cases. Finally, pragmatic clinical use trials are needed to determine whether integrating speech-based screening into HHC workflows improves downstream outcomes, including timeliness of formal cognitive evaluation, care plan modifications, and patient safety. Collectively, these efforts would bridge the gap between technical performance and meaningful improvements in care delivery for older adults at risk of cognitive decline.\n\nFindings will be disseminated through peer-reviewed publications and presentations to clinical and informatics audiences. To support reproducibility while protecting privacy, the study team plans to share (1) detailed feature definitions and extraction procedures, (2) deidentified analytic code and configuration files where permissible, and (3) the annotation schema and evaluation framework for text extraction.",
    "mesh_query": "mass screening[MeSH Terms]",
    "source_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12910275/",
    "data_crawling": "2026-02-19T18:22:09.995433"
  },
  {
    "pmc_id": "12910004",
    "title": "Determinants of cervical cancer screening among women living with HIV in Lesotho using nationally representative 2023/24 DHS data",
    "abstract": "Women living with HIV are more prone to develop cervical cancer since they have a compromised immune system; hence, they need to be screened continuously in an attempt to identify and prevent it. Despite Lesotho’s high HIV prevalence (25.6%), cervical cancer screening coverage and its determinants among women living with HIV remain insufficiently characterized. This study aimed to quantify the rate and determinants of cervical cancer screening among women living with HIV using the 2023/24 Lesotho DHS data. Cross-sectional analysis was performed using the Lesotho DHS Individual Women’s Recode file. A weighted sample of 611 HIV-positive women aged 25 years and older participated in the study, as this age group is eligible for cervical cancer screening. Individual and community-level factors were determined using multilevel mixed-effects logistic regression. The level of significance was determined by the 95% confidence interval and a  p  value less than 0.05 for associations. The total prevalence of cervical cancer screening among women living with HIV was 85.4%. Women aged 40–44 years (adjusted odds ratio [AOR] 4.14; 95% confidence interval [CI] 1.53–11.18) and those who had a clinical breast exam (AOR 5.53; 95% CI 2.54–12.05) were more likely to undergo cervical cancer screening, whereas low parity (AOR 0.19; 95% CI 0.05–0.78) and rural residence (AOR 0.50; 95% CI 0.25–0.99) were associated with lower odds of screening. Adoption of cervical cancer screening among women living with HIV in Lesotho is high, with most screened women receiving normal results. Screening uptake varied by demographics, being higher among older women and those who had breast examinations, while lower among women with low parity and rural residents. Integration of breast and cervical cancer screening, rural outreach targeting, and health education for low-parity women can increase coverage and equity.",
    "text": "Cervical cancer is still one of the most common causes of morbidity and mortality in women worldwide 1 , 2 . According to GLOBOCAN, in 2022 alone, there were an expected 662,044 new cases and 348,709 fatalities globally; disproportionately, around 90% of the incidence happened in low- and middle-income countries (LMICs) 3 , 4 . Women living with HIV (WLHIV) are particularly at high risk, experiencing an estimated sixfold risk of acquiring cervical cancer 5  because HIV infection greatly weakens the immune response; thus, it becomes harder for the body to resist human papillomavirus (HPV) infections that lead to cervical cancer 5 , 6 . Hence, WLHIV should regularly and timely be screened for cervical cancer to allow for early diagnosis and prevent the disease from progressing. Despite the availability of effective screening technologies, cervical cancer screening coverage in Africa ranges from 13 to 19% 7 , 8 .\n\nLesotho, a country with one of the highest HIV prevalence rates globally, estimated at 25.6% among adults 9 , and cervical cancer represents the most common cancer among women in the country. National cervical cancer screening in Lesotho mainly uses visual inspection with acetic acid (VIA), supplemented by cytology and HPV testing 10 . Despite the availability of national cervical cancer screening guidelines, evidence showed that screening coverage in Lesotho remains low, with only 38.9% of women aged 15–49 years having ever been screened for cervical cancer 11 . This coverage is far below the World Health Organization (WHO) target of 70% screening coverage, set as part of the global strategy for cervical cancer elimination 12 . Although national policies in Lesotho recommend initiating cervical cancer screening at age 25 years for women living with HIV 13 , progress toward achieving high screening coverage has been constrained by a lack of integrated services and limited system resources. Beyond structural constraints, sociocultural factors—including gender-related barriers, concerns about privacy, fear of a cancer diagnosis, and stigma associated with screening—continue to hinder utilization of cervical cancer screening services 8 , 14 , 15 .\n\nWhile several studies have examined cervical cancer screening in the general female population in Lesotho, there is limited nationally representative evidence focusing specifically on women living with HIV and accounting for both individual- and community-level determinants of screening utilization. Thus, this study addresses an important issue by using nationally representative and anonymized data from the 2023/24 Lesotho Demographic and Health Survey to assess cervical cancer screening prevalence and determinants among women living with HIV in Lesotho, providing timely evidence relevant to current HIV and cancer prevention strategies.\n\nThis study is a secondary analysis of data from the Lesotho Demographic and Health Survey (DHS), a nationally representative household study conducted in all districts of Lesotho. The DHS data were collected from 27 November 2023 to 29 February 2024 through a cross-sectional survey and were analyzed in 2025, as it was the most recent dataset available at the time of the study.\n\nLesotho is a small, landlocked country in Southern Africa, covering approximately 30,355 km 2 , the only nation globally where the entire territory exceeds 1,000 m above sea level. With a population of 2.3 million, the population is relatively young, with a median age in the early twenties, and around one‑third resides in urban areas. The country faces socioeconomic challenges and significant health issues, particularly a high HIV prevalence.\n\nLesotho DHS employed a stratified two-stage cluster sampling design. At the first stage, the country was stratified by district and residence, yielding 29 sampling strata. Within each stratum, enumeration areas (EAs) were selected by probability proportional to size. At stage two, households were selected within each selected EA using systematic random sampling. Sampling weights (v005/1,000,000) were applied to account for oversampling and unequal selection probabilities. Weighted frequencies and percentages were calculated by multiplying each participant’s data by their sampling weight, ensuring that the estimates accurately reflect the national population and provide unbiased, population-representative results.\n\nThis study analyzed the Individual Recode (IR) dataset, which includes women aged 15–49 years. Of the 6413 households initially screened, 6,085 women reported having ever been tested for HIV. Women who had not been tested for HIV (n = 328) were excluded. Among those tested, 6,043 received their HIV test results, and 1,285 were identified as HIV-positive.\n\nWomen were included in the analysis if they were aged 25–49 years, HIV-positive, and had complete information on cervical cancer screening. Women aged under 25 years (n = 121) were excluded, as they were not eligible for cervical cancer screening according to Lesotho’s national guidelines, which recommend screening for women living with HIV starting at age 25 or upon becoming sexually active 16 . Participants with missing data on the outcome variable or key explanatory variables were also excluded.\n\nAfter applying these criteria, 1,164 women living with HIV were retained, of whom 606 reported having undergone cervical cancer screening. These 606 women constitute the study’s primary sample of interest. Following the application of DHS sampling weights, the final weighted sample size was 611.17 women (Fig.  1 ). Fig. 1 Diagrammatic representation of sample selection in the study.\n\nWomen aged 25–49 years living with HIV who participated in the 2023/24 Lesotho Demographic and Health Survey.\n\nThe outcome variable was self-reported cervical cancer screening uptake, coded as a binary variable, with “1” indicating women who reported ever having undergone cervical cancer screening and “0” indicating those who reported never being screened.\n\nIndependent variables included individual-level factors such as age, marital status, education level, current employment, wealth index, health insurance, awareness of antiretroviral therapy (ART), current use of ART, history of cancer, receipt of cancer treatment, presence of chronic diseases, receipt of treatment for chronic diseases, history of breast cancer examination, age at first sexual intercourse, number of children, contraceptive use, sexual partnerships, awareness of sexually transmitted infections (STIs), self-reported STI, history of genital sore, and genital discharge. Community-level factors such as place of residence, distance to health facility, mobile telephone use, and media usage were also analyzed.\n\nAs presented in Table  1 , independent variables with binary responses were analyzed as originally coded in the DHS dataset, whereas selected variables that required grouping or transformation for analysis were recoded into meaningful categories for analysis. These included marital status, educational level, wealth index, age at first sexual intercourse, number of children, contraceptive methods, sexual partnerships, and media usage. Table 1 Operationalization and categorization of non-binary independent variables. Variables Categorization/operationalization Marital status Categorized as married (married and living with a partner) and unmarried (never in union, widowed, divorced, and separated) Educational level Categorized as low (no/primary) and high (secondary or higher) education Wealth index Categorized as poor (poorest/poorer), middle, and rich (richer/richest) Age at first sex Categorized as early (< 15 years), typical (15–19 years), and later (≥ 20 years) Number of children Categorized as nulliparous (0), low parity (1–2), and high parity (≥ 3) Contraceptive methods Categorized as not using hormonal methods (pill, injections, implants/Norplant, emergency contraception, lactational amenorrhea) and non-hormonal or barrier methods (intrauterine device, male and female condoms, female sterilization, periodic abstinence, withdrawal, other traditional, and other modern methods) Sexual partnerships Categorized as no extra-marital sexual partnerships (0–1 partner) and extra-marital sexual partnerships (≥ 2 partners) Media usage Categorized as low (< 50% of women exposed) and high (≥ 50% of women exposed). Although the DHS collects individual-level data on radio, television, and newspaper use (yes/no), this information was aggregated at the cluster level to create a community-level variable, based on the proportion of women exposed to at least one type of mass media\n\nData cleaning, recoding, and analysis were performed with STATA version 14.0. Descriptive statistics were utilized to describe the sociodemographic, behavioral, and health-related factors of the study population. A two-level mixed-effects logistic regression model was fitted, with individual-level variables treated as fixed effects and cluster-level random intercepts included to account for unobserved community-level heterogeneity. Sampling weights (v005/1,000,000) were applied so that frequencies, percentages, and regression estimates are representative of the national population.\n\nThe analysis involved building several models (Model 0, Model I, Model II, and Model III): the null model to assess the variability in cervical cancer screening, a Model I with only individual-level variables, a Model II with only community-level variables, and a Model III that included both levels of variables. Adjusted odds ratios (AORs) with 95% confidence intervals were calculated to identify significant predictors of cervical cancer screening. Measures of random effects, including the intraclass correlation coefficient (ICC), median odds ratio (MOR), and proportional change in variance (PCV), were reported. Model fit was assessed using log-likelihood ratio tests and deviance statistics.\n\nAccording to Table  2 , participants were predominantly aged 40–44 years (25.79%), and more than half of the respondents were married (58.82%). Regarding education, the largest proportion of participants had completed secondary education (47.23%). The majority was employed (52.36%) and belonged to the rich wealth index category (49.29%). Table 2 Socio-demographic, health, and behavioral characteristics of respondents in the study. Variable Categorized Unweighted frequency Weighted frequency Age 25–29 75 (12.38%) 64.52 (10.56%) 30–34 108 (17.82%) 111.12 (18.18%) 35–39 136 (22.44%) 135.58 (22.18%) 40–44 154 (25.41%) 157.65 (25.79%) 45–49 133 (21.95%) 142.28 (23.28%) Marital status Married 330 (54.46%) 359.48 (58.82%) Unmarried 276 (45.54%) 251.68 (41.18%) Educational level No education 4 (0.66%) 4.48 (0.73%) Primary 284 (46.86%) 257.01 (42.05%) Secondary 264 (43.56%) 288.64 (47.23%) Higher 54 (8.91%) 61.04 (9.99%) Current employment No 318 (52.48%) 291.15 (47.64%) Yes 288 (47.52%) 320.02 (52.36%) Wealth index Poor 261 (43.00%) 183.05 (29.95%) Middle 124 (20.43%) 126.87 (20.76%) Rich 221 (36.57%) 301.25 (49.29%) Health insurance No 590 (97.36%) 597.55 (97.77%) Yes 16 (2.64%) 13.62 (2.23%) Place of residence Urban 241 (39.77%) 297.42 (48.66%) Rural 365 (60.23%) 313.74 (51.34%) Distance to a health facility Big problem 204 (33.66%) 166.24 (27.20%) Not a big problem 402 (66.34%) 444.92 (72.80%) Heard of ARVs to treat HIV? No 4 (0.66%) 2.38 (0.39%) Yes 602 (99.34%) 608.78 (99.61%) Currently taking ARVs No 6 (0.99%) 8.19 (1.34%) Yes 600 (99.01%) 602.97 (98.66%) Have cancer or a tumor No 595 (98.18%) 600.65 (98.28%) Yes 11 (1.82%) 10.52 (1.72%) Receive any treatment for cancer/tumor? No 6 (54.55%) 6.04 (57.39%) Yes 5 (45.45%) 4.48 (42.61%) Other chronic diseases No 507 (83.66%) 518.61 (84.86%) Yes 99 (16.34%) 92.55 (15.14%) Receive treatment for chronic diseases No 13 (13.13%) 18.48 (19.97%) Yes 86 (86.87) 74.07 (80.03%) Breasts examined for cancer by health care provider No 406 (67.00%) 386.66 (63.27%) Yes 200 (33.00%) 224.50 (36.73%) Age at first sex Early initiation 54 (8.91%) 47.27 (7.74%) Typical initiation 440 (72.61%) 431.14 (70.54%) Later initiation 112 (18.48%) 132.75 (21.72%) Number of children Nulliparous 40 (6.60%) 47.81 (7.82%) Low parity 271 (44.72%) 290.89 (47.60%) High parity 295 (48.68%) 272.46 (44.58%) Contraceptive methods Not using 212 (34.98%) 213.59 (34.95%) Hormonal 248 (40.92%) 222.98 (36.48%) Non-hormonal 146 (24.09%) 174.59 (28.57%) Sexual partnerships No extramarital sexual partnerships 316 (52.15%) 346.52 (56.70%) Extramarital sexual partnerships 290 (47.85%) 264.64 (43.30%) Used a method of sexual intercourse No 132 (25.10%) 127.71 (24.08%) Yes 394 (74.90%) 402.69 (75.92%) Ever heard of an STI? No 268 (44.22%) 234.76 (38.41%) Yes 338 (55.78%) 376.41 (61.59%) Had an STI in the last 12 months No 575 (94.88%) 571.07 (93.44%) Yes 31 (5.12%) 40.09 (6.56%) Had a genital sore/ulcer in the last 12 months No 552 (91.09%) 557.71 (90.93%) Yes 54 (8.91%) 55.45 (9.07%) Had a genital discharge in the last 12 months No 491 (81.02%) 489.66 (80.12%) Yes 115 (18.98%) 121.50 (19.88%) Mobile telephone use No 42 (6.93%) 38.29 (6.27%) Yes 564 (93.07%) 572.87 (93.73) Media usage (access to radio & TV) Low 462 (76.24%) 460.14 (75.29%) High 144 (23.76%) 151.03 (24.71%)\n\nSocio-demographic, health, and behavioral characteristics of respondents in the study.\n\nMost participants were rural residents (51.34%), with 97.77% did not have health insurance. A significant proportion of participants reported that distance to a health facility was not a big problem (72.80%) and that they had access to radio and TV (24.71%). Additionally, almost all respondents had heard of antiretroviral (ARV) treatment for HIV (99.61%) and were taking ARVs (98.66%). The vast majority did not have cancer or a tumor (98.28%) and did not have other chronic diseases (84.86%).\n\nRegarding reproductive health, 70.54% of participants had initiated sexual activity between the ages of 15–19 years. Majorities had low parity (1–2 children) (47.60%) and predominantly used hormonal contraceptives (36.48%). In terms of sexual behavior, most participants reported having had only one sexual partner or no extramarital sexual partner in the last 12 months (56.70%) and indicated that they had used a method during sexual intercourse (75.92%).\n\nMore than half of the participants were aware of sexually transmitted infections (STIs), with 61.59% being aware of STIs. STI incidence in the last 12 months was reported as 6.56%, 9.07% reported genital sores or ulcers, and 19.88% reported genital discharge.\n\nThe overall cervical cancer screening among women living with HIV was 85.4% [95% CI 82.35–87.98%] (Fig.  2 ). Among those screened, the majority had normal/negative results (95.63%). The prevalence of abnormal/positive results was 0.83%, 1.73% were classified as suspect cancer, 0.10% were unclear/inconclusive, 1.40% did not receive results, and 0.31% did not know their result (Fig.  3 ). Fig. 2 Cervical cancer screening among women living with HIV in Lesotho, 2023/24. Fig. 3 Last cervical cancer screening result among women living with HIV in Lesotho, 2023/24.\n\nLast cervical cancer screening result among women living with HIV in Lesotho, 2023/24.\n\nAs shown in Table  3 , the null model showed substantial community-level clustering in cervical cancer screening (variance = 2.38; ICC = 0.42; MOR 4.33), indicating that 42% of the total variance was attributable to differences between communities. Adding individual-level variables (Model I) and community-level variables (Model II) reduced the unexplained variance (PCV = 79.4% and 97.1%, respectively), while the full model (Model III) retained moderate residual variance (0.37; ICC = 0.10; MOR 1.78) and explained 84.4% of the between-community variability. Model III also had the lowest deviance (437.06) and highest log-likelihood (− 218.53), indicating the best fit. Therefore, Model III, which includes both individual- and community-level factors, was selected as the optimal model for identifying predictors of cervical cancer screening among women living with HIV. Table 3 Multivariable mixed-effect logistic regression analysis results of both individual-level and community-level factors. Variable Categorized Null model Model I AOR (95% CI) Model II AOR (95% CI) Model III AOR (95% CI) Age 25–29 1.00 – 1.00 30–34 1.91 [0.73–4.95] – 2.01 [0.78–5.20] 35–39 1.75 [0.72–4.25] – 1.79 [0.74–4.34] 40–44 4.06 [1.51–10.90] – 4.14 [1.53–11.18]* 45–49 2.39 [0.90–6.36] 2.43 [0.91–6.51] Marital status Married 1.00 – 1.00 Unmarried 0.79 [0.42–1.47] 0.83 [0.45–1.56] Educational level Low-level education 1.00 – 1.00 High-level education 1.28 [0.69–2.37] – 1.33 [0.72–2.46] Currently working No 1.00 1.00 Yes 1.01 [0.58–1.74] 0.95 [0.55–1.64] Wealth index Poor 1.00 – 1.00 Middle 1.69 [0.77–3.75] – 1.35 [0.59–3.05] Rich 1.04 [0.54–2.02] 0.60 [0.27–1.32] Health insurance No 1.00 – 1.00 Yes 0.83 [0.12–5.84] 0.74 [0.10–5.19] Other chronic diseases No 1.00 – 1.00 Yes 0.86 [0.42–1.79] 0.87 [0.42–1.80] Breasts examined for cancer by health care provider No 1.00 – 1.00 Yes 5.92 [2.72–12.91] 5.53 [2.54–12.05]* Age at initial sex Early initiation 1.00 – 1.00 Typical initiation 1.26 [0.47–3.39] – 1.28 [0.47–3.46] Later initiation 1.35 [0.44–4.20] 1.46 [0.47–4.53] Number of children No children 1.00 – 1.00 Low parity 0.19 [0.05–0.80] – 0.19 [0.05–0.78]* High parity 0.24 [0.06–1.05] 0.24 [0.06–1.05] Contraceptive methods Not using 1.00 1.00 Hormonal 1.56 [0.80–3.03] – 1.51 [0.77–2.93] Non-hormonal 1.38 [0.69–2.74] – 1.27 [0.64–2.53] Sexual partnerships No extramarital sexual partnerships 1.00 – 1.00 Extramarital sexual partnerships 1.30 [0.71–2.37] – 1.26 [0.69–2.31] Ever heard of an STI No 1.00 – 1.00 Yes 1.66 [0.91–3.04] 1.66 [0.91–3.03] Had an STI in the last 12 months No 1.00 – 1.00 Yes 2.53 [0.49–13.13] 2.46 [0.48–12.64] Had a genital sore/ulcer in the last 12 months No 1.00 – 1.00 Yes 1.23 [0.45–3.39] 1.22 [0.44–3.38] Had genital discharge in the last 12 months No 1.00 – 1.00 Yes 0.77 [0.38–1.58] 0.74 [0.36–1.51] Place of residence Urban – 1.00 1.00 Rural 0.54 [0.32–0.91] 0.50 [0.25–0.99]* Distance to a health facility Big problem – 1.00 1.00 Not a big problem 1.50 [0.89–2.54] 1.42 [0.77–2.65] Mobile telephone use No – 1.00 1.00 Yes 1.53 [0.65–3.56] 1.24 [0.45–3.41] Media usage Low access – 1.00 1.00 High access 1.13 [0.65–1.98] 1.15 [0.59–2.27] Random effect and model comparisons  Random effect Variance 2.38 0.49 0.07 0.37 ICC 0.42 0.13 0.02 0.10 MOR 4.33 1.95 1.28 1.78 PCV Reff 79.4% 97.1% 84.4%  Model comparison Log-likelihood ratio − 253.95 − 221.92 − 247.23 − 218.53 Deviance 507.90 443.84 494.46 437.06 “*” indicates variables with  p  < 0.05, considered statistically significant.\n\nMultivariable mixed-effect logistic regression analysis results of both individual-level and community-level factors.\n\nIn the multilevel logistic regression analysis, women aged 40–44 years were significantly more likely to undergo cervical cancer screening (AOR 4.14; 95% CI [1.53–11.18]) compared with women aged 25–29 years. Women who had previously undergone a breast examination by a medical professional had higher odds of cervical cancer screening (AOR 5.53; 95% CI [2.54–12.05]) than those who had not. In contrast, women with low parity were less likely to undergo cervical cancer screening (AOR 0.19; 95% CI [0.05–0.78]) compared with women with no children. Rural residence was associated with a reduced likelihood of cervical cancer screening (AOR 0.50; 95% CI [0.25–0.99]) compared with urban residence.\n\nThis study aimed to assess the prevalence and determinants of cervical cancer screening among Lesotho’s HIV-positive women using DHS 2023/24 data. Cervical cancer screening coverage was high, with more than four-fifths of participants reported to have been screened. Older age (40–44 years) and prior breast examination by a medical professional were positively associated with screening uptake, whereas low parity and rural residence were associated with lower odds of screening.\n\nThe findings of this research revealed that cervical cancer screening among women living with HIV, was 85.4% (95% CI 82.35–87.98%), which is in line with findings from the USA (83%) 17  and England (85.7%) 18 . This rate is significantly higher than studies from other regions, including Spain (50.6%) 19 , India (41.15%) 20 , Uganda (44%) 21 , and Ethiopia (27.8%) 22 , but lower than studies in Italy (91%) 23 . The relatively high screening coverage in Lesotho may reflect the integration of cervical cancer screening into HIV care services and the support of donor-funded programs that promote routine screening among women living with HIV. Additionally, ongoing national HPV vaccination efforts may have increased awareness of cervical cancer and preventive behaviors, indirectly encouraging higher screening uptake. The observed high prevalence may also be influenced by self-reported data and is specific to the HIV-positive study population; studies in the general population, including both women living with HIV and those without HIV, generally report lower screening coverage.\n\nIn this study, age emerged as a significant determinant; women aged 40–44 were more than four times more likely to undergo screening than women aged 25–29 years. This finding is consistent with studies from Ethiopia 24 , Kenya 25 , and a global analysis comparing LMICs and high-income countries 26 . The possible explanation is that older women tend to have had prior exposure to health care, more exposure to health education campaigns, or family or individual illness experiences 27 , 28 .\n\nOne of the significant findings was that women who had undergone a breast examination by a healthcare provider were over five times more likely to have been screened for cervical cancer than women who had no prior breast examination. This is in line with findings that using other preventive care services raises the uptake of cervical cancer screening, as reported in South Africa 29  and Malawi 30  studies. This association indicates that women who use other preventive health services, such as breast examinations, tend to undergo cervical cancer screening. This observation emphasizes a powerful implication for integrated care policy: exposure to one preventive service may encourage uptake of others, reinforcing the benefit of co-locating or linking breast and cervical cancer services 31 .\n\nAnother finding of this study was that parity influenced cervical cancer screening uptake; women with low parity (1–2 children) were less likely to undergo cervical cancer screening compared with women with no children. The plausible explanation is that women without children may be more likely to be engaged in general health checkups, family planning, or HIV care services where screening opportunities arise, whereas women with one or two children might have completed initial reproductive health visits and are less consistently engaged in follow-up screening. Similar patterns have been observed in studies conducted in Jamaica 32 , and Ethiopia 33 . In addition, low-parity women may perceive themselves at lower risk of reproductive health issues compared with women without children, or may face competing caregiving responsibilities that reduce opportunities for preventive care 27 .\n\nFurthermore, the study identified rural residence as a significant barrier to cervical cancer screening compared to women living in urban areas, with women living in rural areas being less likely to be screened despite the presence of Lesotho’s mobile outreach programs. This finding is consistent with other studies in Uganda 34 , Nigeria 35 , and Kenya 25  where rural women face numerous barriers to accessing healthcare. This may reflect challenges such as limited frequency of mobile clinic visits, low community awareness, and logistical barriers in accessing screening services. To strengthen rural coverage, lessons can be drawn from neighboring countries like Rwanda’s HPV-DNA testing model and Malawi’s integration of VIA screening into routine health services, which have demonstrated increased uptake in rural populations 36 , 37 .\n\nOverall, the findings highlight the need for targeted strategies to promote equitable access to cervical cancer screening in Lesotho. Integrating screening into routine HIV care and other preventive services could enhance uptake by utilizing women’s existing contact with the health system. Expanding services to rural and underserved areas through mobile outreach, community mobilization, and strengthened clinic–community linkages remains crucial. Tailored health education approaches, including those directed at low-parity women, may further address gaps in utilization. Promoting multiple preventive health behaviors within community health platforms may also contribute to improved screening coverage. Furthermore, future studies should consider district-, sub-district-, or community-level analyses to identify geographic disparities and guide targeted interventions.\n\nThis study highlights several strengths in its approach to investigating cervical cancer screening among women living with HIV. It employs a recent nationally representative dataset and applies multilevel mixed-effects modeling analysis, enabling a comprehensive assessment of both individual and community-level factors influencing screening behaviors. By concentrating on a high-risk, disadvantaged population, the study contributes valuable insights that can inform global strategies for cervical cancer control. However, the study also presents notable limitations, particularly its cross-sectional design, which impedes the ability to draw causal conclusions. Furthermore, essential psychosocial factors such as knowledge, stigma, and screening motivation were not considered, potentially impacting screening behaviors. There is also a concern regarding the reliability of self-reported screening status due to possible social desirability bias, and pertinent data on the frequency and recency of screenings were not collected.\n\nCervical cancer screening uptake among women living with HIV in Lesotho appears encouraging; however, important disparities persist. Screening behavior is influenced by demographic, reproductive, healthcare access, and residence-related factors, indicating that not all women benefit equally from existing screening efforts. Strengthening integrated, equitable, and context-specific screening strategies within HIV care services may contribute to further improving coverage and reducing cervical cancer–related morbidity and mortality in this high-risk population.",
    "mesh_query": "mass screening[MeSH Terms]",
    "source_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12910004/",
    "data_crawling": "2026-02-19T18:22:11.091897"
  },
  {
    "pmc_id": "12911645",
    "title": "Association between building density and screening myopia in children and adolescents: a 5-year longitudinal study from China",
    "abstract": "Abstract Background This study explored the relationship between school-level built environment factors (building density and height) and myopia prevalence. Methods In all 16 districts of Shanghai, students from public schools and kindergartens underwent visual acuity testing and non-cycloplegic refraction between 2019 and 2023. Building density and height within 500-metre and 1,000-metre buffers around the schools were obtained using GIS (Geographic Information System)-based spatial and network analyses from the Tianditu database. Demographic and behavioural covariates, such as age, gender, outdoor activity time and near-work time, were collected through questionnaires. Logistic mixed-effect models were used to analyse the association between the building environment and myopia, adjusting for relevant demographic and behavioural factors. Results Data from 8 647 375 participants were analysed, with 1 116 851 followed consistently. The myopia rate among students was significantly higher during 2020–2021 compared with 2019, 2022 and 2023 (p<0.05). Myopia was more prevalent in students aged 6 to 12, with a higher average age for myopic individuals (8.13 vs 6.64 years). The average building density was higher at schools of myopic students (15.70%) versus non-myopic students (14.90%) (p<0.001). Building density near schools was negatively correlated with spherical equivalent (p<0.05), while no significant association was found with building height. The risk of myopia increased with higher building density (OR: 1.143–1.202, p<0.05), particularly in boys over 12 years old. A stronger association was observed in inner suburban areas (p<0.001). Conclusions Prolonged exposure to high building density may accelerate myopia progression. Urban planning should consider vision health, with regulations on building density near schools to reduce myopia risk.",
    "text": "Myopia prevalence has been rising globally, particularly in urban regions. While genetic factors and environmental influences like prolonged near-work and lack of outdoor activity are well-established risk factors, the relationship between built environment factors, such as building density, and myopia remains underexplored. Previous studies indicate urban environments may contribute to higher myopia risk, but the specific impact of building density and height is not fully understood.\n\nThis study establishes a significant association between higher building density and myopia risk, particularly among boys aged 12 and older. Unlike previous research, it finds that the role of outdoor activity in mediating this relationship is minimal. Additionally, the study identifies a stronger effect in inner suburban areas, adding a regional perspective to the understanding of how the built environment influences myopia.\n\nThe findings underscore the importance of considering built environment factors, particularly building density, in urban planning. Future research should explore the underlying mechanisms behind this relationship. Policy implications suggest that regulating building density around schools could be a critical strategy to mitigate myopia risk in children and adolescents, guiding both urban planning and public health interventions.\n\nMyopia, a common refractive error primarily developing during childhood and early adulthood, occurs when images are focused in front of the retina, leading to blurred distance vision. 1  The global prevalence of myopia has risen alarmingly, from 28.3% in 2010 to 34.0% in 2020, with approximately 2.62 billion individuals affected worldwide. 2  This rising trend is particularly pronounced in East and Southeast Asia, where the prevalence among young adults can reach as high as 80%–90%, with a substantial portion suffering from high myopia (10%–20%). 2  Myopia negatively impacts the quality of life, causing difficulties in school performance and social interactions and is associated with an estimated productivity loss of $244 billion. 3  High myopia increases the risk of severe ocular pathologies such as retinal detachment and myopic maculopathy. In addition, under-corrected refractive errors have become the third leading cause of blindness globally as of 2020. 4\n\nBoth environmental and genetic factors influence myopia. 1 5  Children with myopic parents exhibit a heightened likelihood of developing myopia, underscoring the substantial role of genetic factors. 5  Additionally, environmental factors, such as prolonged close work and insufficient outdoor activities, are also crucial in increasing myopia risk. 6  Furthermore, lack of outdoor activities and natural light are key factors. Studies showed that spending more time outdoors can help reduce myopia. 7  Biological studies suggest that reduced exposure to high-intensity outdoor light can disrupt normal retinal dopamine signalling, which plays a protective role against excessive axial elongation—a key factor in myopia development. 8  Experimental evidence from animal models has shown that bright light exposure suppresses axial growth of the eyeball. 9  Epidemiological studies in humans also support the association that children exposed to less natural light due to dense urban environments or prolonged indoor activity show greater axial elongation and higher rates of myopia progression. 10  The global rise in myopia coincides with urbanisation, with higher prevalence observed in urban areas compared with rural regions. 6\n\nThe rapid pace of urbanisation and advancements in geographic information technology has raised awareness of how the built environment affects health. The built environment includes all structures built by humans, such as high-rise buildings, residential areas and commercial complexes. 11  It is now recognised as one of the critical drivers of non-communicable diseases. 11 12  As building density increases, natural light diminishes, and outdoor spaces for physical activity become limited, negatively impacting the visual health of adolescents. 13 14  Therefore, we hypothesise that building environment significantly influences myopia by limiting sunlight, outdoor activities and increasing exposure to pollutants. 14\n\nFew studies have specifically investigated the relationship between building density/height and myopia, the potential mediating role of outdoor activities or the interaction effects. To address this gap, this study analysed longitudinal primary care data from a large cohort of children and adolescents in Shanghai, China. The data were collected over several years through school-based screening programmes and linked to environmental exposures. The primary aim was to determine whether building density and height are independently associated with myopia and to investigate potential mediating factors and whether age or gender influences these associations.\n\nThis study used data from the Shanghai child and adolescent large-scale eye study project (SCALE), covering myopia screenings from 2019 to 2023 across public schools and kindergartens in 16 districts. The scientific rationale, study design and survey methods for this project have been reported previously. 15  The study population included students aged 3 to 20 years. The study population included students aged 3 to 20 years. Individuals with pre-existing eye conditions, or missing data on the building environment or eye health, were excluded. In total, 8 914 614 data records over the years were included in final analysis ( online supplemental figure S2 ).\n\nThe study protocol was approved by the Institutional Ethics Committee of Shanghai General Hospital and follows the tenets of the Declaration of Helsinki for experimentation on humans. Parents or guardians of all participants were contacted and informed of the study purpose and procedures, and informed consent was obtained.\n\nEach child enrolled in the study was required to undergo distance visual acuity (VA) and non-cycloplegic refraction by trained staff using standard procedures at school sites. Uncorrected VA was measured using a logarithmic E chart at 5 metres, with luminance of 80–320 cd/m 2 . VA was recorded in decimal notation and was transformed into five-grade notation for statistical calculation, which adhered to the National Standard of China (GB 11 533–1989). 16  It could be equivalently mapped to other notations, including Snellen decimal/fraction and logMAR ( online supplemental table S1 ). The procedure followed international criteria for recording VA. 17\n\nNon-cycloplegic auto-refraction was conducted using either the Topcon KR-8900 (Tokyo, Japan), Nidek AR-330A (Nagoya, Japan) or HUVITZ HRK-7000A (Gemjeong-dong, South Korea) auto-refractors. The consistency of the inter-instrument among the three auto-refractors was acceptable. 15  An average of three consecutive readings was recorded for each eye. If any two measurements varied by more than 0.50 dioptre, the readings were discarded, and the eye remeasured. Calibration of the instruments was performed at the start of each examination day. The municipal and district-level project groups organised field supervision and quality control across all study sites.\n\nThe primary outcome was prevalent myopia. We calculated the non-cycloplegic SE for each eye using the formula spherical power +0.5* cylindrical power. Myopia was defined as VA <5.0 (in five-grade notation) and non-cycloplegic SE <−0.5D of either eye. 18  Although cycloplegic eye drops were not used, we used well-established screening methods from prior studies to predict myopia, and the consistency of the screening methods and cycloplegic refraction has been previously explored. 19 20\n\nMyopia is often referred to as a ‘classroom disease’, with most students spending significant time at school. Therefore, we primarily analysed the impact of the built environment around schools on myopia. Based on prior literature and hypothesised understandings, the primary exposures of interest were school-level built environment characteristics, building density and building height. 1421 24  Environmental exposure metrics were developed through GIS-based spatial and network analyses of data within a 1000-metre or 500-metre radius of participants’ schools or kindergartens from multiple databases. The selection of such a distance area for measuring environmental exposures was based on previous reports, corresponding to 5–10 min of walking at a moderate pace. 22\n\nBuilding density and height are the key parameters for assessing land use and spatial breadth. Building density was defined as the ratio of the total footprint area of all buildings within a defined buffer zone (500 m or 1000 m radius around each school) to the total land area of the buffer zone. Mathematically, this was calculated as: Building Density (%) = (Σ Building Footprint Area within Buffer) / (Total Buffer Area)*100%. For our Shanghai study area, building footprint data were primarily obtained from the authoritative Tianditu database, with OpenStreetMap (2023 dataset) used for completeness verification. Building height data were derived from Tianditu’s 3D building models, representing the average height of all buildings within each buffer zone. All spatial analyses were performed using ArcGIS 10.3, employing the ‘Buffer’ tool to create analysis zones and the ‘Zonal Statistics’ function to aggregate building characteristics.\n\nThe study adjusted for demographic covariates (age and gender). Additionally, several schools covering about ten per cent of the total students every year were randomly selected and invited to complete a previously validated questionnaire. The questionnaire included daily time spent near-work and outdoor activities during school days and weekends. Weekly time was calculated by multiplying weekdays by five and weekends by two.\n\nBecause of the multilevel nature of the data, this study used linear mixed effect models to explore the relationship between school-level built environment and individual-level refractive outcome—screening myopia. Building density and height within 500/1000 metres were set as fixed effect predictors, with schools as random effects. The response variable was myopia rate. In model I, we included building density and height. Model II adjusted for average age and gender ratio. Model III was further adjusted for daily outdoor time and near-work time. Model IV included the survey year as an additional fixed effect. OR and 95% CIs were presented, and a forest plot was created to illustrate the relationship between myopia and building density. Then, stratified analyses by gender, age (<6, 6–12, >12 years) and geographical location were performed. A two-sided P value of <0.05 was considered significant. Subsequently, we analysed individuals followed up for over 3 years. We employed age, grade, school and gender at the onset of the follow-up as independent variables, and whether they developed myopia as the dependent variable, to construct a multifactor logistic regression model examining the influence of these variables on the onset of myopia. All spatial analyses were conducted with ArcGIS V.10.3, and statistical analysis was performed with R V.4.1.2.\n\nComplete data on myopia and built environment indicators were available for 8 914 614 participants from the SCALE project, conducted across multiple years (2019–2023). Among these, valid questionnaire data were available for 8 647 375 participants ( online supplemental figure S2 ).\n\nFrom 2019 to 2023, the majority of participants in the myopia screening were children aged 6–12 years, with primary and junior high school students accounting for the largest proportions ( table 1 ). Gender distribution remained consistent, with boys representing a similar proportion each year, though the differences were statistically significant. The prevalence of myopia showed an initial increase, peaking in 2021, followed by a gradual decline (p<0.05), which may be attributed to the increased visual load associated with widespread online learning during the COVID-19 pandemic. Although daily outdoor time and near-work duration did not show significant differences across the years, environmental factors such as building height within 0.5 km and 1 km of schools exhibited a gradual increase over time, while building density showed no significant changes. Meanwhile, refractive parameters, including spherical equivalent refraction (SER) and axial length, remained relatively stable with minimal fluctuations across the study period.\n\nALR, axial length of the right eye; SER, spherical equivalent refraction; VAR, visual acuity of the right eye.\n\nBased on the entire study population, we identified a fixed cohort of 1 116 851 participants who were consistently followed over 3 years. The results revealed that the average age of myopic individuals (8.13 years) was higher than that of non-myopic individuals (6.64 years), with myopia being more prevalent among those aged 6 to 12. Additionally, the residential building density for myopic individuals was significantly higher than that for non-myopic individuals (15.70 vs 14.90, p<0.001) ( online supplemental table S2 ).\n\nThe spatial distribution maps of adolescent myopia prevalence in Shanghai from 2019 to 2023 reveal distinct patterns across Shanghai, China ( figure 1 ). The southern districts, particularly Minhang and Pudong, consistently exhibited the highest myopia prevalence, exceeding 40% annually. In contrast, central and northern districts, such as Jing’an and Huangpu, displayed moderate to high prevalence rates, ranging from 30% to 40%, with a noticeable upward trend over the 5 years, indicating an increasingly severe public health concern. The 2023 building density map highlights that the highest density was concentrated in central districts, especially Jing’an, Huangpu and Xuhui districts, where the overlap between high building density and elevated myopia prevalence suggests a potential correlation between urban environmental factors and the rising myopia rates among adolescents.\n\nHigher building density within 0.5 km and 1 km of schools was consistently associated with more severe myopia, as indicated by a significant negative correlation with the SER (p<0.05) ( table 2 ,  online supplemental table S3 and S4 ). In contrast, no significant relationship was found between building height and SER. The risk of myopia significantly increased with higher building density (OR: 1.143–1.202 in models I-IV, p<0.05). Although the direct effect of building density on myopia was significant (OR=1.246, p<0.001), the mediation effect through outdoor time was minimal and not significant (p=0.510) ( table 3 ). This indicated that the influence of building density on myopia was primarily direct, with limited mediation by outdoor activities.\n\nModel Ⅱ adjusted for fixed effect of age, gender, grade, daily outdoor time, daily near work time, years and random effect of schools.\n\nAdjusted for fixed effect of age, gender, grade, daily outdoor time, daily near work time and random effect of schools.\n\nThe stratified analysis revealed significant age and gender differences in the association between building density and myopia risk ( figure 2  and  online supplemental figure S1 ). Among boys aged 6 years or younger, each unit increase in building density was associated with an 18.9% increase in myopia risk within 0.5 km (OR=1.189, p<0.001) and a 21.1% increase within 1 km (OR=1.211, p<0.001). In contrast, no significant association was observed for girls in the same age group (p>0.05). For children aged 6 to 12 years, building density significantly impacted both boys and girls, with boys showing slightly higher odds ratios (1.200 to 1.304) than girls (OR=1.173 to 1.272). Notably, among boys over 12 years old, the impact of building density was even more pronounced, with ORs of 1.276 (p=0.004) and 1.311 (p=0.003) within 0.5 km and 1 km, respectively. However, for girls over 12, no significant association was found (p>0.05). Overall, the results indicate that boys, particularly those older than 12, are more susceptible to the effects of building density on myopia risk, with the impact intensifying with age.\n\nThis study found that the association between building density and the risk of myopia varied across different regions ( online supplemental table S5 ). Building density in inner suburban areas had the most pronounced impact on adolescent myopia risk. Higher density within both the 1 km and 0.5 km radii significantly increased myopia incidence (p<0.001). In comparison, while central urban areas demonstrated some association with myopia risk, the effect was less substantial (OR=1.080, p=0.008).\n\nMyopia is a significant concern for individuals worldwide, yet the relationship between building density and myopia has yet to be explored. In this large, objectively measured study, we identified a significant association between higher building density and increased prevalence of myopia, particularly among boys aged 12 years and older. In contrast, building height had no significant effect. While building density had a substantial direct effect on myopia, outdoor time played a minor role. This suggested that the influence of building density on myopia is primarily direct, with outdoor activity playing a limited role in this process. Additionally, there were significant age and gender differences in the association between building density and myopia risk, particularly among boys aged 12 and older, where the impact of building density was most pronounced. As age increased, this influence on myopia risk gradually intensified for boys, while no significant association was observed in their female peers.\n\nThis study found a significant association between higher building density around schools and increased myopia prevalence among students. In contrast, average building height within the same buffer zones was not significantly associated with myopia risk. Our study identified a significant association between higher building density and the incidence of myopia among children and adolescents, which several potential mechanisms can explain. First, high-density areas often reduce green spaces and outdoor activity opportunities, which protect against myopia. 14 21 25  However, our findings showed that outdoor time had a limited mediating effect between building density and myopia. Second, high building density often obstructs natural light exposure, limiting the amount of outdoor light children in these areas receive, thereby reducing the protective effect of light on eye growth. 13 26  Additionally, areas with higher building density typically have increased levels of air pollution, which may contribute to myopia development by inducing ocular inflammation or directly irritating eye tissues. 27 28  Finally, high building density often obstructs long-distance views, which can discourage children from engaging in activities that promote distance vision, further exacerbating the risk of myopia. Children living in densely built areas may be more prone to prolonged screen time and near-work. 29 30  Thus, our findings support the hypothesis that building density negatively impacts myopia development through environmental and behavioural mechanisms. The mediation effect analysis of building density and outdoor time on myopia-related outcomes highlighted a strong association between urban building density and myopia among schoolchildren, suggesting that effective interventions may need to address factors beyond merely increasing outdoor activity.\n\nThe increased effects of building density in participants aged 7 to 12 years might be associated with rapid refractive development and hormonal changes characteristic of this age group. 31 32  The study indicated that boys, particularly those over 12 years old, were more sensitive to the impact of building density on myopia risk. However, previous studies suggested that girls were more affected by high building density, which contrasted with the findings of this study. 33  This discrepancy might have been due to differences in behaviour, as girls typically engage in more indoor activities during and after adolescence, while boys are more likely to participate in outdoor activities, potentially leading to differing sensitivities to environmental factors. 33 34  Therefore, further research is needed to better understand the influence of building density on myopia across different age groups and genders.\n\nThe consistency observed between outcome measures and building density within a 500-metre radius provides compelling evidence supporting a potential causal relationship. In contrast, no significant association was found between building height and myopia, suggesting that horizontal building density has a more substantial impact on myopia development than vertical height. Furthermore, the influence of building environments on myopia risk varied across different regions. This study demonstrated that, within both the 1 km and 0.5 km radii, higher building density in inner suburban areas significantly increased the risk of myopia in children and adolescents. The mixed nature of inner suburban areas, which lack both the extensive outdoor activity spaces of outer suburbs and the abundant recreational facilities of central urban areas, may explain the more pronounced effect of building density on myopia in these regions.\n\nOur study has several limitations. First, we could not assess the impact of environmental factors near participants’ homes, although the proximity between home and school environments in Shanghai might suggest similarities. Second, potential student mobility between schools could have led to misclassification of environmental exposure. Additionally, conducted in a single, densely populated city, the study may limit the generalisability of our findings. Finally, we lacked detailed information on some potential confounders, which could have introduced bias. Our study did not adjust for certain socioeconomic confounders, such as parental education, household income and home lighting quality, which are known to be associated with myopia risk. These unmeasured variables may influence children’s time spent on near work, screen use and access to outdoor environments, thereby potentially biasing the observed associations. Future studies incorporating detailed individual- or household-level socioeconomic data will be essential to better isolate the independent effect of building density on myopia development. In addition, we did not account for variations in school infrastructure, such as classroom lighting, screen exposure policies or the availability of outdoor activity facilities, which could differ across schools and regions and influence the observed associations. Neighbourhood-level socioeconomic status may also affect both the built environment and myopia risk by shaping access to resources, recreational spaces and parental awareness of vision health. Future research incorporating detailed individual and contextual socioeconomic data is essential to better isolate the independent effect of building density on myopia development. These factors may co-vary with building density and independently influence myopia development through behavioural and environmental pathways. Without controlling for these contextual variables, the observed associations between building density and myopia risk may partly reflect these unmeasured influences.\n\nHowever, this study also has significant strengths. The use of 5-year longitudinal data allowed us to observe the long-term effects of the built environment on myopia, which many studies cannot achieve. The unique scale and extensive geographical coverage of the SCALE project provided robust statistical power to detect effects and explore interactions. Our study is among the first to explore the relationship between building density and myopia, using objective measurements to minimise bias. Built environment factors may function as the distal cause of myopia, characterised by introducing weak and long-term effects that will influence a wide population and range of diseases. 35  We also examined the mediating role of outdoor time, offering valuable insights for future research. Finally, our findings stress the importance of including public health in urban planning to support eye health.\n\nThis study suggests that higher building density may have long-term adverse effects on the eye health of children and adolescents. The study underscores the importance of incorporating public health considerations into urban planning, especially in designing school and urban environments that support eye health. Future research should further explore these findings, focusing on controlling additional potential confounders to better understand the long-term impact of the built environment on myopia and to inform effective prevention strategies.",
    "mesh_query": "mass screening[MeSH Terms]",
    "source_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12911645/",
    "data_crawling": "2026-02-19T18:22:12.234704"
  },
  {
    "pmc_id": "12911812",
    "title": "Effectiveness of specialised nutritious food, local diet promotion and social and behaviour change communication in preventing stunting among children in Afghanistan: a mixed-method study protocol",
    "abstract": "Abstract Introduction Stunting is a global public health challenge, resulting from chronic undernutrition that begins in utero and continues through the first 2 years of life. In Afghanistan, the burden is severe, with nearly 45% of children under five being stunted. Maternal undernutrition, suboptimal breastfeeding, inadequate complementary feeding and other risks are the major contributors towards stunting. Despite ongoing nutrition interventions, evidence on integrated and context-specific approaches in Afghanistan is scarce. This study aims to assess the effectiveness of specialised nutritious food, promotion of local diets through local nutritious seasonal foods, appropriate maternal, infant and young child feeding practices and social and behaviour change communication (SBCC) to prevent stunting among children 6–24 months and to improve nutritional status and dietary diversity among pregnant and breastfeeding women (PBW). Methods and analysis A mixed-method observational study design will be employed, combining a prospective longitudinal cohort study, process evaluation and pre–post cross-sectional surveys. The intervention package will be implemented in two districts through health posts and community-based platforms. Lipid-based nutrient supplements for children aged 6–24 months and wheat–soya blend for PBW will be distributed monthly, along with SBCC sessions. Statistical analysis will use descriptive analysis, time-to-event analysis, generalised linear mixed models and thematic analysis for qualitative process evaluation. Expected outcomes include improved dietary diversity, reduced prevalence of stunting, underweight and wasting and enhanced weight and linear growth. Process evaluation will assess programme implementation fidelity, acceptability and dose–response relationships.",
    "text": "The first 1000 days, from conception to 2 years of age, is the most effective window of opportunity to improve nutrition and life-long health outcomes for a child.\n\nComplementary feeding strategies, including specialised nutritious foods and targeted social and behaviour change communication interventions, have proven effective in improving infant and young child feeding practices, enhancing nutritional status and reducing child stunting in food-insecure settings.\n\nThis study aims to generate robust, context-specific evidence about the effectiveness of nutrition interventions in reducing the burden of undernutrition, particularly stunting among children under 2 years of age.\n\nIt will identify various risk factors and determinants of stunting among children, in the local context of Afghanistan, where experimental research is often challenging.\n\nThe evidence from the study will support the nationwide scale-up of stunting prevention activities, informing future public health and nutrition policies and strategies.\n\nUndernutrition in children is a serious global health challenge, affecting their growth, development and long-term well-being. 1 3  Undernutrition appears in various forms, such as stunting, wasting, underweight and deficiencies of vitamins and minerals. 1 2  Stunting is defined as impaired linear growth, with a height-for-age Z-score below −2 SD from the WHO growth standards median. 4  It usually begins in utero and persists through the first 24 months of life, 2 3  with lasting effects on physical and cognitive development. 5 7  It also has far-reaching impacts, including lower educational attainment, reduced economic productivity and increased risk of infections and degenerative diseases in adulthood. 5 8  Beyond its health effects, stunting has a significant impact on human capital development and economic growth at the individual, household and national levels. 9 10\n\nGlobally, an estimated 148 million children under the age of 5 years (22.3%) are stunted. 11  In Afghanistan, the prevalence is among the highest worldwide, with 44.7% of children under 5 stunted. 12  Recent estimates from the 2023 MICS (Multiple Indicator Cluster Survey) survey report rates of 36.5% in Logar and 47.7% in Nangarhar provinces. 12  Multiple drivers contribute to this burden, including maternal undernutrition, suboptimal breastfeeding and complementary feeding practices, frequent infections, poor sanitation, limited access to maternal health services, food insecurity and low parental education. 27 13 15  Conflict, displacement and economic instability further exacerbate food insecurity and access to health services, intensifying the risk of stunting in Afghan children.\n\nSeveral studies have evaluated nutrition-specific interventions in Afghanistan, including complementary feeding support and micronutrient supplementation. While these have shown modest improvements, their impact on stunting reduction has been limited, often due to short implementation periods, inadequate coverage or lack of integration with behaviour change communication (BCC). 16 18  Thus, despite numerous efforts, stunting prevalence remains persistently high, underscoring a critical research and programmatic gap.\n\nEffective interventions during the first 1000 days of life include nutrition education, improved maternal diets, provision of specialised nutritious foods and BCC to support infant and young child feeding (IYCF). 16 20  However, in Afghanistan, integrated strategies that combine these interventions and assess their effectiveness remain scarce.\n\nThis study addresses this gap by evaluating the combined impact of lipid-based nutrient supplements for medium quantity, social and behaviour change communication (SBCC) and duration of assistance on child growth, morbidity, dietary diversity and other determinants of undernutrition among children aged 6–24 months and pregnant and breastfeeding women (PBW). By testing a multicomponent approach in a highly food-insecure and fragile setting, this research aims to provide new evidence on innovative, context-specific solutions to reduce stunting in Afghanistan.\n\nThe programme aims to contribute towards a reduction in stunting among children 6–24 months of age in the targeted districts of Afghanistan using longitudinal cohort study, cross-sectional surveys and a process evaluation. The intervention ( box 1 ) is delivered as an integrated package; nutrition supplements address dietary gaps, SBCC promotes improved feeding and care practices and community health workers (CHWs) provide ongoing support using existing health delivery platforms through 74 community health posts (HPs), in 2 districts (Lalpur district of Nangarhar Province and Mohammad Aga district of Logar Province).\n\nA comprehensive package of community-based interventions will be implemented for children aged 6–24 months and pregnant and breastfeeding women (PBW) to reduce the burden of stunting among children and improve the nutritional status of PBW.\n\nChildren 6–24 months : provision of lipid-based nutrition supplement-medium quantity every month. Anthropometric measurements, along with data on sex, age and diet, will be recorded at baseline, endline and during monthly follow-up for 18 months. Subsequent enrolment of children will be conducted to assess the impact of the intervention duration.\n\nPBW : wheat–soya blend will be provided during pregnancy and the first 6 months of lactation period. The PBW will be followed up every month and the mid–upper arm circumference measurement will be recorded. Subsequent enrolment of PBW will help assess the impact of the duration of the intervention.\n\nTailored social and behaviour change communication sessions and promotive health education messages  will be delivered by community health workers in monthly follow-up, focusing on the appropriate use and benefits of nutritious products, infant and young child feeding practices, maternal nutrition and local healthy foods. Health education will also include messages on hygiene, immunisation and general health, based on the material developed under Community-Based Nutrition package, from the Public Nutrition Directorate of the Ministry of Public Health, Afghanistan.\n\nThis project is unique in embedding supplements, counselling and health services within Afghanistan’s existing community health system, thereby strengthening local capacity and promoting sustainability.\n\nThe project will also ensure the provision of health services, including management of moderate acute malnutrition, treatment of severe acute malnutrition, antenatal care, postnatal care, counselling and immunisation, by the health facility to which the HP is attached through a two-way referral mechanism. Furthermore, community engagement will be achieved, and village leaders, teachers, influential people, youth and schoolgirls at community level will be mobilised to support the project, where possible.\n\nThe World Food Programme (WFP) will be responsible for the provision of commodities, equipment, training of staff, programme monitoring and process evaluation through implementing partners. AKU (Aga Khan University) will be responsible for cross-sectional surveys, developing digital tools and dashboards for programme progress and data analysis. Additionally, AKU, in collaboration with WFP and Public Nutrition Directorate (PND), will be involved in capacity building of local medical universities in advanced research methods and studying local dietary patterns and behaviours.\n\nThe intervention package will be implemented in selective HPs in the Lalpur district of Nangarhar Province and Mohammad Aga district of Logar Province, Afghanistan. These sites were chosen because they represent two diverse but high-burden regions: Lalpur is a remote, mountainous area with poor road access and limited health coverage, while Mohammad Aga is periurban with relatively better infrastructure. Both districts have high stunting rates and limited dietary diversity, making them suitable for testing interventions in different Afghan contexts.\n\nThe target population for both cross surveys and cohort study includes children aged 6–24 months and PBW from the selected districts. The participants enrolled in the cohort study will be observed through monthly follow-up for 18 months of the study.\n\nFor prospective cohort study, children aged between 6–24 months who are non-wasted and without oedema, and for cross-sectional surveys, children under 2 years of age will be included in the study. PBW with either underweight or normal weight will be eligible for inclusion in the cohort study.\n\nChildren with acute malnutrition, chronic illness, medical complications or from households with anticipated absence or without caregiver consent will be excluded from participation. PBW who are overweight or obese, or experiencing other health complications, will be excluded from participation in the study to focus on undernutrition since the interventions designed for undernutrition may not be safe or appropriate for overweight women.\n\nThis study uses a prospective cohort design, complemented with cross-sectional baseline and endline surveys to evaluate intervention impact. It will also include a process evaluation to monitor implementation and a research capacity-building component for local medical universities.\n\nThe primary objective of this cohort is to evaluate the effectiveness and causal impact of intervention duration and modality on nutritional and health outcomes while identifying predictive risk factors and temporal relationships influencing stunting and related indicators. A sample size of 3995 children aged 6–24 months was calculated to detect 3.5% absolute risk reduction in stunting, with 90% power and 5% alpha level. 12  For PBW, a total of 4408 was calculated to detect 3.0% absolute risk reduction in underweight, with 90% power and 5% alpha levels among PBW. 21  These thresholds were chosen because they reflect a meaningful but realistic improvement over existing trends in Afghanistan based on prior regional programme evidence. Participants will be recruited purposively through health facility and CHW listings.\n\nTo reduce potential bias from purposive sampling (vs random sampling in surveys), eligibility and follow-up will use standardised inclusion criteria, rigorous training and continuous supervision to ensure representativeness and limit selection bias.\n\nHousehold level cross-sectional surveys at baseline and endline will be conducted in the target districts to assess pre- and postintervention nutritional status and knowledge, attitude and practice related to feeding practices, among children under 2 years and PBW. These will also assess food security, dietary diversity and determinants of malnutrition among children. Dietary diversity will be assessed using the WHO/Food and Agriculture Organization (FAO) indicators: minimum dietary diversity for women and minimum dietary diversity for children, based on a 24-hour recall of food groups consumed. A sample size of 4959 children under 2 years was calculated to detect 3.5% absolute risk reduction in stunting, with 90% power and 5% alpha level. 15  To detect an absolute risk reduction of 3.0% underweight among PBW, a sample size of 5274 was calculated. 21  Participants will be recruited using a two-staged cluster random method. In the first stage, all covered villages within the HPs in each district will be considered as primary sampling units using probability proportional to size. In the second stage, households with children under 2 years and PBW within selected villages will be randomly sampled for data collection. Data will be collected electronically using AKU data tools on tablets by trained field staff.\n\nA process evaluation will be conducted in the middle of the study to assess and identify bottlenecks, opportunities and operational factors affecting the implementation. The evaluation will use a mixed-method approach, including predesigned checklists to capture information on intervention activities and 20–25 key informant interviews (KIIs) with stakeholders involved in the stunting project. WFP will carry out this activity and data analysis will be performed by AKU. Triangulation of quantitative and qualitative findings will be performed at the end of the study.\n\nAll data collection instruments will be developed in English and then will be translated into local languages and pretested in the field before the actual data collection. A 6-day training workshop will be organised to train field staff.\n\nPatients or the public were not involved in the design, or conduct, or reporting or dissemination plans of our research.\n\nStaff will be trained following WHO standards. Inter- and intraobserver reliability tests will be conducted before data collection. Regular spot checks, supervision and calibration of equipment will ensure standardisation.\n\nData will be collected on tablets using encrypted devices for confidentiality and synced daily to a secure server at AKU.\n\nMultiple strategies (tracking through CHWs, household revisits and backup phone contacts) will minimise loss to follow-up. Missing data will be examined for patterns, and multiple imputations will be used where appropriate.\n\nFor the cohort study, time-to-event analysis using Cox models will assess the effect of supplementation duration on malnutrition onset, while regression models will evaluate programme impact across baseline and endline surveys, accounting for clustering and adjusting for potential confounders. For the baseline and endline surveys, programme impact will be evaluated using regression models that account for clustering and adjust for potential confounders (eg, age, sex, socioeconomic status and maternal education). The analyses will be conducted using standardised statistical software (Stata V.18). A standard survey module will account for the multistage design, including clustering. Descriptive statistics will be reported as means (±SD), medians, ranges and frequencies.\n\nFor process evaluation, the data from the checklists will be analysed using descriptive statistics, summarising the key aspects of implementation. For the KIIs, thematic analysis of transcribed interviews will identify major themes and subthemes. Findings from both will be triangulated. Finally, a structured report will be produced to summarise the key themes and recommendations, effectively communicating the results and guiding improvements.\n\nTo ensure the sustainability and contextual relevance, the programme aims to strengthen research capacity of two local medical universities in districts where the programme is implemented. The focus of capacity building will be to enhance institutional and individual competencies in designing, implementing, analysing and using evidence for nutrition policies and programmes. Key areas will include research design and methodology, tool development and its pretesting and data collection and analysis, with advanced research techniques and dissemination of the findings of the research. A pre- and post-training survey will be conducted to measure changes in research knowledge and skills of the staff engaged from the local institute for research capacity building. A separate concept note and plan will be designed for the course and design of the capacity development component.\n\nA study timeline outlines baseline survey, intervention rollout, monthly follow-up, midline process evaluation and endline survey ( figure 1 ).\n\nA logic model illustrates the pathway from integrated interventions (supplements, SBCC and community health services) to improved feeding practices and reduced stunting ( figure 1 ).\n\nThe findings of the study will be disseminated through workshops held at provincial and central levels. AKU, in collaboration with WFP and PND, will disseminate the findings to relevant stakeholders. Findings will be disseminated via international conferences and peer-reviewed journals.",
    "mesh_query": "health promotion[MeSH Terms]",
    "source_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12911812/",
    "data_crawling": "2026-02-19T18:22:16.958204"
  }
]